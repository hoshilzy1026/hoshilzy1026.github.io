[{"title":"Alluxio","url":"/2024/10/23/Alluxio/","content":"Alluxio\nAlluxio（之前名为 Tachyon），是一个开源的具有内存级速度的虚拟分布式存储系统， 使得应用程序可以以内存级速度与任何存储系统中的数据进行交互。\n源码：https://github.com/Alluxio/alluxio\n论文：https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-29.pdf\n\n架构\n文档：https://docs.alluxio.io/os/user/stable/cn/Overview.html\n初衷：建立底层存储和大数据计算框架之间的存储系统，为大数据应用提供一个数量级的加速，同时它还提供了通用的数据访问接口。\n\n\n\n主要分为两层：UFS 和 Alluxio\nUFS：底层文件存储，该存储空间代表不受Alluxio管理的空间。\nUFS存储可能来自外部文件系统，包括如HDFS或S3。 Alluxio可能连接到一个或多个UFS并在一个命名空间中统一呈现这类底层存储。\n通常，UFS存储旨在相当长一段时间持久存储大量数据。\n\n\nAlluxio 存储：\nAlluxio 做为一个分布式缓存来管理 Alluxio workers 本地存储，包括内存。这个在用户应用程序与各种底层存储之间的快速数据层带来的是显著提高的I&#x2F;O性能。\nAlluxio存储主要用于存储热的、暂时的数据，而不关注长期的持久性。\n要管理的每个Alluxio工作节点的存储数量和类型由用户配置决定。\n即使数据当前不在Alluxio存储中，通过Alluxio连接的UFS中的文件仍然 对Alluxio客户可见。当客户端尝试读取仅可从UFS获得的文件时数据将被复制到Alluxio存储中。\n\n\n\n\n和其他常见的分布式文件系统对比：\n\n\n角色\nAlluxio的设计使用了单Master和多Worker的架构。从高层的概念理解，Alluxio可以被分为三个部分，Master，Worker和Client。\nMaster和Worker一起组成了Alluxio的服务端，它们是系统管理员维护和管理的组件。\nClient通常是应用程序，如Spark或MapReduce作业，或者Alluxio的命令行用户。\n\n\n以前的版本需要借助 ZooKeeper 进行高可用选主，后续的 Alluxio 自己实现了高可用机制。（注：Tachyon 为 Alluxio 旧称）\n\n\nMaster\n主从模式：主Master主要负责处理全局的系统元数据，从Master不断的读取并处理主Master写的日志。同时从Master会周期性的把所有的状态写入日志。从Master不处理任何请求。\n主从之间心跳检测\n主Master不会主动发起与其他组件的通信，它只是以回复请求的方式与其他组件进行通信。一个Alluxio集群只有一个主Master。\n\n\n简单模式：最多只会有一个从Master，而且这个从Master不会被转换为主Maste。\n高可用模式：可以有零个或者多个从Master。 当主Master异常的时候，系统会选一个从Master担任新的主Master。\n\nWorker\n类似于 OSD\nAlluxio的Worker负责管理分配给Alluxio的本地资源。这些资源可以是本地内存，SSD 或者硬盘，其可以由用户配置。\nAlluxio的Worker以块的形式存储数据，并通过读或创建数据块的方式处理来自Client读写数据的请求。但Worker只负责这些数据块上的数据；文件到块的实际映射只会存储在Master上。\n\nFeatures全局命名空间\nAlluxio通过使用透明的命名机制和挂载API来实现有效的跨不同底层存储系统的数据管理。\n\n\n\nhttps://www.alluxio.io/resources/whitepapers/unified-namespace-allowing-applications-to-access-data-anywhere/\n\n智能多层级缓存\nAlluxio支持分层存储，以便管理内存之外的其它存储类型。目前Alluxio支持这些存储类型(存储层)：MEM (内存)，SSD (固态硬盘)，HDD (硬盘驱动器)\n单层&#x2F;多层 区别？\n\n单层存储\n启动时默认分配一个 ramdisk，Alluxio将在每个worker节点上默认发放一个ramdisk并占用一定比例的系统的总内存。 此ramdisk将用作分配给每个Alluxio worker的唯一存储介质。\n可以显示地设置每个 Worker 的 ramdisk 大小\n\nalluxio.worker.ramdisk.size=16GB\n\n\n可以指定多个存储介质共同组成一个 level，也可以自定义添加存储介质类型\n\nalluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd1,/mnt/ssd2alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM,SSD,SSD\n\n\n所提供的路径应该指向安装适当存储介质的本地文件系统中的路径。要启用短路操作，这些路径的权限应该允许客户端用户对该路径进行读、写和执行。例如，启动Alluxio服务的同一用户组中的客户端用户需要770权限。\n在更新存储媒体之后，我们需要指出为每个存储目录分配了多少存储空间。例如，如果我们想在ramdisk上使用 16GB，在每个 SSD 上使用 100GB:\n\nalluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB,100GB\n\n多层存储\n通常建议使用具有异构存储介质的单一存储层。在某些环境中，工作负载将受益于基于I&#x2F;O速度的存储介质显式排序。Alluxio假设层是根据I&#x2F;O性能从上到下排序的。例如，用户经常指定以下层:\n\nMEM\nSSD\nHDD\n\n\n写策略\n：用户写新的数据块时，默认情况下会将其写入顶层存储。如果顶层没有足够的可用空间， 则会尝试下一层促成。如果在所有层上均未找到存储空间，因Alluxio的设计是易失性存储，Alluxio会释放空间来存储新写入的数据块。会基于 block annotation policies 尝试从 worker 中驱逐数据，如果不能释放出新的空间，那么该写入将会失败。\n\neviction model 是同步的且是代表客户端来执行空间的释放的，主要是为要写入的客户端的数据腾出一块空闲空间，这种同步模式预计不会导致性能下降，因为在 block annotation policies 下有序的一组数据块通常都是可用的。\n\n\n读策略\n：如果数据已经存在于Alluxio中，则客户端将简单地从已存储的数据块读取数据。 如果将Alluxio配置为多层，则不一定是从顶层读取数据块， 因为数据可能已经透明地挪到更低的存储层。有两种数据读取策略：\nReadType.CACHE\n\nand\nReadType.CACHE_PROMOTE\n\n。\n\n用 ReadType.CACHE_PROMOTE 读取数据将在从worker读取数据前尝试首先将数据块挪到 顶层存储。也可以将其用作为一种数据管理策略 显式地将热数据移动到更高层存储读取。\nReadType.CACHE Alluxio将块缓存到有可用空间的最高层。因此，如果该块当前位于磁盘(SSD&#x2F;HDD)上，您将以磁盘速度读取该缓存块。\n\n\n\n# configure 2 tiers in Alluxioalluxio.worker.tieredstore.levels=2# the first (top) tier to be a memory tieralluxio.worker.tieredstore.level0.alias=MEM# defined `/mnt/ramdisk` to be the file path to the first tieralluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk# defined MEM to be the medium type of the ramdisk directoryalluxio.worker.tieredstore.level0.dirs.mediumtype=MEM# set the quota for the ramdisk to be `100GB`alluxio.worker.tieredstore.level0.dirs.quota=100GB# configure the second tier to be a hard disk tieralluxio.worker.tieredstore.level1.alias=HDD# configured 3 separate file paths for the second tieralluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3# defined HDD to be the medium type of the second tieralluxio.worker.tieredstore.level1.dirs.mediumtype=HDD,HDD,HDD# define the quota for each of the 3 file paths of the second tieralluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB\n\nBlock Allocation Policies\nAlluxio使用块分配策略来定义如何跨多个存储目录(在同一层或不同层中)分配新块。分配策略定义将新块分配到哪个存储目录中。这是通过 worker 属性\nalluxio.worker.allocate.class\n\n配置的。\n\nMaxFreeAllocator：从 0 层开始尝试到最低层，尝试将块分配到当前最具有可用性的存储目录。这是默认行为。\nRoundRobinAllocator：从 0 层到最低层开始尝试。在每一层上，维护存储目录的循环顺序。尝试按照轮询顺序将新块分配到一个目录中，如果这不起作用，就转到下一层。\nGreedyAllocator：这是 Allocator 接口的一个示例实现。它从顶层循环到最低层，尝试将新块放入可以包含该块的第一个目录中。\n\n\n\n[Experimental] Block Allocation Review Policies\n这是在Alluxio 2.4.1中增加的一个实验特性。在未来的版本中，接口可能会发生变化。\n\nAlluxio 使用块分配审查策略来补充分配策略。与定义分配应该是什么样子的分配策略相比，分配审查过程验证分配决策，并防止那些不够好的分配决策。评审者与分配器一起工作\n\n这是由worker属性\nalluxio.worker.review.class\n\n配置的。\n\nProbabilisticBufferReviewer\n：基于每个存储目录对应的可用的空间，概率性低拒绝把新的数据块写入对应目录的请求。这个概率由 \nalluxio.worker.reviewer.probabilistic.hardlimit.bytes\n 和 \nalluxio.worker.reviewer.probabilistic.softlimit.bytes\n         来决定。    - 当可用空间低于 hardlimit，默认是 64MB，新的块将被拒绝    - 当可用空间大于 softlimit，默认 256MB，新的数据块将不会被拒绝    - 当可用空间介于上下限之间时，接受新的块的写入的概率将会随着可用容量的下降而线性低下降，我们选择在目录被填满之前尽早拒绝新的块，因为当我们读取块中的新数据时，目录中的现有块会扩大大小。在每个目录中留下缓冲区可以减少 eviction 的机会。  - `AcceptingReviewer`：此审阅者接受每个块分配。和 v2.4.1 之前的行为完全一样##### Block Annotation Policies- Alluxio使用块注释策略(从v2.3开始)来保持存储中块的严格顺序。Annotation策略定义了跨层块的顺序，并在以下过程中被参考:  - Eviction  - Dynamic Block Placement.- 与写操作一起发生的 Eviction 操作将尝试根据块注释策略执行的顺序删除块。按注释顺序排列的最后一个块是驱逐的第一个候选者，无论它位于哪一层。- 可配置对应的 Anotator 类型，\nalluxio.worker.block.annotator.class\n。有如下 annotation 实现：- `LRUAnnotator`：根据最近最少使用的顺序注释块。这是Alluxio的默认注释器。- ```  LRFUAnnotator\n\n：使用可配置的权重，根据最近最不常用和最不常用的顺序注释块。\n\n- 如果权重完全偏向最近最少使用的，行为将与LRUAnnotator相同。\n- 使用 `alluxio.worker.block.annotator.lrfu.step.factor` 和 `alluxio.worker.block.annotator.lrfu.attenuation.factor` 来配置。\n\n\n\n\n\nManaging Data Replication in AlluxioPassive Replication\n与许多分布式文件系统一样，Alluxio中的每个文件都包含一个或多个分布在集群中存储的存储块。默认情况下，Alluxio可以根据工作负载和存储容量自动调整不同块的复制级别。例如，当更多的客户以类型CACHE或CACHE_PROMOTE请求来读取此块时Alluxio可能会创建此特定块更多副本。当较少使用现有副本时，Alluxio可能会删除一些不常用现有副本 来为经常访问的数据征回空间(块注释策略)。 在同一文件中不同的块可能根据访问频率不同而具有不同数量副本。\n默认情况下，此复制或征回决定以及相应的数据传输 对访问存储在Alluxio中数据的用户和应用程序完全透明。\n\nActive Replication\n除了动态复制调整之外，Alluxio还提供API和命令行 界面供用户明确设置文件的复制级别目标范围。 尤其是，用户可以在Alluxio中为文件配置以下两个属性:\nalluxio.user.file.replication.min 是此文件的最小副本数。 默认值为0，即在默认情况下，Alluxio可能会在文件变冷后从Alluxio管理空间完全删除该文件。 通过将此属性设置为正整数，Alluxio 将定期检查此文件中所有块的复制级别。当某些块 的复制数不足时，Alluxio不会删除这些块中的任何一个，而是主动创建更多 副本以恢复其复制级别。\nalluxio.user.file.replication.max 是最大副本数。一旦文件该属性 设置为正整数，Alluxio将检查复制级别并删除多余的 副本。将此属性设置为-1为不设上限(默认情况)，设置为0以防止 在Alluxio中存储此文件的任何数据。注意，alluxio.user.file.replication.max 的值 必须不少于 alluxio.user.file.replication.min。\n\n\n\nEvaluationTesting Alluxio for Memory Speed Computation on Ceph Objects\nhttps://blog.zhaw.ch/icclab/testing-alluxio-for-memory-speed-computation-on-ceph-objects/#more-12747\n4th SEPTEMBER 2020\n\n环境介绍\n底层存储：Ceph mimic\n6 OpenStack VMs\none Ceph monitor\nthree storage devices running Object Storage Devices (OSDs)\none Ceph RADOS Gateway (RGW) node\none administration node\n\n\ntotal storage size of 420GiB was spread over 7 OSD volumes attached to the three OSD nodes\n\n\nAlluxio 2.3， Java8 (换成 Java11 即升级 Alluxio 后会有后续提升)\nSpark 3.0.0\n两种模式：\n单 VM 运行 Alluxio 和 Spark （16vCPU，40GB of memory）\n集群模式：two additional Spark and Alluxio worker nodes are configured (with 16vCPUs and 40GB of memory).\n\n\n对比测试：\n直接访问 Ceph RGW 和 通过 Alluxio 访问\n通过 Alluxio 访问时，第一次访问文件的话，Alluxio 会将文件上传到内存中，后续的文件访问将直接命中内存，从而带来显著的性能提升。\n\n\n不同文件大小： 1GB, 5GB and 10GB，记录第一层和第二次访问文件需要的时间。\n平均会运行超过 10 次\n然后再次启动相同的应用程序，以再次测量相同的文件访问时间。这样做的目的是展示内存中的 Alluxio 缓存如何为以后访问相同数据的应用程序带来好处。\n\n\n\n\n\n测试结果：\n如下为单节点测试测试结果，Ceph 上第二次访问该文件相比于 Alluxio 在 1GB,5GB,10GB 时的执行时间分别为 75x，111x，107x\n\n\n\n如下为集群模式下的测试结果，所有情况的整体时间比单机的时候少了很多，Ceph 相比于 Alluxio 的第二次访问时间为 35x, 57x, 65x\n\n\nAccelerate And Scale Big Data AnAlytics with Alluxio And intel®optane™ persistent Memory\nhttps://www.alluxio.io/app/uploads/2020/05/Intel-Alluxio-DCPMM-Whitepaper-200507.pdf\n\nReliability Testing\nTODO\n\nInstall &amp; DeploySingle ServerDownload\nDownload Binary: https://www.alluxio.io/download/\nChoose Version. (eg. Alluxio 2.4.1 Release. 1.4GB)\nTar file: tar -xzf alluxio-2.4.1-bin.tar.gz\n\nInitial Config\ncd alluxio-2.4.1/conf &amp;&amp; cp alluxio-site.properties.template alluxio-site.properties\necho &quot;alluxio.master.hostname=localhost&quot; &gt;&gt; conf/alluxio-site.properties\n[Optional] If use local file system, you can specific configuration in conf files like this: echo &quot;alluxio.master.mount.table.root.ufs=/root/shunzi/Alluxio/tmp&quot; &gt;&gt; conf/alluxio-site.properties\nValidate env: ./bin/alluxio validateEnv local\n\n2 Errors:ValidateRamDiskMountPrivilegeValidateHdfsVersion复制代码\n\nStart Alluxio\nFormat journal and storage directory:\n./bin/alluxio format\n\n\nIt may throw exceptions java.nio.file.NoSuchFileException: /mnt/ramdisk/alluxioworker in log/task.log. So you need to mkdir -p /mnt/ramdisk/alluxioworker\n\n\nStart alluxio (with a master and a worker): ./bin/alluxio-start.sh local SudoMount\n\nStop local server:\n./bin/alluxio-stop.sh local\n\n\n./bin/alluxio-stop.sh all\n\n\n\nVerify\nAccess website http://localhost:19999 to check the master server status.\n\nAccess website http://localhost:30000 to check the worker server status.\n\nFor internal network, you can use reverse proxy like this: (And you can access website\nmaster\nhttp://114.116.234.136:19999\n\nand\nworker\nhttp://114.116.234.136:30000\n\n)\n\nautossh -M 1999 -fNR 19999:localhost:19999 root@114.116.234.136\nautossh -M 3000 -fNR 30000:localhost:30000 root@114.116.234.136\n\n\n\nRun tests\nVerify run status and run test cases: ./bin/alluxio runTests\n\nThe runTests command runs end-to-end tests on an Alluxio cluster to provide a comprehensive sanity check.\n\nIt will generate directory\n/default_tests_files\n\nand use different cache policy to upload files.\n\nBASIC_CACHE_ASYNC_THROUGH\nBASIC_CACHE_CACHE_THROUGH\nBASIC_CACHE_MUST_CACHE\nBASIC_CACHE_PROMOTE_ASYNC_THROUGH\nBASIC_CACHE_PROMOTE_CACHE_THROUGH\nBASIC_CACHE_PROMOTE_MUST_CACHE\nBASIC_CACHE_PROMOTE_THROUGH\nBASIC_CACHE_THROUGH\nBASIC_NON_BYTE_BUFFER_CACHE_ASYNC_THROUGH\nBASIC_NON_BYTE_BUFFER_CACHE_CACHE_THROUGH\nBASIC_NON_BYTE_BUFFER_CACHE_MUST_CACHE\nBASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_ASYNC_THROUGH\nBASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_CACHE_THROUGH\nBASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_MUST_CACHE\nBASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_THROUGH\nBASIC_NON_BYTE_BUFFER_CACHE_THROUGH\nBASIC_NON_BYTE_BUFFER_NO_CACHE_ASYNC_THROUGH\nBASIC_NON_BYTE_BUFFER_NO_CACHE_CACHE_THROUGH\nBASIC_NON_BYTE_BUFFER_NO_CACHE_MUST_CACHE\nBASIC_NON_BYTE_BUFFER_NO_CACHE_THROUGH\nBASIC_NO_CACHE_ASYNC_THROUGH\nBASIC_NO_CACHE_CACHE_THROUGH\nBASIC_NO_CACHE_MUST_CACHE\nBASIC_NO_CACHE_THROUGH\n\n\n\nSimple ExampleUpload files from local server\nShow fs command help: ./bin/alluxio fs\nList the files in Alluxio: ./bin/alluxio fs ls /\nCopy files from local server: ./bin/alluxio fs copyFromLocal LICENSE /LICENSE\nList again: ./bin/alluxio fs ls /\nCat the file: ./bin/alluxio fs cat /LICENSE\n\nReferences\n[1] Alluxio 快速上手指南\n\n[2] CSDN - Alluxio学习\n\n[3] 知乎 - 路云飞：Alluxio 技术分析\n\n[4] 知乎 - Alluxio 专栏\n\n[5] 简书 - Alluxio：架构及数据流\n\n[6] InfoQ - Alluxio在多级分布式缓存系统中的应用\n\n[7] Alluxio 开源 AI 和大数据存储编排平台-顾荣\n\n\n本文作者： zhengyun li\n\n版权声明： 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！\n\n\n","tags":["存储","缓存"]},{"title":"CRaft An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost","url":"/2025/10/13/CRaft/","content":"\nFAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销\n\n\n\nAbstract\n一致性协议主要是在分布式系统中用于保证可靠性和可用性的，现有的一致性协议大多都是要将日志项给备份到所有的服务器中，这种全量的副本的策略在存储和网络上的开销都很大，严重影响性能，所以后来出现了纠删码，即在保证相同的容错能力的条件下减少存储和网络的开销。\nRS-Paxos 是第一个支持 EC 数据的一致性协议，但是比起通用的一致性协议，如 Paxos&#x2F;Raft，可用性都相对更差。我们指出了RSPaxos的活性问题，并试图解决，基于 Raft 提出了 CRaft，既能使用 EC 码像 RS-Paxos 一样降低存储和网络开销，也能保证如 Raft 一样的 liveness。\n基于 CRaft 实现了一个 KVs，实验表明相对于 Raft 节省了 66% 的存储空间，写吞吐量提升了 250%，写延迟减少了 60.8%\n\nIntroduction\n共识算法介绍\n：共识协议协议通常保证安全性和活动性，这意味着它们总是返回正确的结果，并且在大多数服务器都没有发生故障的情况下可以完全正常工作。\n\nGoogle’s Chubby 会使用 Paxos 对 metadata 做副本\nGaios(NSDI2011) 表明一致性协议可以被用于所有数据的 replicated\n现如今大量应用如 etcd, TinyKV, FSS 等大规模系统都使用了 Raft&#x2F;Paxos 来 replicated TB 数量级的数据，并提供更好的可用性\n\n\n多副本介绍：数据操作通常在分布式系统中被转换为一系列的日志指令，然后使用一致性协议在所有的服务器之间进行备份，所以数据需要经过网络传输到所有的服务器，然后还要刷会到磁盘持久化保存。一致性问题中，容错率如果为 F，那么则至少需要 N &#x3D; (2F + 1) 的服务器，否则就可能因为分组的原因出现不一致的情况。因此传统的副本策略往往就意味值原始数据量的 N 倍的网络和存储开销，而且随着这些协议在大规模存储系统中得到了越来越多的应用，N 倍的网络和存储开销带来的则是延迟的增加和吞吐量的下降。所以出现了 Erasure Coding\n\n纠删码介绍：纠删码相比于全量拷贝的副本策略，极大地减小了存储和网络的开销。通过将数据进行分片，编码分片后的数据并生成一些校验的分片，原始的数据就能从足够数量的分片子集中恢复出来，这时候每个服务器只存储一个分片，而不是数据的全量拷贝，开销极大减小。FSS 中就使用了纠删码来减少存储开销，但是 FSS 在编码之前使用了一个 5 way 流水线 Paxos 来备份完整的用户数据和元数据，因此额外的网络开销还是有 4 倍数据量大小。\n\nRS-Paxos 是第一个结合了 Paxos 和 EC 的共识协议，虽然减少了存储和网络的开销，但是在可用性上比 Paxos 还是更差，RA-Paxos 牺牲了 liveness 来使用 EC 提升性能，换句话说就是 RS-Paoxs 如果有 N &#x3D; (2F + 1) 的服务器不再能容忍 F 个错误，即容错率下降了，主要是因为 RS-Paxos 中的提交要求越来越严格。\n\n作者提出了 erasure-coding-supported version of Raft CRaft (Coded Raft)。该方案中，一个 leader 有两种方法备份日志项到 followers，如果 leader 能够和足够数量的 followers 通信，那么 leader 将使用分片后的日志项进行备份，即传统纠删码的方式，否则将备份完整的数据以保证可用性。相比于 RS-Paxos，CRaft 最大的不同是拥有和 Paxos&#x2F;Raft 相同级别的 liveness，而 RS-Paxos 没有，但是两个方案都节省了网络和存储的成本。\n\n\nBackgroundRaft\nhttps://raft.github.io/\nRaft 原始论文：https://raft.github.io/raft.pdf\nRaft 中主要有三个角色&#x2F;三种状态。Candidate 收到了来自大多数 servers 的选票后成为 Leader，一个 Server 只会给 和该 Server 日志同步的 Candidate 投票。每个 Server 每一轮最多投一次，所以 Raft 保证每一轮最多就一个 leader。\nLeader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader。\nFollower: 类似选民，完全被动\nCandidate候选人: 类似Proposer，可以被选为一个新的 Leader\n\n\nleader 从客户端接收日志条目，并试图将它们复制到其他服务器，迫使其他服务器的日志与自己的日志一致。当 leader 发现这一轮中有日志被被分到了大多数 servers，该日志项和之前的日志将被安全地应用到状态机中。Leader 将提交并应用这些日志项，然后告诉 followers 也 apply 他们。\n\n\n\n用于实际系统的共识协议通常具有以下特性：\nSafety：它们不会在所有非拜占庭条件下返回错误的结果\nLiveness：只要大多数服务器都处于活动状态，并且能够相互通信和与客户端通信，它们就能完全发挥作用。我们称这组服务器是健康的\n\n\nRaft 中的 Safety 是由 Leader Completeness Property 来保证的， 如果在给定 term 提交了日志条目，那么该条目将出现在所有编号较高的 term 的 leader 日志中。\nLiveness 由 Raft 规则保证，通常使用了一致性协议的系统的服务器的数量常常为奇数，假设 N &#x3D; 2F + 1，Raft 可以容忍 F 个错误，我们定义一个一致性协议可以容忍的失败数量作为 liveness level，所以此时的 liveness level 为 F，更高的 liveness level 意味着更好的 liveness，没有一个协议的 liveness level 可以达到 F+1，因为如果存在这样的协议，则可能存在两个分裂的 F 个健康服务器组，这两个组可以分别就不同的内容达成一致，这是违反安全特性的。\n\nErasure Coding\n擦除编码是存储系统和网络传输中容忍错误的常用技术。们已经提出了大量的编码，其中最常用的是Reed-Solomon (RS)编码。RS 码中有两个可配置的正整数参数 k 和 m，数据被分成了相同大小的 k 个分片，然后使用这 k 个原始的数据分片计算出 m 个类似的校验分片，也就是编码过程，此时总共将有 k+m 个分片，(k,m)-RS 码就意味着所有分片中的任意 k 个分片就能恢复出原始数据，这就是 RS 码的容错原理。（类似于解方程的过程）\n当引入一致性协议，k + m &#x3D; N，N 为服务器的总数量，存储和网络开销将被见效的全拷贝的 1&#x2F;k，然而如何保证 safety 和 liveness 不容忽视\n\nRS-Paxos\nRS-Paxos 是将纠删编码与 Paxos 相结合的一种 Paxos 的改革版本，可以节省存储和网络成本。在 Paxos 中，命令被完全传输。然而，在 RS-Paxos 中，命令是通过代码片段传输的。根据这一变化，服务器在 RS-Paxos 中只能存储和传输片段，从而降低了存储和网络成本。\n\n为了保证安全性和活动性，Paxos和Raft基于以下包容-排斥原则。\n​                                                 ∣A∪B∣&#x3D;∣A∣+∣B∣−∣A∩B∣\n\n\n包含排除原则保证在两个不同的服务器组合中至少有一个服务器的数量差距，这样安全性就可以得到保证。\n\nRS-Paxos 的想法是增加交集集的大小。具体来说，在选择了一个 (k,m)-RS 代码后，读quorum QR、写 quorum QW 和服务器数量 N 应该符合以下公式。\n​\t\t\t\t\t\t\t\t\t\t\t\t\tQ**R+Q**W−N≥k\n\n\n\n\n本文作者： zhengyun Li\n版权声明： 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！\n\n","tags":["存储","分布式"]},{"title":"Raft算法","url":"/2023/03/13/Raft/","content":"  raft是工程上使用较为广泛的强一致性、去中心化、高可用的分布式协议。在这里强调了是在工程上，因为在学术理论界，最耀眼的还是大名鼎鼎的Paxos。但Paxos是：少数真正理解的人觉得简单，尚未理解的人觉得很难，大多数人都是一知半解。本人也花了很多时间、看了很多材料也没有真正理解。直到看到raft的论文，两位研究者也提到，他们也花了很长的时间来理解Paxos，他们也觉得很难理解，于是研究出了raft算法。\n   raft是一个共识算法（consensus algorithm），所谓共识，就是多个节点对某个事情达成一致的看法，即使是在部分节点故障、网络延时、网络分割的情况下。这些年最为火热的加密货币（比特币、区块链）就需要共识算法，而在分布式系统中，共识算法更多用于提高系统的容错性，比如分布式存储中的复制集（replication），在带着问题学习分布式系统之中心化复制集一文中介绍了中心化复制集的相关知识。raft协议就是一种leader-based的共识算法，与之相应的是leaderless的共识算法。\n  本文基于论文In Search of an Understandable Consensus Algorithm对raft协议进行分析，当然，还是建议读者直接看论文。\n\n\nraft算法概览  Raft算法的头号目标就是容易理解（UnderStandable），这从论文的标题就可以看出来。当然，Raft增强了可理解性，在性能、可靠性、可用性方面是不输于Paxos的。\n\nRaft more understandable than Paxos and also provides a better foundation for building practical systems\n\n   为了达到易于理解的目标，raft做了很多努力，其中最主要是两件事情：\n\n问题分解\n状态简化\n\n   问题分解是将”复制集中节点一致性”这个复杂的问题划分为数个可以被独立解释、理解、解决的子问题。在raft，子问题包括，leader election， log replication，safety，membership changes。而状态简化更好理解，就是对算法做出一些限制，减少需要考虑的状态数，使得算法更加清晰，更少的不确定性（比如，保证新选举出来的leader会包含所有commited log entry）\n\nRaft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. A leader can fail or become disconnected from the other servers, in which case a new leader is elected.\n\n   上面的引文对raft协议的工作原理进行了高度的概括：raft会先选举出leader，leader完全负责replicated log的管理。leader负责接受所有客户端更新请求，然后复制到follower节点，并在“安全”的时候执行这些请求。如果leader故障，followes会重新选举出新的leader。\n   这就涉及到raft最新的两个子问题： leader election和log replication\nleader election  raft协议中，一个节点任一时刻处于以下三个状态之一：\n\nleader\nfollower\ncandidate\n\n   给出状态转移图能很直观的直到这三个状态的区别\n\n可以看出所有节点启动时都是follower状态；在一段时间内如果没有收到来自leader的心跳，从follower切换到candidate，发起选举；如果收到majority的造成票（含自己的一票）则切换到leader状态；如果发现其他节点比自己更新，则主动切换到follower。\n   总之，系统中最多只有一个leader，如果在一段时间里发现没有leader，则大家通过选举-投票选出leader。leader会不停的给follower发心跳消息，表明自己的存活状态。如果leader故障，那么follower会转换成candidate，重新选出leader。\n\nterm\n\n 从上面可以看出，哪个节点做leader是大家投票选举出来的，每个leader工作一段时间，然后选出新的leader继续负责。这根民主社会的选举很像，每一届新的履职期称之为一届任期，在raft协议中，也是这样的，对应的术语叫term。\n\n term（任期）以选举（election）开始，然后就是一段或长或短的稳定工作期（normal Operation）。从上图可以看到，任期是递增的，这就充当了逻辑时钟的作用；另外，term 3展示了一种情况，就是说没有选举出leader就结束了，然后会发起新的选举，后面会解释这种split vote的情况。\n选举过程详解  上面已经说过，如果follower在election timeout内没有收到来自leader的心跳，（也许此时还没有选出leader，大家都在等；也许leader挂了；也许只是leader与该follower之间网络故障），则会主动发起选举。步骤如下：\n\n增加节点本地的 current term ，切换到candidate状态\n投自己一票\n并行给其他节点发送 RequestVote RPCs\n等待其他节点的回复\n\n   在这个过程中，根据来自其他节点的消息，可能出现三种结果\n\n收到majority的投票（含自己的一票），则赢得选举，成为leader\n被告知别人已当选，那么自行切换到follower\n一段时间内没有收到majority投票，则保持candidate状态，重新发出选举\n\n   第一种情况，赢得了选举之后，新的leader会立刻给所有节点发消息，广而告之，避免其余节点触发新的选举。在这里，先回到投票者的视角，投票者如何决定是否给一个选举请求投票呢，有以下约束：\n\n在任一任期内，单个节点最多只能投一票\n候选人知道的信息不能比自己的少（这一部分，后面介绍log replication和safety的时候会详细介绍）\nfirst-come-first-served 先来先得\n\n   第二种情况，比如有三个节点A B C。A B同时发起选举，而A的选举消息先到达C，C给A投了一票，当B的消息到达C时，已经不能满足上面提到的第一个约束，即C不会给B投票，而A和B显然都不会给对方投票。A胜出之后，会给B,C发心跳消息，节点B发现节点A的term不低于自己的term，知道有已经有Leader了，于是转换成follower。\n   第三种情况，没有任何节点获得majority投票，比如下图这种情况：\n\n   总共有四个节点，Node C、Node D同时成为了candidate，进入了term 4，但Node A投了NodeD一票，NodeB投了Node C一票，这就出现了平票 split vote的情况。这个时候大家都在等啊等，直到超时后重新发起选举。如果出现平票的情况，那么就延长了系统不可用的时间（没有leader是不能处理客户端写请求的），因此raft引入了randomized election timeouts来尽量避免平票情况。同时，leader-based 共识算法中，节点的数目都是奇数个，尽量保证majority的出现。\n\nlog replication\n 当有了leader，系统应该进入对外工作期了。客户端的一切请求来发送到leader，leader来调度这些并发请求的顺序，并且保证leader与followers状态的一致性。raft中的做法是，将这些请求以及执行顺序告知followers。leader和followers以相同的顺序来执行这些请求，保证状态一致。\n\nReplicated state machines\n\n  共识算法的实现一般是基于复制状态机（Replicated state machines），何为复制状态机：\n\nIf two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.\n\n   简单来说：相同的初识状态 + 相同的输入 &#x3D; 相同的结束状态。引文中有一个很重要的词deterministic，就是说不同节点要以相同且确定性的函数来处理输入，而不要引入一下不确定的值，比如本地时间等。如何保证所有节点 get the same inputs in the same order，使用replicated log是一个很不错的注意，log具有持久化、保序的特点，是大多数分布式系统的基石。\n  因此，可以这么说，在raft中，leader将客户端请求（command）封装到一个个log entry，将这些log entries复制（replicate）到所有follower节点，然后大家按相同顺序应用（apply）log entry中的command，则状态肯定是一致的。\n  下图形象展示了这种log-based replicated state machine\n\n\n\n请求完整流程 当系统（leader）收到一个来自客户端的写请求，到返回给客户端，整个过程从leader的视角来看会经历以下步骤：\n\nleader append log entry\nleader issue AppendEntries RPC in parallel\nleader wait for majority response\nleader apply entry to state machine\nleader reply to client\nleader notify follower apply log\n\n  可以看到日志的提交过程有点类似两阶段提交(2PC)，不过与2PC的区别在于，leader只需要大多数（majority）节点的回复即可，这样只要超过一半节点处于工作状态则系统就是可用的。\n  那么日志在每个节点上是什么样子的呢\n\n不难看到，logs由顺序编号的log entry组成 ，每个log entry除了包含command，还包含产生该log entry时的leader term。从上图可以看到，五个节点的日志并不完全一致，raft算法为了保证高可用，并不是强一致性，而是最终一致性，leader会不断尝试给follower发log entries，直到所有节点的log entries都相同。\n  在上面的流程中，leader只需要日志被复制到大多数节点即可向客户端返回，一旦向客户端返回成功消息，那么系统就必须保证log（其实是log所包含的command）在任何异常的情况下都不会发生回滚。这里有两个词：commit（committed），apply(applied)，前者是指日志被复制到了大多数节点后日志的状态；而后者则是节点将日志应用到状态机，真正影响到节点状态。\n\nThe leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is committed once the leader that created the entry has replicated it on a majority of the servers\n\nsafety 在上面提到只要日志被复制到majority节点，就能保证不会被回滚，即使在各种异常情况下，这根leader election提到的选举约束有关。在这一部分，主要讨论raft协议在各种各样的异常情况下如何工作的。\n  衡量一个分布式算法，有许多属性，如\n\nsafety：nothing bad happens,\nliveness： something good eventually happens.\n\n  在任何系统模型下，都需要满足safety属性，即在任何情况下，系统都不能出现不可逆的错误，也不能向客户端返回错误的内容。比如，raft保证被复制到大多数节点的日志不会被回滚，那么就是safety属性。而raft最终会让所有节点状态一致，这属于liveness属性。\n  raft协议会保证以下属性\n\nElection safety 选举安全性，即任一任期内最多一个leader被选出。这一点非常重要，在一个复制集中任何时刻只能有一个leader。系统中同时有多余一个leader，被称之为脑裂（brain split），这是非常严重的问题，会导致数据的覆盖丢失。在raft中，两点保证了这个属性：\n\n一个节点某一任期内最多只能投一票；\n只有获得majority投票的节点才会成为leader。\n\n  因此，某一任期内一定只有一个leader。\nlog matching 很有意思，log匹配特性， 就是说如果两个节点上的某个log entry的log index相同且term相同，那么在该index之前的所有log entry应该都是相同的。如何做到的？依赖于以下两点\n\nIf two entries in different logs have the same index and term, then they store the same command.\nIf two entries in different logs have the same index and term, then the logs are identical in all preceding entries.\n\n  首先，leader在某一term的任一位置只会创建一个log entry，且log entry是append-only。其次，consistency check。leader在AppendEntries中包含最新log entry之前的一个log 的term和index，如果follower在对应的term index找不到日志，那么就会告知leader不一致。\n  在没有异常的情况下，log matching是很容易满足的，但如果出现了node crash，情况就会变得负责。比如下图\n\n 注意：上图的a-f不是6个follower，而是某个follower可能存在的六个状态\n  leader、follower都可能crash，那么follower维护的日志与leader相比可能出现以下情况\n\n比leader日志少，如上图中的ab\n比leader日志多，如上图中的cd\n某些位置比leader多，某些日志比leader少，如ef（多少是针对某一任期而言）\n\n  当出现了leader与follower不一致的情况，leader强制follower复制自己的log\n\nTo bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point.\n\n leader会维护一个nextIndex[]数组，记录了leader可以发送每一个follower的log index，初始化为eader最后一个log index加1， 前面也提到，leader选举成功之后会立即给所有follower发送AppendEntries RPC（不包含任何log entry， 也充当心跳消息）,那么流程总结为：\n\ns1 leader 初始化nextIndex[x]为 leader最后一个log index + 1s2 AppendEntries里prevLogTerm prevLogIndex来自 logs[nextIndex[x] - 1]s3 如果follower判断prevLogIndex位置的log term不等于prevLogTerm，那么返回 False，否则返回Trues4 leader收到follower的回复，如果返回值是False，则nextIndex[x] -&#x3D; 1, 跳转到s2. 否则s5 同步nextIndex[x]后的所有log entries\n\nleader completeness vs elcetion restriction leader完整性：如果一个log entry在某个任期被提交（committed），那么这条日志一定会出现在所有更高term的leader的日志里面。这个跟leader election、log replication都有关。\n\n一个日志被复制到majority节点才算committed\n一个节点得到majority的投票才能成为leader，而节点A给节点B投票的其中一个前提是，B的日志不能比A的日志旧。下面的引文指处了如何判断日志的新旧\n\n\nvoter denies its vote if its own log is more up-to-date than that of the candidate.\n\n\nIf the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.\n\n上面两点都提到了majority：commit majority and vote majority，根据Quorum，这两个majority一定是有重合的，因此被选举出的leader一定包含了最新的committed的日志。\n  raft与其他协议（Viewstamped Replication、mongodb）不同，raft始终保证leade包含最新的已提交的日志，因此leader不会从follower catchup日志，这也大大简化了系统的复杂度。\ncorner casestale leaderraft保证Election safety，即一个任期内最多只有一个leader，但在网络分割（network partition）的情况下，可能会出现两个leader，但两个leader所处的任期是不同的。如下图所示\n\n  系统有5个节点ABCDE组成，在term1，Node B是leader，但Node A、B和Node C、D、E之间出现了网络分割，因此Node C、D、E无法收到来自leader（Node B）的消息，在election time之后，Node C、D、E会分期选举，由于满足majority条件，Node E成为了term 2的leader。因此，在系统中貌似出现了两个leader：term 1的Node B， term 2的Node E, Node B的term更旧，但由于无法与Majority节点通信，NodeB仍然会认为自己是leader。\n  在这样的情况下，我们来考虑读写。\n  首先，如果客户端将请求发送到了NodeB，NodeB无法将log entry 复制到majority节点，因此不会告诉客户端写入成功，这就不会有问题。\n  对于读请求，stale leader可能返回stale data，比如在read-after-write的一致性要求下，客户端写入到了term2任期的leader Node E，但读请求发送到了Node B。如果要保证不返回stale data，leader需要check自己是否过时了，办法就是与大多数节点通信一次，这个可能会出现效率问题。另一种方式是使用lease，但这就会依赖物理时钟。\n  从raft的论文中可以看到，leader转换成follower的条件是收到来自更高term的消息，如果网络分割一直持续，那么stale leader就会一直存在。而在raft的一些实现或者raft-like协议中，leader如果收不到majority节点的消息，那么可以自己step down，自行转换到follower状态。\nState Machine Safety 前面在介绍safety的时候有一条属性没有详细介绍，那就是State Machine Safety：\n\nState Machine Safety: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.\n\n 如果节点将某一位置的log entry应用到了状态机，那么其他节点在同一位置不能应用不同的日志。简单点来说，所有节点在同一位置（index in log entries）应该应用同样的日志。但是似乎有某些情况会违背这个原则：\n\n上图是一个较为复杂的情况。在时刻(a), s1是leader，在term2提交的日志只赋值到了s1 s2两个节点就crash了。在时刻（b), s5成为了term 3的leader，日志只赋值到了s5，然后crash。然后在(c)时刻，s1又成为了term 4的leader，开始赋值日志，于是把term2的日志复制到了s3，此刻，可以看出term2对应的日志已经被复制到了majority，因此是committed，可以被状态机应用。不幸的是，接下来（d）时刻，s1又crash了，s5重新当选，然后将term3的日志复制到所有节点，这就出现了一种奇怪的现象：被复制到大多数节点（或者说可能已经应用）的日志被回滚。\n  究其根本，是因为term4时的leader s1在（C）时刻提交了之前term2任期的日志。为了杜绝这种情况的发生：\n\nRaft never commits log entries from previous terms by counting replicas.Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property.\n\n 也就是说，某个leader选举成功之后，不会直接提交前任leader时期的日志，而是通过提交当前任期的日志的时候“顺手”把之前的日志也提交了，具体怎么实现了，在log matching部分有详细介绍。那么问题来了，如果leader被选举后没有收到客户端的请求呢，论文中有提到，在任期开始的时候发立即尝试复制、提交一条空的log\n\nRaft handles this by having each leader commit a blank no-op entry into the log at the start of its term.\n\n因此，在上图中，不会出现（C）时刻的情况，即term4任期的leader s1不会复制term2的日志到s3。而是如同(e)描述的情况，通过复制-提交 term4的日志顺便提交term2的日志。如果term4的日志提交成功，那么term2的日志也一定提交成功，此时即使s1crash，s5也不会重新当选。\nleader crashfollower的crash处理方式相对简单，leader只要不停的给follower发消息即可。当leader crash的时候，事情就会变得复杂。在这篇文章中，作者就给出了一个更新请求的流程图。\n\n我们可以分析leader在任意时刻crash的情况，有助于理解raft算法的容错性。\n总结raft将共识问题分解成两个相对独立的问题，leader election，log replication。流程是先选举出leader，然后leader负责复制、提交log（log中包含command）\n  为了在任何异常情况下系统不出错，即满足safety属性，对leader election，log replication两个子问题有诸多约束\nleader election约束：\n\n同一任期内最多只能投一票，先来先得\n选举人必须比自己知道的更多（比较term，log index）\n\nlog replication约束：\n\n一个log被复制到大多数节点，就是committed，保证不会回滚\nleader一定包含最新的committed log，因此leader只会追加日志，不会删除覆盖日志\n不同节点，某个位置上日志相同，那么这个位置之前的所有日志一定是相同的\nRaft never commits log entries from previous terms by counting replicas.\n\n  本文是在看完raft论文后自己的总结，不一定全面。个人觉得，如果只是相对raft协议有一个简单了解，看这个动画演示就足够了，如果想深入了解，还是要看论文，论文中Figure 2对raft算法进行了概括。最后，还是找一个实现了raft算法的系统来看看更好。\nreferenceshttps://web.stanford.edu/~ouster/cgi-bin/papers/raft-atc14https://raft.github.io/http://thesecretlivesofdata.com/raft/\n\n\n本文作者： zhengyun Li\n版权声明： 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！\n\n","tags":["分布式"]},{"title":"Ceph-RBD源码阅读","url":"/2025/10/13/Ceph-RBD%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","content":"\nRBD 是 Ceph 分布式存储系统中提供的块存储服务\n该篇主要针对 RBD 中的整体架构以及 IO 流程进行介绍\n针对 librbd 中提供的接口进行简单介绍，后续将在此基础上进行实战\n\nCeph RBD\nRBD：RADOS Block Devices. Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster.\n\n\n\n整体介绍\nCeph RBD 模块主要提供了两种对外接口：\n\n一种是基于 librados 的用户态接口库 librbd，支持 C&#x2F;C++ 接口以及 Python 等高级语言的绑定；\n另外一种是通过 kernel Module 的方式（一个叫 krbd 的内核模块），通过用户态的 rbd 命令行工具，将 RBD 块设备映射为本地的一个块设备文件。\n\n\n\n\n\nRDB 的块设备由于元数据信息少而且访问不频繁，故 RBD 在 Ceph 集群中不需要单独的守护进程讲元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的 Monitor 服务和 OSD 服务进行交互。\n\nRBD IO 流\n\nRBD 模块 IO 流图\n\n\n几个重要的存储组织\nPool：存储资源池。IO 之前，需要先创建一个存储池，存储池统一地对逻辑存储单元进行管理，并对其进行初始化。同时指定一个 Pool 中的 PG 数量。是 Ceph 存储数据时的逻辑分区，类似于 HDFS 中的 namespace\n\nceph osd pool create rbd 32rbd pool init rbd\n\n\nRBD：块设备镜像。在创建好 Pool 的基础之上，对应的创建块设备镜像并和存储池进行映射绑定\n\nObject：按照数据切片的大小，将所有数据切片为一个个对象，进行相应的对象存储操作。其中 Key 需要根据序号进行生成从而进行区分。\n\n\nrbd create --size &#123;megabytes&#125; &#123;pool-name&#125;/&#123;image-name&#125;rbd create --size 1024 swimmingpool/bar\n\n\nPG：Placement Group，用于放置标准大小的 Object 的载体。其数量的计算公式：Total PGs = (Total_number_of_OSD * 100) / max_replication_count 再对结果向上取 2 的 N 次方作为最终的数量。PG 同时作为数据均衡和迁移的最小单位，PG 也有相应的主从之分。\nOSD：OSD 是负责物理存储的进程，也可以理解为最终的对象存储节点。一般情况下，一块磁盘启动一个 OSD 进程，一组 PG （多副本）分布在不同的 OSD 上。\n\nIO流程\n客户端创建对应的存储池 Pool，指定相应的 PG 个数以及 PGP 个数（用于 PG 中的数据均衡）\n创建 pool&#x2F;image rbd设备进行挂载\n用户写入的数据进行切块，每个块有默认大小，并且每个块都有一个 Key，Key 就是 object+序号\n将每个 object 通过 pg 进行副本位置的分配\nPG 根据 cursh 算法会寻找指定个数的 osd（主从个数），把这个 object 分别保存在这些 osd 上\nosd 上实际是把底层的 disk 进行了格式化操作，一般部署工具会将它格式化为 xfs 文件系统\nobject 的存储就变成了存储一个文件 rbd0.object1.file\n\nRBD IO 框架\n客户端写数据osd过程：\n采用的是 librbd 的形式，使用 librbd 创建一个块设备，向这个块设备中写入数据\n在客户端本地同过调用 librados 接口，然后经过 pool，rbd，object，pg 进行层层映射（CRUSH 算法）,在 PG 这一层中，可以知道数据保存在哪几个 OSD 上，这几个 OSD 分为主从的关系\n客户端与 primary OSD 建立 SOCKET 通信，将要写入的数据传给 primary OSD，由 primary OSD 再将数据发送给其他 replica OSD 数据节点。\n\nlibrbd\nlibrbd 到 OSD 的数据流向如下：\n\n\n模块介绍\nlibrbd：Librbd 是Ceph提供的块存储接口的抽象，它提供C&#x2F;C++、Python等多种接口。对于C++，最主要的两个类就是RBD 和 Image。 RBD 主要负责创建、删除、克隆映像等操作，而Image 类负责映像的读写等操作。\ncls_rbd：cls_rbd是Cls的一个扩展模块，Cls允许用户自定义对象的操作接口和实现方法，为用户提供了一种比较直接的接口扩展方式。通过动态链接的形式加入 osd 中，在 osd 上直接执行。\nlibrados：librados 提供客户端访问 Ceph 集群的原生态统一接口。其它接口或者命令行工具都基于该动态库实现。在 librados 中实现了 Crush 算法和网络通信等公共功能，数据请求操作在 librados 计算完成后可以直接与对应的 OSD 交互进行数据传输。\nOSDC：该模块是客户端模块比较底层的模块，用于封装操作数据，计算对象的地址、发送请求和处理超时。\nOSD：部署在每一个硬盘上的 OSD 进程，主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor\nOS：操作系统，在此处则主要是 OSD 的 IO 请求下发到对应的硬盘上的文件系统，由文件系统来完成后续的 IO 操作。\n\nlibrbd 详细介绍功能模块\n核心机制\nlibrbd 是一个将 block io （[off, len]）转换成 rados object io （[oid, off, len]）的中间层。为了支持高性能 io 处理，其内部维护了一个 io 队列，一个异步回调队列，以及对这两个队列中的请求进行处理的线程池，如下图所示。\n\n\n\nIO 时序图\n\n\n\nlibrbd 提供了针对 image 的数据读写和管理操作两种访问接口，其中数据读写请求入 io_work_queue，然后由线程池中的线程将 io 请求以 object 粒度切分并分别调用 rados 层的 aio 接口（IoCtxImpl）下发，当所有的 object 请求完成时，调用 librbd io 回调（librbd::io::AioCompletion）完成用户层的数据 io。而对 image 的管理操作通常需要涉及单个或多个对象的多次访问以及对内部状态的多次更新，其第一次访问将从用户线程调用至 rados 层 aio 接口或更新状态后入 op_work_queue 队列进行异步调用，当 rados aio 层回调或 Context 完成时再根据实现逻辑调用新的 rados aio 或构造 Context 回调，如此反复，最后调用应用层的回调完成管理操作请求。\n\n此外为了支持多客户端共享访问 image，librbd 提供了构建于 rados watch&#x2F;notify 之上的通知、远程执行以及 exclusive lock 分布式锁机制。每个 librbd 客户端在打开 image 时（以非只读方式打开）都会 watch image 的 header 对象，从远程发往本地客户端的通知消息或者内部的 watch 错误消息会通过 RadosClient 的 Finisher 线程入 op_work_queue 队列进行异步处理。\n\n\n组成元素\nimage 主要由 rbd_header 元数据 rados 对象及 rbd_data 数据 rados 对象组成，随着特性的增加会增加其它一些元数据对象，但 librbd 内部的运行机制并不会有大的变化，一切都以异步 io、事件（请求）驱动为基础。\n\n相关接口声明\n此处以 librbd 的 C++ 库 librbd.hpp 为例对 librbd 提供的相关功能 API 进行介绍（除此以外还提供了 C 语言的相关库 librbd.h）\nlibrbd 提供的接口导图如下：\n\n\nnamespace librbd &#123; // 库在librbd名字空间中  using librados::IoCtx; // librados 库对外提供的接口  class Image;  class ImageOptions;  class PoolStats;  typedef void *image_ctx_t;  typedef void *completion_t;  typedef void (*callback_t)(completion_t cb, void *arg); // 异步操作回调接口  ...class CEPH_RBD_API RBD&#123;public:  RBD();  ~RBD();  // This must be dynamically allocated with new, and  // must be released with release().  // Do not use delete.  struct AioCompletion &#123;    void *pc;    AioCompletion(void *cb_arg, callback_t complete_cb);    bool is_complete();    int wait_for_complete();    ssize_t get_return_value();    void release();  &#125;;  // 接下来一些API: open/create/clone/remove/rename/list/migration 等  // RBD groups support functions  create/remove/list/renameprivate:  /* We don&#x27;t allow assignment or copying */  RBD(const RBD&amp; rhs);  const RBD&amp; operator=(const RBD&amp; rhs);&#125;;// Image 参数设置class CEPH_RBD_API ImageOptions &#123;public:  ImageOptions();  ImageOptions(rbd_image_options_t opts);  ImageOptions(const ImageOptions &amp;imgopts);  ~ImageOptions();  int set(int optname, const std::string&amp; optval);  int set(int optname, uint64_t optval);  int get(int optname, std::string* optval) const;  int get(int optname, uint64_t* optval) const;  int is_set(int optname, bool* is_set);  int unset(int optname);  void clear();  bool empty() const;private:  friend class RBD;  friend class Image;  rbd_image_options_t opts;&#125;;// 存储池 Pool 状态class CEPH_RBD_API PoolStats &#123;public:  PoolStats();  ~PoolStats();  PoolStats(const PoolStats&amp;) = delete;  PoolStats&amp; operator=(const PoolStats&amp;) = delete;  int add(rbd_pool_stat_option_t option, uint64_t* opt_val);private:  friend class RBD;  rbd_pool_stats_t pool_stats;&#125;;class CEPH_RBD_API UpdateWatchCtx &#123;public:  virtual ~UpdateWatchCtx() &#123;&#125;  /**   * Callback activated when we receive a notify event.   */  virtual void handle_notify() = 0;&#125;;class CEPH_RBD_API Image&#123;public:  Image();  ~Image();  // 镜像的读写，flatten，trim等操作private:  friend class RBD;  Image(const Image&amp; rhs);  const Image&amp; operator=(const Image&amp; rhs);  image_ctx_t ctx; // viod*, 实际指向具体实现的类&#125;;&#125;\n\nclass CEPH_RBD_API RBD\nRBD 主要负责 Image 的创建、删除、重命名、克隆映像等操作，包括对存储池的元数据的管理\n针对部分操作提供异步接口\n\nclass CEPH_RBD_API RBD&#123;public:  RBD();  ~RBD();  // This must be dynamically allocated with new, and  // must be released with release().  // Do not use delete.  struct AioCompletion &#123;    void *pc;    AioCompletion(void *cb_arg, callback_t complete_cb);    bool is_complete();    int wait_for_complete();    ssize_t get_return_value();    void *get_arg();    void release();  &#125;;  void version(int *major, int *minor, int *extra);  int open(IoCtx&amp; io_ctx, Image&amp; image, const char *name);  int open(IoCtx&amp; io_ctx, Image&amp; image, const char *name, const char *snapname);  int open_by_id(IoCtx&amp; io_ctx, Image&amp; image, const char *id);  int open_by_id(IoCtx&amp; io_ctx, Image&amp; image, const char *id, const char *snapname);  int aio_open(IoCtx&amp; io_ctx, Image&amp; image, const char *name,\t       const char *snapname, RBD::AioCompletion *c);  int aio_open_by_id(IoCtx&amp; io_ctx, Image&amp; image, const char *id,\t             const char *snapname, RBD::AioCompletion *c);  // see librbd.h  int open_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *name,\t\t     const char *snapname);  int open_by_id_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *id,                           const char *snapname);  int aio_open_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *name,\t\t\t const char *snapname, RBD::AioCompletion *c);  int aio_open_by_id_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *id,                               const char *snapname, RBD::AioCompletion *c);  int list(IoCtx&amp; io_ctx, std::vector&lt;std::string&gt;&amp; names)    __attribute__((deprecated));  int list2(IoCtx&amp; io_ctx, std::vector&lt;image_spec_t&gt;* images);  int create(IoCtx&amp; io_ctx, const char *name, uint64_t size, int *order);  int create2(IoCtx&amp; io_ctx, const char *name, uint64_t size,\t      uint64_t features, int *order);  int create3(IoCtx&amp; io_ctx, const char *name, uint64_t size,\t      uint64_t features, int *order,\t      uint64_t stripe_unit, uint64_t stripe_count);  int create4(IoCtx&amp; io_ctx, const char *name, uint64_t size,\t      ImageOptions&amp; opts);  int clone(IoCtx&amp; p_ioctx, const char *p_name, const char *p_snapname,\t       IoCtx&amp; c_ioctx, const char *c_name, uint64_t features,\t       int *c_order);  int clone2(IoCtx&amp; p_ioctx, const char *p_name, const char *p_snapname,\t     IoCtx&amp; c_ioctx, const char *c_name, uint64_t features,\t     int *c_order, uint64_t stripe_unit, int stripe_count);  int clone3(IoCtx&amp; p_ioctx, const char *p_name, const char *p_snapname,\t     IoCtx&amp; c_ioctx, const char *c_name, ImageOptions&amp; opts);  int remove(IoCtx&amp; io_ctx, const char *name);  int remove_with_progress(IoCtx&amp; io_ctx, const char *name, ProgressContext&amp; pctx);  int rename(IoCtx&amp; src_io_ctx, const char *srcname, const char *destname);  int trash_move(IoCtx &amp;io_ctx, const char *name, uint64_t delay);  int trash_get(IoCtx &amp;io_ctx, const char *id, trash_image_info_t *info);  int trash_list(IoCtx &amp;io_ctx, std::vector&lt;trash_image_info_t&gt; &amp;entries);  int trash_purge(IoCtx &amp;io_ctx, time_t expire_ts, float threshold);  int trash_purge_with_progress(IoCtx &amp;io_ctx, time_t expire_ts, float threshold,                                ProgressContext &amp;pctx);  int trash_remove(IoCtx &amp;io_ctx, const char *image_id, bool force);  int trash_remove_with_progress(IoCtx &amp;io_ctx, const char *image_id,                                 bool force, ProgressContext &amp;pctx);  int trash_restore(IoCtx &amp;io_ctx, const char *id, const char *name);  // Migration  int migration_prepare(IoCtx&amp; io_ctx, const char *image_name,                        IoCtx&amp; dest_io_ctx, const char *dest_image_name,                        ImageOptions&amp; opts);  int migration_execute(IoCtx&amp; io_ctx, const char *image_name);  int migration_execute_with_progress(IoCtx&amp; io_ctx, const char *image_name,                                      ProgressContext &amp;prog_ctx);  int migration_abort(IoCtx&amp; io_ctx, const char *image_name);  int migration_abort_with_progress(IoCtx&amp; io_ctx, const char *image_name,                                    ProgressContext &amp;prog_ctx);  int migration_commit(IoCtx&amp; io_ctx, const char *image_name);  int migration_commit_with_progress(IoCtx&amp; io_ctx, const char *image_name,                                     ProgressContext &amp;prog_ctx);  int migration_status(IoCtx&amp; io_ctx, const char *image_name,                       image_migration_status_t *status, size_t status_size);  // RBD pool mirroring support functions  int mirror_mode_get(IoCtx&amp; io_ctx, rbd_mirror_mode_t *mirror_mode);  int mirror_mode_set(IoCtx&amp; io_ctx, rbd_mirror_mode_t mirror_mode);  int mirror_peer_add(IoCtx&amp; io_ctx, std::string *uuid,                      const std::string &amp;cluster_name,                      const std::string &amp;client_name);  int mirror_peer_remove(IoCtx&amp; io_ctx, const std::string &amp;uuid);  int mirror_peer_list(IoCtx&amp; io_ctx, std::vector&lt;mirror_peer_t&gt; *peers);  int mirror_peer_set_client(IoCtx&amp; io_ctx, const std::string &amp;uuid,                             const std::string &amp;client_name);  int mirror_peer_set_cluster(IoCtx&amp; io_ctx, const std::string &amp;uuid,                              const std::string &amp;cluster_name);  int mirror_peer_get_attributes(      IoCtx&amp; io_ctx, const std::string &amp;uuid,      std::map&lt;std::string, std::string&gt; *key_vals);  int mirror_peer_set_attributes(      IoCtx&amp; io_ctx, const std::string &amp;uuid,      const std::map&lt;std::string, std::string&gt;&amp; key_vals);  int mirror_image_status_list(IoCtx&amp; io_ctx, const std::string &amp;start_id,      size_t max, std::map&lt;std::string, mirror_image_status_t&gt; *images);  int mirror_image_status_summary(IoCtx&amp; io_ctx,      std::map&lt;mirror_image_status_state_t, int&gt; *states);  int mirror_image_instance_id_list(IoCtx&amp; io_ctx, const std::string &amp;start_id,      size_t max, std::map&lt;std::string, std::string&gt; *sevice_ids);  // RBD groups support functions  int group_create(IoCtx&amp; io_ctx, const char *group_name);  int group_remove(IoCtx&amp; io_ctx, const char *group_name);  int group_list(IoCtx&amp; io_ctx, std::vector&lt;std::string&gt; *names);  int group_rename(IoCtx&amp; io_ctx, const char *src_group_name,                   const char *dest_group_name);  int group_image_add(IoCtx&amp; io_ctx, const char *group_name,\t\t      IoCtx&amp; image_io_ctx, const char *image_name);  int group_image_remove(IoCtx&amp; io_ctx, const char *group_name,\t\t\t IoCtx&amp; image_io_ctx, const char *image_name);  int group_image_remove_by_id(IoCtx&amp; io_ctx, const char *group_name,                               IoCtx&amp; image_io_ctx, const char *image_id);  int group_image_list(IoCtx&amp; io_ctx, const char *group_name,                       std::vector&lt;group_image_info_t&gt; *images,                       size_t group_image_info_size);  int group_snap_create(IoCtx&amp; io_ctx, const char *group_name,\t\t\tconst char *snap_name);  int group_snap_remove(IoCtx&amp; io_ctx, const char *group_name,\t\t\tconst char *snap_name);  int group_snap_rename(IoCtx&amp; group_ioctx, const char *group_name,                        const char *old_snap_name, const char *new_snap_name);  int group_snap_list(IoCtx&amp; group_ioctx, const char *group_name,                      std::vector&lt;group_snap_info_t&gt; *snaps,                      size_t group_snap_info_size);  int group_snap_rollback(IoCtx&amp; io_ctx, const char *group_name,                          const char *snap_name);  int group_snap_rollback_with_progress(IoCtx&amp; io_ctx, const char *group_name,                                        const char *snap_name,                                        ProgressContext&amp; pctx);  int namespace_create(IoCtx&amp; ioctx, const char *namespace_name);  int namespace_remove(IoCtx&amp; ioctx, const char *namespace_name);  int namespace_list(IoCtx&amp; io_ctx, std::vector&lt;std::string&gt;* namespace_names);  int namespace_exists(IoCtx&amp; io_ctx, const char *namespace_name, bool *exists);  int pool_init(IoCtx&amp; io_ctx, bool force);  int pool_stats_get(IoCtx&amp; io_ctx, PoolStats *pool_stats);  int pool_metadata_get(IoCtx &amp;io_ctx, const std::string &amp;key,                        std::string *value);  int pool_metadata_set(IoCtx &amp;io_ctx, const std::string &amp;key,                        const std::string &amp;value);  int pool_metadata_remove(IoCtx &amp;io_ctx, const std::string &amp;key);  int pool_metadata_list(IoCtx &amp;io_ctx, const std::string &amp;start, uint64_t max,                         std::map&lt;std::string, ceph::bufferlist&gt; *pairs);  int config_list(IoCtx&amp; io_ctx, std::vector&lt;config_option_t&gt; *options);private:  /* We don&#x27;t allow assignment or copying */  RBD(const RBD&amp; rhs);  const RBD&amp; operator=(const RBD&amp; rhs);&#125;;\n\nclass CEPH_RBD_API Image\nImage 类负责镜像的读写(read&#x2F;write)，以及快照相关的操作等等。\n同时提供了相关异步操作的接口。\n\nclass CEPH_RBD_API Image&#123;public:  Image();  ~Image();  // 镜像的读写，resize, flush, flatten，trim等操作  int close();  int aio_close(RBD::AioCompletion *c);  int resize(uint64_t size);  int resize2(uint64_t size, bool allow_shrink, ProgressContext&amp; pctx);  int resize_with_progress(uint64_t size, ProgressContext&amp; pctx);  int stat(image_info_t &amp;info, size_t infosize);  int get_name(std::string *name);  int get_id(std::string *id);  std::string get_block_name_prefix();  int64_t get_data_pool_id();  int parent_info(std::string *parent_poolname, std::string *parent_name,\t\t  std::string *parent_snapname)      __attribute__((deprecated));  int parent_info2(std::string *parent_poolname, std::string *parent_name,                   std::string *parent_id, std::string *parent_snapname)      __attribute__((deprecated));  int get_parent(linked_image_spec_t *parent_image, snap_spec_t *parent_snap);  int old_format(uint8_t *old);  int size(uint64_t *size);  int get_group(group_info_t *group_info, size_t group_info_size);  int features(uint64_t *features);  int update_features(uint64_t features, bool enabled);  int get_op_features(uint64_t *op_features);  int overlap(uint64_t *overlap);  int get_flags(uint64_t *flags);  int set_image_notification(int fd, int type);  /* exclusive lock feature */  int is_exclusive_lock_owner(bool *is_owner);  int lock_acquire(rbd_lock_mode_t lock_mode);  int lock_release();  int lock_get_owners(rbd_lock_mode_t *lock_mode,                      std::list&lt;std::string&gt; *lock_owners);  int lock_break(rbd_lock_mode_t lock_mode, const std::string &amp;lock_owner);  /* object map feature */  int rebuild_object_map(ProgressContext &amp;prog_ctx);  int check_object_map(ProgressContext &amp;prog_ctx);  int copy(IoCtx&amp; dest_io_ctx, const char *destname);  int copy2(Image&amp; dest);  int copy3(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts);  int copy4(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts,\t    size_t sparse_size);  int copy_with_progress(IoCtx&amp; dest_io_ctx, const char *destname,\t\t\t ProgressContext &amp;prog_ctx);  int copy_with_progress2(Image&amp; dest, ProgressContext &amp;prog_ctx);  int copy_with_progress3(IoCtx&amp; dest_io_ctx, const char *destname,\t\t\t  ImageOptions&amp; opts, ProgressContext &amp;prog_ctx);  int copy_with_progress4(IoCtx&amp; dest_io_ctx, const char *destname,\t\t\t  ImageOptions&amp; opts, ProgressContext &amp;prog_ctx,\t\t\t  size_t sparse_size);  /* deep copy */  int deep_copy(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts);  int deep_copy_with_progress(IoCtx&amp; dest_io_ctx, const char *destname,                              ImageOptions&amp; opts, ProgressContext &amp;prog_ctx);  /* striping */  uint64_t get_stripe_unit() const;  uint64_t get_stripe_count() const;  int get_create_timestamp(struct timespec *timestamp);  int get_access_timestamp(struct timespec *timestamp);  int get_modify_timestamp(struct timespec *timestamp);  int flatten();  int flatten_with_progress(ProgressContext &amp;prog_ctx);  int sparsify(size_t sparse_size);  int sparsify_with_progress(size_t sparse_size, ProgressContext &amp;prog_ctx);  /**   * Returns a pair of poolname, imagename for each clone   * of this image at the currently set snapshot.   */  int list_children(std::set&lt;std::pair&lt;std::string, std::string&gt; &gt; *children)      __attribute__((deprecated));  /**  * Returns a structure of poolname, imagename, imageid and trash flag  * for each clone of this image at the currently set snapshot.  */  int list_children2(std::vector&lt;librbd::child_info_t&gt; *children)      __attribute__((deprecated));  int list_children3(std::vector&lt;linked_image_spec_t&gt; *images);  int list_descendants(std::vector&lt;linked_image_spec_t&gt; *images);  /* advisory locking (see librbd.h for details) */  int list_lockers(std::list&lt;locker_t&gt; *lockers,\t\t   bool *exclusive, std::string *tag);  int lock_exclusive(const std::string&amp; cookie);  int lock_shared(const std::string&amp; cookie, const std::string&amp; tag);  int unlock(const std::string&amp; cookie);  int break_lock(const std::string&amp; client, const std::string&amp; cookie);  /* snapshots */  int snap_list(std::vector&lt;snap_info_t&gt;&amp; snaps);  /* DEPRECATED; use snap_exists2 */  bool snap_exists(const char *snapname) __attribute__ ((deprecated));  int snap_exists2(const char *snapname, bool *exists);  int snap_create(const char *snapname);  int snap_remove(const char *snapname);  int snap_remove2(const char *snapname, uint32_t flags, ProgressContext&amp; pctx);  int snap_remove_by_id(uint64_t snap_id);  int snap_rollback(const char *snap_name);  int snap_rollback_with_progress(const char *snap_name, ProgressContext&amp; pctx);  int snap_protect(const char *snap_name);  int snap_unprotect(const char *snap_name);  int snap_is_protected(const char *snap_name, bool *is_protected);  int snap_set(const char *snap_name);  int snap_set_by_id(uint64_t snap_id);  int snap_rename(const char *srcname, const char *dstname);  int snap_get_limit(uint64_t *limit);  int snap_set_limit(uint64_t limit);  int snap_get_timestamp(uint64_t snap_id, struct timespec *timestamp);  int snap_get_namespace_type(uint64_t snap_id,                              snap_namespace_type_t *namespace_type);  int snap_get_group_namespace(uint64_t snap_id,                               snap_group_namespace_t *group_namespace,                               size_t snap_group_namespace_size);  int snap_get_trash_namespace(uint64_t snap_id, std::string* original_name);  /* I/O */  ssize_t read(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl);  /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */  ssize_t read2(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl, int op_flags);  int64_t read_iterate(uint64_t ofs, size_t len,\t\t       int (*cb)(uint64_t, size_t, const char *, void *), void *arg);  int read_iterate2(uint64_t ofs, uint64_t len,\t\t    int (*cb)(uint64_t, size_t, const char *, void *), void *arg);  /**   * get difference between two versions of an image   *   * This will return the differences between two versions of an image   * via a callback, which gets the offset and length and a flag   * indicating whether the extent exists (1), or is known/defined to   * be zeros (a hole, 0).  If the source snapshot name is NULL, we   * interpret that as the beginning of time and return all allocated   * regions of the image.  The end version is whatever is currently   * selected for the image handle (either a snapshot or the writeable   * head).   *   * @param fromsnapname start snapshot name, or NULL   * @param ofs start offset   * @param len len in bytes of region to report on   * @param include_parent true if full history diff should include parent   * @param whole_object 1 if diff extents should cover whole object   * @param cb callback to call for each allocated region   * @param arg argument to pass to the callback   * @returns 0 on success, or negative error code on error   */  int diff_iterate(const char *fromsnapname,\t\t   uint64_t ofs, uint64_t len,\t\t   int (*cb)(uint64_t, size_t, int, void *), void *arg);  int diff_iterate2(const char *fromsnapname,\t\t    uint64_t ofs, uint64_t len,                    bool include_parent, bool whole_object,\t\t    int (*cb)(uint64_t, size_t, int, void *), void *arg);  ssize_t write(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl);  /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */  ssize_t write2(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl, int op_flags);  int discard(uint64_t ofs, uint64_t len);  ssize_t writesame(uint64_t ofs, size_t len, ceph::bufferlist &amp;bl, int op_flags);  ssize_t compare_and_write(uint64_t ofs, size_t len, ceph::bufferlist &amp;cmp_bl,                            ceph::bufferlist&amp; bl, uint64_t *mismatch_off, int op_flags);  int aio_write(uint64_t off, size_t len, ceph::bufferlist&amp; bl, RBD::AioCompletion *c);  /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */  int aio_write2(uint64_t off, size_t len, ceph::bufferlist&amp; bl,\t\t  RBD::AioCompletion *c, int op_flags);  int aio_writesame(uint64_t off, size_t len, ceph::bufferlist&amp; bl,                    RBD::AioCompletion *c, int op_flags);  int aio_compare_and_write(uint64_t off, size_t len, ceph::bufferlist&amp; cmp_bl,                            ceph::bufferlist&amp; bl, RBD::AioCompletion *c,                            uint64_t *mismatch_off, int op_flags);  /**   * read async from image   *   * The target bufferlist is populated with references to buffers   * that contain the data for the given extent of the image.   *   * NOTE: If caching is enabled, the bufferlist will directly   * reference buffers in the cache to avoid an unnecessary data copy.   * As a result, if the user intends to modify the buffer contents   * directly, they should make a copy first (unconditionally, or when   * the reference count on ther underlying buffer is more than 1).   *   * @param off offset in image   * @param len length of read   * @param bl bufferlist to read into   * @param c aio completion to notify when read is complete   */  int aio_read(uint64_t off, size_t len, ceph::bufferlist&amp; bl, RBD::AioCompletion *c);  /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */  int aio_read2(uint64_t off, size_t len, ceph::bufferlist&amp; bl,\t\t  RBD::AioCompletion *c, int op_flags);  int aio_discard(uint64_t off, uint64_t len, RBD::AioCompletion *c);  int flush();  /**   * Start a flush if caching is enabled. Get a callback when   * the currently pending writes are on disk.   *   * @param image the image to flush writes to   * @param c what to call when flushing is complete   * @returns 0 on success, negative error code on failure   */  int aio_flush(RBD::AioCompletion *c);  /**   * Drop any cached data for this image   *   * @returns 0 on success, negative error code on failure   */  int invalidate_cache();  int poll_io_events(RBD::AioCompletion **comps, int numcomp);  int metadata_get(const std::string &amp;key, std::string *value);  int metadata_set(const std::string &amp;key, const std::string &amp;value);  int metadata_remove(const std::string &amp;key);  /**   * Returns a pair of key/value for this image   */  int metadata_list(const std::string &amp;start, uint64_t max, std::map&lt;std::string, ceph::bufferlist&gt; *pairs);  // RBD image mirroring support functions  int mirror_image_enable();  int mirror_image_disable(bool force);  int mirror_image_promote(bool force);  int mirror_image_demote();  int mirror_image_resync();  int mirror_image_get_info(mirror_image_info_t *mirror_image_info,                            size_t info_size);  int mirror_image_get_status(mirror_image_status_t *mirror_image_status,\t\t\t      size_t status_size);  int mirror_image_get_instance_id(std::string *instance_id);  int aio_mirror_image_promote(bool force, RBD::AioCompletion *c);  int aio_mirror_image_demote(RBD::AioCompletion *c);  int aio_mirror_image_get_info(mirror_image_info_t *mirror_image_info,                                size_t info_size, RBD::AioCompletion *c);  int aio_mirror_image_get_status(mirror_image_status_t *mirror_image_status,                                  size_t status_size, RBD::AioCompletion *c);  int update_watch(UpdateWatchCtx *ctx, uint64_t *handle);  int update_unwatch(uint64_t handle);  int list_watchers(std::list&lt;image_watcher_t&gt; &amp;watchers);  int config_list(std::vector&lt;config_option_t&gt; *options);private:  friend class RBD;  Image(const Image&amp; rhs);  const Image&amp; operator=(const Image&amp; rhs);  image_ctx_t ctx; // void*, 实际指向具体实现的类&#125;;\n\n具体实现\nlibrbd.cc 主要实现了 I&#x2F;O 相关接口 read&#x2F;write。\ninternal.cc 主要实现了定义在头文件中的相关函数接口\n\nCls\ncls_rbd是Cls的一个扩展模块，Cls允许用户自定义对象的操作接口和实现方法，为用户提供了一种比较直接的接口扩展方式。通过动态链接的形式加入 osd 中，在 osd 上直接执行。\n\nClient\ncls_rbd_client.h&#x2F;cc 该文件中主要定义了客户端上运行的接口，将函数参数封装后发送给服务端 OSD ，然后做后续处理.\ncls_rbd_client.h/cc 定义了通过客户端访问osd注册的cls函数的方法。以 snapshot_add 函数和 create_image 函数为例，这个函数将参数封装进 bufferlist ，通过 ioctx-&gt;exec 方法，把操作发送给osd处理。\n\nvoid snapshot_add(librados::ObjectWriteOperation *op, snapid_t snap_id,              const std::string &amp;snap_name, const cls::rbd::SnapshotNamespace &amp;snap_namespace)&#123;  bufferlist bl;  ::encode(snap_name, bl);  ::encode(snap_id, bl);  ::encode(cls::rbd::SnapshotNamespaceOnDisk(snap_namespace), bl);  op-&gt;exec(&quot;rbd&quot;, &quot;snapshot_add&quot;, bl);&#125;void create_image(librados::ObjectWriteOperation *op, uint64_t size,                  uint8_t order, uint64_t features,                  const std::string &amp;object_prefix, int64_t data_pool_id)&#123;  bufferlist bl;  ::encode(size, bl);  ::encode(order, bl);  ::encode(features, bl);  ::encode(object_prefix, bl);  ::encode(data_pool_id, bl);  op-&gt;exec(&quot;rbd&quot;, &quot;create&quot;, bl);&#125;int create_image(librados::IoCtx *ioctx, const std::string &amp;oid,                 uint64_t size, uint8_t order, uint64_t features,                 const std::string &amp;object_prefix, int64_t data_pool_id)&#123;  librados::ObjectWriteOperation op;  create_image(&amp;op, size, order, features, object_prefix, data_pool_id);  return ioctx-&gt;operate(oid, &amp;op);&#125;\n\nServer\ncls_rbd.h&#x2F;cc 该类中主要定义了服务端上（OSD）执行的函数，响应客户端的请求。在 cls_rbd.cc 函数中，对函数进行定义和注册。\n例如，下面的代码注册了rbd模块，以及 snapshot_add 和 create 函数。\n\ncls_register(&quot;rbd&quot;, &amp;h_class);cls_register_cxx_method(h_class, &quot;snapshot_add&quot;,            CLS_METHOD_RD | CLS_METHOD_WR,            snapshot_add, &amp;h_snapshot_add);            cls_register_cxx_method(h_class, &quot;create&quot;,\t  CLS_METHOD_RD | CLS_METHOD_WR,\t  create, &amp;h_create);\n\n\ncls_rbd.cc定义了方法在服务端的实现，其一般流程是：从bufferlist将客户端传入的参数解析出来，调用对应的方法实现，然后将结果返回客户端。\n\n/** * Adds a snapshot to an rbd header. Ensures the id and name are unique. */int snapshot_add(cls_method_context_t hctx, bufferlist *in, bufferlist *out)&#123;  bufferlist snap_namebl, snap_idbl;  cls_rbd_snap snap_meta;  uint64_t snap_limit;  // 从bl中解析参数  try &#123;    bufferlist::iterator iter = in-&gt;begin();    ::decode(snap_meta.name, iter);    ::decode(snap_meta.id, iter);    if (!iter.end()) &#123;      ::decode(snap_meta.snapshot_namespace, iter);    &#125;  &#125; catch (const buffer::error &amp;err) &#123;    return -EINVAL;  &#125;  // 判断参数合法性，略  ......  // 完成操作，在rbd_header对象中增加新的snapshot元数据，并更新sanp_seq。  map&lt;string, bufferlist&gt; vals;  vals[&quot;snap_seq&quot;] = snap_seqbl;  vals[snapshot_key] = snap_metabl;  r = cls_cxx_map_set_vals(hctx, &amp;vals);  if (r &lt; 0) &#123;    CLS_ERR(&quot;error writing snapshot metadata: %s&quot;, cpp_strerror(r).c_str());    return r;  &#125;  return 0;&#125;/** * Initialize the header with basic metadata. * Extra features may initialize more fields in the future. * Everything is stored as key/value pairs as omaps in the header object. * * If features the OSD does not understand are requested, -ENOSYS is * returned. * * Input: * @param size number of bytes in the image (uint64_t) * @param order bits to shift to determine the size of data objects (uint8_t) * @param features what optional things this image will use (uint64_t) * @param object_prefix a prefix for all the data objects * @param data_pool_id pool id where data objects is stored (int64_t) * * Output: * @return 0 on success, negative error code on failure */int create(cls_method_context_t hctx, bufferlist *in, bufferlist *out)&#123;  string object_prefix;  uint64_t features, size;  uint8_t order;  int64_t data_pool_id = -1;  // 从 buffer 里解析参数  try &#123;    auto iter = in-&gt;cbegin();    decode(size, iter);    decode(order, iter);    decode(features, iter);    decode(object_prefix, iter);    if (!iter.end()) &#123;      decode(data_pool_id, iter);    &#125;  &#125; catch (const buffer::error &amp;err) &#123;    return -EINVAL;  &#125;  CLS_LOG(20, &quot;create object_prefix=%s size=%llu order=%u features=%llu&quot;,\t  object_prefix.c_str(), (unsigned long long)size, order,\t  (unsigned long long)features);  if (features &amp; ~RBD_FEATURES_ALL) &#123;    return -ENOSYS;  &#125;  if (!object_prefix.size()) &#123;    return -EINVAL;  &#125;  bufferlist stored_prefixbl;    // 从 cls context 里获取 object_prefix  int r = cls_cxx_map_get_val(hctx, &quot;object_prefix&quot;, &amp;stored_prefixbl);  if (r != -ENOENT) &#123;    CLS_ERR(&quot;reading object_prefix returned %d&quot;, r);    return -EEXIST;  &#125;  bufferlist sizebl;  bufferlist orderbl;  bufferlist featuresbl;  bufferlist object_prefixbl;  bufferlist snap_seqbl;  bufferlist timestampbl;  uint64_t snap_seq = 0;  utime_t timestamp = ceph_clock_now();  encode(size, sizebl);  encode(order, orderbl);  encode(features, featuresbl);  encode(object_prefix, object_prefixbl);  encode(snap_seq, snap_seqbl);  encode(timestamp, timestampbl);  // 更新 rbd_header omap   map&lt;string, bufferlist&gt; omap_vals;  omap_vals[&quot;size&quot;] = sizebl;  omap_vals[&quot;order&quot;] = orderbl;  omap_vals[&quot;features&quot;] = featuresbl;  omap_vals[&quot;object_prefix&quot;] = object_prefixbl;  omap_vals[&quot;snap_seq&quot;] = snap_seqbl;  omap_vals[&quot;create_timestamp&quot;] = timestampbl;  omap_vals[&quot;access_timestamp&quot;] = timestampbl;  omap_vals[&quot;modify_timestamp&quot;] = timestampbl;  if ((features &amp; RBD_FEATURE_OPERATIONS) != 0ULL) &#123;    CLS_ERR(&quot;Attempting to set internal feature: operations&quot;);    return -EINVAL;  &#125;  if (features &amp; RBD_FEATURE_DATA_POOL) &#123;    if (data_pool_id == -1) &#123;      CLS_ERR(&quot;data pool not provided with feature enabled&quot;);      return -EINVAL;    &#125;    bufferlist data_pool_id_bl;    encode(data_pool_id, data_pool_id_bl);    omap_vals[&quot;data_pool_id&quot;] = data_pool_id_bl;  &#125; else if (data_pool_id != -1) &#123;    CLS_ERR(&quot;data pool provided with feature disabled&quot;);    return -EINVAL;  &#125;  // 更新 OMAP  r = cls_cxx_map_set_vals(hctx, &amp;omap_vals);  if (r &lt; 0)    return r;  return 0;&#125;\n\nOthers\n分片 Striper\nCeph RBD 默认分片到了许多对象上，这些对象最终会存储在 RADOS 中，对 RBD Image 的读写请求会分布在集群中的很多个节点上，从而避免当 RBD Image 特别大或者繁忙的时候，单个节点不会成为瓶颈。\nCeph RBD 的分片由三个参数控制\nobject-size，代码中常常简写为 os，通常为 2 的幂指数，默认的对象大小是4 MB，最小的是4K，最大的是32M\nstripe_unit，代码中常常简写为 su，一个对象中存储了连续该大小的分片。默认和对象大小相等。\nstripe_count，代码中常常简写为 sc，在向 [stripe_count] 对象写入 [stripe_unit] 字节后，循环到初始对象并写入另一个条带，直到对象达到其最大大小。此时，将继续处理下一个 [stripe_count] 对象。\n\n\n分片的组织形式类似于 RAID0，看一个来自官网的 例子。此处只举例一个 Object Set，对象大小在如下图示中即为纵向的一个对象大小，由整数个分片单元组成，而一个分片是指指定 stripe_count 个 stripe_unit，图示中的 stripe_count 即为 5，假设 stripe_unit 为 64KB，那么对应的分片大小即为 320KB，对象大小如果设置为 64GB，那么一个对象对应的就会有 64GB&#x2F;64KB &#x3D; 1048576 个 stripe_unit.\n\n   _________   _________   _________   _________   _________  /object  0\\ /object  1\\ /object  2\\ /object  3\\ /object  4\\  +=========+ +=========+ +=========+ +=========+ +=========+  |  stripe | |  stripe | |  stripe | |  stripe | |  stripe |o |   unit  | |   unit  | |   unit  | |   unit  | |   unit  | stripe 0b |     0   | |     1   | |     2   | |     3   | |     4   |j |---------| |---------| |---------| |---------| |---------|e |  stripe | |  stripe | |  stripe | |  stripe | |  stripe |c |   unit  | |   unit  | |   unit  | |   unit  | |   unit  | stripe 1t |     5   | |     6   | |     7   | |     8   | |     9   |  |---------| |---------| |---------| |---------| |---------|s |     .   | |     .   | |     .   | |     .   | |     .   |e       .           .           .           .           .t |     .   | |     .   | |     .   | |     .   | |     .   |  |---------| |---------| |---------| |---------| |---------|0 |  stripe | |  stripe | |  stripe | |  stripe | |  stripe | stripe  |   unit  | |   unit  | |   unit  | |   unit  | |   unit  | 1048575  | 5242875 | | 5242876 | | 5242877 | | 5242878 | | 5242879 |  \\=========/ \\=========/ \\=========/ \\=========/ \\=========/\n\n\nceph&#x2F;src&#x2F;osdc&#x2F;Striper.h\n\n  class Striper &#123;  public:    static void file_to_extents(        CephContext *cct, const file_layout_t *layout, uint64_t offset,        uint64_t len, uint64_t trunc_size, uint64_t buffer_offset,        striper::LightweightObjectExtents* object_extents);    /*     * std::map (ino, layout, offset, len) to a (list of) ObjectExtents (byte     * ranges in objects on (primary) osds)     */    static void file_to_extents(CephContext *cct, const char *object_format,\t\t\t\tconst file_layout_t *layout,\t\t\t\tuint64_t offset, uint64_t len,\t\t\t\tuint64_t trunc_size,\t\t\t\tstd::map&lt;object_t, std::vector&lt;ObjectExtent&gt; &gt;&amp; extents,\t\t\t\tuint64_t buffer_offset=0);...\n\n\n分片大小对应的数据结构：\n\nstruct file_layout_t &#123;  // file -&gt; object mapping  uint32_t stripe_unit;   ///&lt; stripe unit, in bytes,  uint32_t stripe_count;  ///&lt; over this many objects  uint32_t object_size;   ///&lt; until objects are this big  int64_t pool_id;        ///&lt; rados pool id  std::string pool_ns;         ///&lt; rados pool namespace  file_layout_t(uint32_t su=0, uint32_t sc=0, uint32_t os=0)    : stripe_unit(su),      stripe_count(sc),      object_size(os),      pool_id(-1) &#123;  &#125;  // 默认分片大小 4MB  static file_layout_t get_default() &#123;    return file_layout_t(1&lt;&lt;22, 1, 1&lt;&lt;22);  &#125;  uint64_t get_period() const &#123;    return static_cast&lt;uint64_t&gt;(stripe_count) * object_size;  &#125;  void from_legacy(const ceph_file_layout&amp; fl);  void to_legacy(ceph_file_layout *fl) const;  bool is_valid() const;  void encode(ceph::buffer::list&amp; bl, uint64_t features) const;  void decode(ceph::buffer::list::const_iterator&amp; p);  void dump(ceph::Formatter *f) const;  void decode_json(JSONObj *obj);  static void generate_test_instances(std::list&lt;file_layout_t*&gt;&amp; o);&#125;;\n\n\n查看分片类对应的实现：ceph&#x2F;src&#x2F;osdc&#x2F;Striper.cc\n\nvoid Striper::file_to_extents(CephContext *cct, const char *object_format,\t\t\t      const file_layout_t *layout,\t\t\t      uint64_t offset, uint64_t len,\t\t\t      uint64_t trunc_size,\t\t\t      std::vector&lt;ObjectExtent&gt;&amp; extents,\t\t\t      uint64_t buffer_offset)&#123;  striper::LightweightObjectExtents lightweight_object_extents;  file_to_extents(cct, layout, offset, len, trunc_size, buffer_offset,                  &amp;lightweight_object_extents);  // convert lightweight object extents to heavyweight version  extents.reserve(lightweight_object_extents.size());  for (auto&amp; lightweight_object_extent : lightweight_object_extents) &#123;    auto&amp; object_extent = extents.emplace_back(      object_t(format_oid(object_format, lightweight_object_extent.object_no)),      lightweight_object_extent.object_no,      lightweight_object_extent.offset, lightweight_object_extent.length,      lightweight_object_extent.truncate_size);    object_extent.oloc = OSDMap::file_to_object_locator(*layout);    object_extent.buffer_extents.reserve(      lightweight_object_extent.buffer_extents.size());    object_extent.buffer_extents.insert(      object_extent.buffer_extents.end(),      lightweight_object_extent.buffer_extents.begin(),      lightweight_object_extent.buffer_extents.end());  &#125;&#125;void Striper::file_to_extents(    CephContext *cct, const file_layout_t *layout, uint64_t offset,    uint64_t len, uint64_t trunc_size, uint64_t buffer_offset,    striper::LightweightObjectExtents* object_extents) &#123;  ldout(cct, 10) &lt;&lt; &quot;file_to_extents &quot; &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; len &lt;&lt; dendl;  ceph_assert(len &gt; 0);  /*   * we want only one extent per object!  this means that each extent   * we read may map into different bits of the final read   * buffer.. hence buffer_extents   */  __u32 object_size = layout-&gt;object_size;  __u32 su = layout-&gt;stripe_unit;  __u32 stripe_count = layout-&gt;stripe_count;  ceph_assert(object_size &gt;= su);  if (stripe_count == 1) &#123;    ldout(cct, 20) &lt;&lt; &quot; sc is one, reset su to os&quot; &lt;&lt; dendl;    su = object_size;  &#125;  uint64_t stripes_per_object = object_size / su;  ldout(cct, 20) &lt;&lt; &quot; su &quot; &lt;&lt; su &lt;&lt; &quot; sc &quot; &lt;&lt; stripe_count &lt;&lt; &quot; os &quot;\t\t &lt;&lt; object_size &lt;&lt; &quot; stripes_per_object &quot; &lt;&lt; stripes_per_object\t\t &lt;&lt; dendl;  uint64_t cur = offset;  uint64_t left = len;  while (left &gt; 0) &#123;    // layout into objects    uint64_t blockno = cur / su; // which block    // which horizontal stripe (Y)    uint64_t stripeno = blockno / stripe_count;    // which object in the object set (X)    uint64_t stripepos = blockno % stripe_count;    // which object set    uint64_t objectsetno = stripeno / stripes_per_object;    // object id    uint64_t objectno = objectsetno * stripe_count + stripepos;    // map range into object    uint64_t block_start = (stripeno % stripes_per_object) * su;    uint64_t block_off = cur % su;    uint64_t max = su - block_off;    uint64_t x_offset = block_start + block_off;    uint64_t x_len;    if (left &gt; max)      x_len = max;    else      x_len = left;    ldout(cct, 20) &lt;&lt; &quot; off &quot; &lt;&lt; cur &lt;&lt; &quot; blockno &quot; &lt;&lt; blockno &lt;&lt; &quot; stripeno &quot;\t\t   &lt;&lt; stripeno &lt;&lt; &quot; stripepos &quot; &lt;&lt; stripepos &lt;&lt; &quot; objectsetno &quot;\t\t   &lt;&lt; objectsetno &lt;&lt; &quot; objectno &quot; &lt;&lt; objectno\t\t   &lt;&lt; &quot; block_start &quot; &lt;&lt; block_start &lt;&lt; &quot; block_off &quot;\t\t   &lt;&lt; block_off &lt;&lt; &quot; &quot; &lt;&lt; x_offset &lt;&lt; &quot;~&quot; &lt;&lt; x_len\t\t   &lt;&lt; dendl;    striper::LightweightObjectExtent* ex = nullptr;    auto it = std::upper_bound(object_extents-&gt;begin(), object_extents-&gt;end(),                               objectno, OrderByObject());    striper::LightweightObjectExtents::reverse_iterator rev_it(it);    if (rev_it == object_extents-&gt;rend() ||        rev_it-&gt;object_no != objectno ||        rev_it-&gt;offset + rev_it-&gt;length != x_offset) &#123;      // expect up to &quot;stripe-width - 1&quot; vector shifts in the worst-case      ex = &amp;(*object_extents-&gt;emplace(        it, objectno, x_offset, x_len,        object_truncate_size(cct, layout, objectno, trunc_size)));        ldout(cct, 20) &lt;&lt; &quot; added new &quot; &lt;&lt; *ex &lt;&lt; dendl;    &#125; else &#123;      ex = &amp;(*rev_it);      ceph_assert(ex-&gt;offset + ex-&gt;length == x_offset);      ldout(cct, 20) &lt;&lt; &quot; adding in to &quot; &lt;&lt; *ex &lt;&lt; dendl;      ex-&gt;length += x_len;    &#125;    ex-&gt;buffer_extents.emplace_back(cur - offset + buffer_offset, x_len);    ldout(cct, 15) &lt;&lt; &quot;file_to_extents  &quot; &lt;&lt; *ex &lt;&lt; dendl;    // ldout(cct, 0) &lt;&lt; &quot;map: ino &quot; &lt;&lt; ino &lt;&lt; &quot; oid &quot; &lt;&lt; ex.oid &lt;&lt; &quot; osd &quot;    //\t\t  &lt;&lt; ex.osd &lt;&lt; &quot; offset &quot; &lt;&lt; ex.offset &lt;&lt; &quot; len &quot; &lt;&lt; ex.len    //\t\t  &lt;&lt; &quot; ... left &quot; &lt;&lt; left &lt;&lt; dendl;    left -= x_len;    cur += x_len;  &#125;&#125;\n\n数据 IO\n主要对应于 ImageCtx::io_work_queue 成员变量。librbd::io::ImageRequestWQ 派生自 ThreadPool::PointerWQ（&lt;= Luminous） / ThreadPool::PointerWQ（&gt;= Mimic）。\nlibrbd 支持两种类型的 aio，一种是普通的 aio，一种是非阻塞 aio。前者的行为相对简单，直接在用户线程的上下文进行 io 处理，而后者将用户的 io 直接入 io_work_queue 队列，然后 io 由队列的工作线程出队并在工作线程上下文进行后续的处理。这两种 aio 的行为由配置参数 rbd_non_blocking_aio 决定，默认为 true，因此默认为非阻塞 aio，但需要注意的是，即使默认不是非阻塞 aio，在某些场景下 aio 仍然会需要入 io_work_queue 队列，总结如下:\n\nread\nImageRequestWQ::writes_blocked() 为 true，即已调用 ImageRequestWQ::block_writes，当前已禁止 write io 下发至 rados 层；\nImageRequestWQ::writes_empty() 为 false，即前面已经有 write io 入了 io_work_queue 队列；\nImageRequestWQ::require_lock_on_read() 为 true，这里的 lock 是指 exclusive lock，表示当前还未拿到，在启用 exclusive lock 特性的前提下，一旦开启克隆 COR (copy on read) 或者启用 journaling 特性，处理 read io 也要求拿锁；\n\nwrite\nImageRequestWQ::writes_blocked() 为 true，即已调用 ImageRequestWQ::block_writes，当前已禁止 write io 下发至 rados 层；\n对于 write 而言，并没有类似 ImageRequestWQ::require_lock_on_write 的接口，这是因为一旦启用 exclusive lock 特性，在初始化 exclusive lock 时会调用 ImageRequestWQ::block_writes（参考 ExclusiveLock::init），直至拿到锁（参考 ExclusiveLock::handle_post_acquired_lock），因此增加 ImageRequestWQ::require_lock_on_write 接口并没有必要。\n需要注意的是，ImageRequestWQ::block_writes 并不只是简单的设置禁止标志，还需要 flush 已下发的 rados io，即等待所有已下发的 rados io 结束才返回。\n从上面对 read、write 的分析，似乎 ImageRequestWQ::writes_blocked 改成 ImageRequestWQ::io_blocked 似乎更合理，但实际上这里并没有真的禁止 read io 下发至 rados 层，只是让 read io 先入 io_work_queue 队列。\n\nTCMU-RBDstatic void tcmu_rbd_image_close(struct tcmu_device *dev)&#123;\tstruct tcmu_rbd_state *state = tcmur_dev_get_private(dev);\trbd_close(state-&gt;image);\trados_ioctx_destroy(state-&gt;io_ctx);\trados_shutdown(state-&gt;cluster);\tstate-&gt;cluster = NULL;\tstate-&gt;io_ctx = NULL;\tstate-&gt;image = NULL;&#125;static int tcmu_rbd_image_open(struct tcmu_device *dev)&#123;\tstruct tcmu_rbd_state *state = tcmur_dev_get_private(dev);\tint ret;\tret = rados_create(&amp;state-&gt;cluster, state-&gt;id);\tif (ret &lt; 0) &#123;\t\ttcmu_dev_err(dev, &quot;Could not create cluster. (Err %d)\\n&quot;, ret);\t\treturn ret;\t&#125;\t/* Try default location when conf_path=NULL, but ignore failure */\tret = rados_conf_read_file(state-&gt;cluster, state-&gt;conf_path);\tif (state-&gt;conf_path &amp;&amp; ret &lt; 0) &#123;\t\ttcmu_dev_err(dev, &quot;Could not read config %s (Err %d)&quot;,\t\t\t     state-&gt;conf_path, ret);\t\tgoto rados_shutdown;\t&#125;\trados_conf_set(state-&gt;cluster, &quot;rbd_cache&quot;, &quot;false&quot;);\tret = timer_check_and_set_def(dev);\tif (ret)\t\ttcmu_dev_warn(dev,\t\t\t      &quot;Could not set rados osd op timeout to %s (Err %d. Failover may be delayed.)\\n&quot;,\t\t\t      state-&gt;osd_op_timeout, ret);\tret = rados_connect(state-&gt;cluster);\tif (ret &lt; 0) &#123;\t\ttcmu_dev_err(dev, &quot;Could not connect to cluster. (Err %d)\\n&quot;,\t\t\t     ret);\t\tgoto rados_shutdown;\t&#125;\ttcmu_rbd_detect_device_class(dev);\tret = rados_ioctx_create(state-&gt;cluster, state-&gt;pool_name,\t\t\t\t &amp;state-&gt;io_ctx);\tif (ret &lt; 0) &#123;\t\ttcmu_dev_err(dev, &quot;Could not create ioctx for pool %s. (Err %d)\\n&quot;,\t\t\t     state-&gt;pool_name, ret);\t\tgoto rados_shutdown;\t&#125;\tret = rbd_open(state-&gt;io_ctx, state-&gt;image_name, &amp;state-&gt;image, NULL);\tif (ret &lt; 0) &#123;\t\ttcmu_dev_err(dev, &quot;Could not open image %s. (Err %d)\\n&quot;,\t\t\t     state-&gt;image_name, ret);\t\tgoto rados_destroy;\t&#125;\tret = tcmu_rbd_service_register(dev);\tif (ret &lt; 0)\t\tgoto rbd_close;\treturn 0;rbd_close:\trbd_close(state-&gt;image);\tstate-&gt;image = NULL;rados_destroy:\trados_ioctx_destroy(state-&gt;io_ctx);\tstate-&gt;io_ctx = NULL;rados_shutdown:\trados_shutdown(state-&gt;cluster);\tstate-&gt;cluster = NULL;\treturn ret;&#125;static int tcmu_rbd_open(struct tcmu_device *dev, bool reopen)&#123;\trbd_image_info_t image_info;\tchar *pool, *name, *next_opt;\tchar *config, *dev_cfg_dup;\tstruct tcmu_rbd_state *state;\tuint32_t max_blocks;\tint ret;\tstate = calloc(1, sizeof(*state));\tif (!state)\t\treturn -ENOMEM;\ttcmur_dev_set_private(dev, state);\tdev_cfg_dup = strdup(tcmu_dev_get_cfgstring(dev));\tconfig = dev_cfg_dup;\tif (!config) &#123;\t\tret = -ENOMEM;\t\tgoto free_state;\t&#125;\ttcmu_dev_dbg(dev, &quot;tcmu_rbd_open config %s block size %u num lbas %&quot; PRIu64 &quot;.\\n&quot;,\t\t     config, tcmu_dev_get_block_size(dev),\t\t     tcmu_dev_get_num_lbas(dev));\tconfig = strchr(config, &#x27;/&#x27;);\tif (!config) &#123;\t\ttcmu_dev_err(dev, &quot;no configuration found in cfgstring\\n&quot;);\t\tret = -EINVAL;\t\tgoto free_config;\t&#125;\tconfig += 1; /* get past &#x27;/&#x27; */\tpool = strtok(config, &quot;/&quot;);\tif (!pool) &#123;\t\ttcmu_dev_err(dev, &quot;Could not get pool name\\n&quot;);\t\tret = -EINVAL;\t\tgoto free_config;\t&#125;\tstate-&gt;pool_name = strdup(pool);\tif (!state-&gt;pool_name) &#123;\t\tret = -ENOMEM;\t\ttcmu_dev_err(dev, &quot;Could not copy pool name\\n&quot;);\t\tgoto free_config;\t&#125;\tname = strtok(NULL, &quot;;&quot;);\tif (!name) &#123;\t\ttcmu_dev_err(dev, &quot;Could not get image name\\n&quot;);\t\tret = -EINVAL;\t\tgoto free_config;\t&#125;\tstate-&gt;image_name = strdup(name);\tif (!state-&gt;image_name) &#123;\t\tret = -ENOMEM;\t\ttcmu_dev_err(dev, &quot;Could not copy image name\\n&quot;);\t\tgoto free_config;\t&#125;\t/* The next options are optional */\tnext_opt = strtok(NULL, &quot;;&quot;);\twhile (next_opt) &#123;\t\tif (!strncmp(next_opt, &quot;osd_op_timeout=&quot;, 15)) &#123;\t\t\tstate-&gt;osd_op_timeout = strdup(next_opt + 15);\t\t\tif (!state-&gt;osd_op_timeout ||\t\t\t    !strlen(state-&gt;osd_op_timeout)) &#123;\t\t\t\tret = -ENOMEM;\t\t\t\ttcmu_dev_err(dev, &quot;Could not copy osd op timeout.\\n&quot;);\t\t\t\tgoto free_config;\t\t\t&#125;\t\t&#125; else if (!strncmp(next_opt, &quot;conf=&quot;, 5)) &#123;\t\t\tstate-&gt;conf_path = strdup(next_opt + 5);\t\t\tif (!state-&gt;conf_path || !strlen(state-&gt;conf_path)) &#123;\t\t\t\tret = -ENOMEM;\t\t\t\ttcmu_dev_err(dev, &quot;Could not copy conf path.\\n&quot;);\t\t\t\tgoto free_config;\t\t\t&#125;\t\t&#125; else if (!strncmp(next_opt, &quot;id=&quot;, 3)) &#123;\t\t\tstate-&gt;id = strdup(next_opt + 3);\t\t\tif (!state-&gt;id || !strlen(state-&gt;id)) &#123;\t\t\t\tret = -ENOMEM;\t\t\t\ttcmu_dev_err(dev, &quot;Could not copy id.\\n&quot;);\t\t\t\tgoto free_config;\t\t\t&#125;\t\t&#125;\t\tnext_opt = strtok(NULL, &quot;;&quot;);\t&#125;\tret = tcmu_rbd_image_open(dev);\tif (ret &lt; 0) &#123;\t\tgoto free_config;\t&#125;\ttcmu_rbd_check_excl_lock_enabled(dev);\tret = tcmu_rbd_check_image_size(dev, tcmu_dev_get_block_size(dev) *\t\t\t\t\ttcmu_dev_get_num_lbas(dev));\tif (ret) &#123;\t\tgoto stop_image;\t&#125;\tret = rbd_stat(state-&gt;image, &amp;image_info, sizeof(image_info));\tif (ret &lt; 0) &#123;\t\ttcmu_dev_err(dev, &quot;Could not stat image.\\n&quot;);\t\tgoto stop_image;\t&#125;\t/*\t * librbd/ceph can better split and align unmaps and internal RWs, so\t * just have runner pass the entire cmd to us. To try and balance\t * overflowing the OSD/ceph side queues with discards/RWs limit it to\t * up to 4.\t */\tmax_blocks = (image_info.obj_size * 4) / tcmu_dev_get_block_size(dev);\ttcmu_dev_set_opt_xcopy_rw_len(dev, max_blocks);\ttcmu_dev_set_max_unmap_len(dev, max_blocks);\ttcmu_dev_set_opt_unmap_gran(dev, image_info.obj_size /\t\t\t\t    tcmu_dev_get_block_size(dev), false);\ttcmu_dev_set_write_cache_enabled(dev, 0);\tfree(dev_cfg_dup);\treturn 0;stop_image:\ttcmu_rbd_image_close(dev);free_config:\tfree(dev_cfg_dup);free_state:\ttcmu_rbd_state_free(state);\treturn ret;&#125;static void tcmu_rbd_close(struct tcmu_device *dev)&#123;\tstruct tcmu_rbd_state *state = tcmur_dev_get_private(dev);\ttcmu_rbd_image_close(dev);\ttcmu_rbd_state_free(state);&#125;static int tcmu_rbd_aio_read(struct tcmu_device *dev, struct rbd_aio_cb *aio_cb,\t\t\t     rbd_completion_t completion, struct iovec *iov,\t\t\t     size_t iov_cnt, size_t length, off_t offset)&#123;\tstruct tcmu_rbd_state *state = tcmur_dev_get_private(dev);\tint ret;\taio_cb-&gt;bounce_buffer = malloc(length);\tif (!aio_cb-&gt;bounce_buffer) &#123;\t\ttcmu_dev_err(dev, &quot;Could not allocate bounce buffer.\\n&quot;);\t\treturn -ENOMEM;\t&#125;\tret = rbd_aio_read(state-&gt;image, offset, length, aio_cb-&gt;bounce_buffer,\t\t\t   completion);\tif (ret &lt; 0)\t\tfree(aio_cb-&gt;bounce_buffer);\treturn ret;&#125;static int tcmu_rbd_aio_write(struct tcmu_device *dev, struct rbd_aio_cb *aio_cb,\t\t\t      rbd_completion_t completion, struct iovec *iov,\t\t\t      size_t iov_cnt, size_t length, off_t offset)&#123;\tstruct tcmu_rbd_state *state = tcmur_dev_get_private(dev);\tint ret;\taio_cb-&gt;bounce_buffer = malloc(length);\tif (!aio_cb-&gt;bounce_buffer) &#123;\t\ttcmu_dev_err(dev, &quot;Failed to allocate bounce buffer.\\n&quot;);\t\treturn -ENOMEM;;\t&#125;\ttcmu_memcpy_from_iovec(aio_cb-&gt;bounce_buffer, length, iov, iov_cnt);\tret = rbd_aio_write(state-&gt;image, offset, length, aio_cb-&gt;bounce_buffer,\t\t\t    completion);\tif (ret &lt; 0)\t\tfree(aio_cb-&gt;bounce_buffer);\treturn ret;&#125;static int tcmu_rbd_read(struct tcmu_device *dev, struct tcmulib_cmd *cmd,\t\t\t     struct iovec *iov, size_t iov_cnt, size_t length,\t\t\t     off_t offset)&#123;\tstruct rbd_aio_cb *aio_cb;\trbd_completion_t completion;\tssize_t ret;\taio_cb = calloc(1, sizeof(*aio_cb));\tif (!aio_cb) &#123;\t\ttcmu_dev_err(dev, &quot;Could not allocate aio_cb.\\n&quot;);\t\tgoto out;\t&#125;\taio_cb-&gt;dev = dev;\taio_cb-&gt;type = RBD_AIO_TYPE_READ;\taio_cb-&gt;read.length = length;\taio_cb-&gt;tcmulib_cmd = cmd;\taio_cb-&gt;iov = iov;\taio_cb-&gt;iov_cnt = iov_cnt;\tret = rbd_aio_create_completion\t\t(aio_cb, (rbd_callback_t) rbd_finish_aio_generic, &amp;completion);\tif (ret &lt; 0) &#123;\t\tgoto out_free_aio_cb;\t&#125;\tret = tcmu_rbd_aio_read(dev, aio_cb, completion, iov, iov_cnt,\t\t\t\tlength, offset);\tif (ret &lt; 0)\t\tgoto out_release_tracked_aio;\treturn TCMU_STS_OK;out_release_tracked_aio:\trbd_aio_release(completion);out_free_aio_cb:\tfree(aio_cb);out:\treturn TCMU_STS_NO_RESOURCE;&#125;static int tcmu_rbd_write(struct tcmu_device *dev, struct tcmulib_cmd *cmd,\t\t\t  struct iovec *iov, size_t iov_cnt, size_t length,\t\t\t  off_t offset)&#123;\tstruct rbd_aio_cb *aio_cb;\trbd_completion_t completion;\tssize_t ret;\taio_cb = calloc(1, sizeof(*aio_cb));\tif (!aio_cb) &#123;\t\ttcmu_dev_err(dev, &quot;Could not allocate aio_cb.\\n&quot;);\t\tgoto out;\t&#125;\taio_cb-&gt;dev = dev;\taio_cb-&gt;type = RBD_AIO_TYPE_WRITE;\taio_cb-&gt;tcmulib_cmd = cmd;\tret = rbd_aio_create_completion\t\t(aio_cb, (rbd_callback_t) rbd_finish_aio_generic, &amp;completion);\tif (ret &lt; 0) &#123;\t\tgoto out_free_aio_cb;\t&#125;\tret = tcmu_rbd_aio_write(dev, aio_cb, completion, iov, iov_cnt,\t\t\t\t length, offset);\tif (ret &lt; 0) &#123;\t\tgoto out_release_tracked_aio;\t&#125;\treturn TCMU_STS_OK;out_release_tracked_aio:\trbd_aio_release(completion);out_free_aio_cb:\tfree(aio_cb);out:\treturn TCMU_STS_NO_RESOURCE;&#125;\n\n参考链接\n[1] Ceph RDB 官方文档介绍\n[2] 简书 - ceph rbd：总览\n[3] 掘金 - Ceph介绍及原理架构分享\n[4] CSDN：Ceph学习——Librbd块存储库与RBD读写流程源码分析\n[5] CSDN：Ceph RBD编程接口Librbd(C++) – 映像创建与数据读写\n[6] 腾讯云专栏：大话Ceph系列\n[7] runsisi.com - librbd 内部运行机制\n[8] CSDN：Ceph学习——Librados与Osdc实现源码解析\n[9] CSDN：librbd代码目录解读\n[10] Ceph: RBD 创建镜像过程以及源码分析\n[11] librbd 架构分析\n\n","tags":["存储","分布式","ceph"]},{"title":"ceph中的存储空间分配器","url":"/2025/09/30/ceph%E4%B8%AD%E7%9A%84%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4%E5%88%86%E9%85%8D%E5%99%A8/","content":"Ceph BlueStore 分配器深度解析目录\n1. 引言\n2. 为什么需要分配器\n3. 分配器架构设计\n4. 五种分配器实现详解\n5. 性能对比与选择\n6. 源码解析\n7. 最佳实践\n8. 问题排查\n9. 总结\n\n\n\n\n1. 引言在 Ceph 存储系统中，BlueStore 作为新一代对象存储引擎，直接管理裸块设备，摒弃了传统文件系统的开销。在这种架构下，分配器（Allocator） 成为了至关重要的组件——它负责管理磁盘空闲空间的分配和回收，直接影响着存储性能、碎片化程度和内存占用。\n本文将深入剖析 Ceph 分配器的设计原理、实现细节和使用场景，帮助你全面理解这一核心组件。\n本文适合谁？\nCeph 运维工程师：了解如何选择和调优分配器\n存储开发者：理解分配器的设计思想和实现细节\n系统架构师：评估不同分配器对系统的影响\n\n\n2. 为什么需要分配器2.1 BlueStore 的存储架构在 BlueStore 中，数据流转路径如下：\n用户写入对象    ↓BlueStore 接收请求    ↓Allocator 分配磁盘空间 ← 本文重点    ↓写入裸块设备    ↓元数据存储到 RocksDB\n\nBlueStore 直接管理裸设备，需要自己实现：\n\n空闲空间管理：哪些磁盘块是空闲的？\n空间分配：为新数据找到合适的存储位置\n空间回收：删除数据后回收磁盘块\n碎片管理：避免磁盘空间碎片化\n\n这就是分配器的职责所在。\n2.2 分配器的核心问题分配器需要解决以下关键问题：\n问题 1：如何高效查找空闲空间？\n对于 10TB 磁盘，有数百万个 4KB 块\n需要快速找到满足大小要求的连续空间\n\n问题 2：如何控制内存占用？\n记录所有空闲区间需要内存\n碎片越多，内存占用越大\n需要在性能和内存间平衡\n\n问题 3：如何减少碎片？\n随机分配导致碎片化\n需要智能的分配策略\n\n问题 4：如何保证并发性能？\n多线程并发读写\n锁竞争影响性能\n\n\n3. 分配器架构设计3.1 核心接口Ceph 定义了统一的分配器接口：\nclass Allocator &#123;public:  // 分配指定大小的空间  virtual int64_t allocate(    uint64_t want_size,      // 期望分配的大小    uint64_t alloc_unit,     // 分配单元（通常 4KB）    uint64_t max_alloc_size, // 单个 extent 最大大小    int64_t hint,            // 位置提示（优化局部性）    PExtentVector *extents   // [输出] 分配的物理 extent 列表  ) = 0;  // 释放空间  virtual void release(    const interval_set&lt;uint64_t&gt;&amp; release_set  ) = 0;  // 获取可用空间大小  virtual uint64_t get_free() = 0;  // 获取碎片率  virtual double get_fragmentation() = 0;  // 初始化时添加空闲空间  virtual void init_add_free(uint64_t offset, uint64_t length) = 0;  // 初始化时移除已使用空间  virtual void init_rm_free(uint64_t offset, uint64_t length) = 0;&#125;;\n\n3.2 关键数据结构Extent（区段）表示一段连续的磁盘空间：\nstruct bluestore_pextent_t &#123;  uint64_t offset;  // 起始偏移  uint64_t length;  // 长度&#125;;typedef std::vector&lt;bluestore_pextent_t&gt; PExtentVector;\n\n分配示例磁盘布局：[已用][  空闲100MB  ][已用][  空闲50MB  ]分配 60MB：  - 查找：找到 100MB 空闲块  - 分配：[offset: 1000MB, length: 60MB]  - 更新：剩余 [offset: 1060MB, length: 40MB] 标记为空闲\n\n3.3 工作流程┌─────────────────────────────────────────────────────────┐│                      BlueStore                          ││                                                         ││  写入对象 ──┐                                           ││            │                                            ││            ↓                                            ││  ┌──────────────────┐        ┌──────────────────┐     ││  │   Allocator      │◄──────►│  FreelistManager │     ││  │                  │        │   (RocksDB)      │     ││  │ • 内存中管理     │        │  持久化空闲信息   │     ││  │ • 快速分配       │        │                  │     ││  └──────────────────┘        └──────────────────┘     ││            │                                            ││            ↓                                            ││  ┌──────────────────┐                                  ││  │   Block Device   │                                  ││  │   (裸块设备)     │                                  ││  └──────────────────┘                                  │└─────────────────────────────────────────────────────────┘\n\n关键点：\n\nAllocator：内存中管理，性能关键路径\nFreelistManager：持久化到 RocksDB，重启时恢复\n\n\n4. 五种分配器实现详解Ceph 提供了 5 种分配器实现，每种都有不同的设计权衡。\n4.1 StupidAllocator（简单分配器）设计原理StupidAllocator 使用 interval_set 结构，按大小分桶（bin）管理空闲块：\nclass StupidAllocator : public Allocator &#123;  // 多个桶，按 2 的幂次分组  std::vector&lt;interval_set_t&gt; free;    // [0-64KB] → bin 0  // [64KB-512KB] → bin 1  // [512KB-4MB] → bin 2  // ...&#125;;\n\n分配策略\n根据请求大小计算 bin\n从对应 bin 中查找\n找不到则去更大的 bin\nSimple first-fit 策略\n\n优缺点优点：\n\n✅ 实现简单，易于理解\n✅ 内存占用中等\n\n缺点：\n\n❌ 性能一般\n❌ 碎片控制差\n❌ 不适合生产环境\n\n适用场景： 测试、开发环境\n\n4.2 BitmapAllocator（位图分配器）设计原理使用 位图（Bitmap） 表示每个分配单元的状态：\n磁盘划分：每 4KB 一个分配单元位图表示：Bit:    0  1  2  3  4  5  6  7  8  9  10 11 12状态:   已 空 空 空 已 已 空 空 空 空 已 空 空       用 闲 闲 闲 用 用 闲 闲 闲 闲 用 闲 闲分配 16KB (4个单元)：  - 扫描位图找到连续的 4 个 1  - 位置 6-9 可用  - 标记为已用：...00000111...\n\n数据结构class BitmapAllocator : public Allocator,  public AllocatorLevel02&lt;AllocatorLevel01Loose&gt; &#123;    // 两层位图结构  // L1: 粗粒度位图（每 bit 代表多个 L2 块）  // L2: 细粒度位图（每 bit 代表 4KB）&#125;;\n\n分配策略\n在 L1 位图快速定位可能的空闲区域\n在 L2 位图精确查找连续空闲块\n利用位运算加速查找\n\n优缺点优点：\n\n✅ 内存占用可预测：O(设备大小&#x2F;分配单元)\n10TB 设备，4KB 单元 → 320MB 内存\n\n\n✅ 查找快速：位运算高效\n✅ 碎片处理好\n\n缺点：\n\n❌ 大设备内存占用高\n❌ 初始化时间长\n\n适用场景：\n\n✅ SSD 设备\n✅ 小文件负载\n✅ 内存充足场景\n\n\n4.3 AvlAllocator（AVL 树分配器）设计原理使用 两棵 AVL 树 管理空闲区间：\nstruct range_seg_t &#123;  uint64_t start;  // 起始位置  uint64_t end;    // 结束位置&#125;;class AvlAllocator : public Allocator &#123;  // 树 1：按偏移量排序（用于位置查找）  avl_set&lt;range_seg_t, by_offset&gt; range_tree;    // 树 2：按大小排序（用于大小查找）  avl_multiset&lt;range_seg_t, by_size&gt; range_size_tree;&#125;;\n\n可视化示例空闲区间：[100MB-200MB] [500MB-600MB] [800MB-1000MB]range_tree (按偏移):         [500-600]        /         \\   [100-200]    [800-1000]range_size_tree (按大小):         [500-600]  100MB        /         \\   [100-200]    [800-1000]    100MB         200MB\n\n动态分配策略// 空间充足时 (&gt;1% 空闲)if (free_space_pct &gt; threshold) &#123;  // First-Fit：按偏移查找  // 优点：减少碎片，提高局部性  search_by_offset(range_tree, cursor);&#125;// 空间紧张时else &#123;  // Best-Fit：按大小查找  // 优点：提高空间利用率  search_by_size(range_size_tree, want_size);&#125;\n\n优缺点优点：\n\n✅ 动态策略：根据空闲度自动调整\n✅ 性能均衡：O(log N) 查找时间\n✅ 碎片控制好\n✅ 生产环境推荐\n\n缺点：\n\n❌ 内存不可控：碎片多时占用高\n极端情况：数百万个小空闲块 → 数 GB 内存\n\n\n❌ 高碎片下性能下降\n\n适用场景：\n\n✅ 通用场景\n✅ 碎片可控环境\n✅ 中等规模设备\n\n\n4.4 BtreeAllocator（B 树分配器）设计原理与 AvlAllocator 类似，但使用 B-tree 替代 AVL 树：\nclass BtreeAllocator : public Allocator &#123;  // 使用 B-tree 结构  btree::btree_map&lt;uint64_t, range_seg_t&gt; range_tree;  btree::btree_multimap&lt;uint64_t, range_seg_t&gt; range_size_tree;&#125;;\n\nB-tree vs AVLAVL Tree (二叉树):       [50]      /    \\   [30]    [70]   /  \\    /  \\[20][40][60][80]B-tree (多叉树):    [30 | 60]    /    |    \\[10,20][40,50][70,80,90]\n\n优缺点优点：\n\n✅ 更好的缓存局部性：节点存储多个元素\n✅ 树高度更低：减少指针跳转\n✅ 大规模数据性能好\n\n缺点：\n\n❌ 实现复杂\n❌ 小规模数据开销大\n\n适用场景：\n\n✅ 大型设备（&gt;10TB）\n✅ 高碎片场景\n\n\n4.5 HybridAllocator（混合分配器）⭐ 推荐设计原理结合 AVL 和 Bitmap 的优势，控制内存占用：\nclass HybridAllocator : public AvlAllocator &#123;  uint64_t max_mem;        // 内存上限  BitmapAllocator* bitmap; // 溢出存储    virtual void _spillover_range(uint64_t start, uint64_t end) override &#123;    // 当 AVL 树超过内存限制时    // 将最小的空闲块溢出到 Bitmap    bitmap-&gt;init_add_free(start, end);  &#125;&#125;;\n\n工作流程┌─────────────────────────────────────────────┐│           HybridAllocator                   ││                                             ││  ┌─────────────────────────┐               ││  │   AVL Tree (in-memory)  │               ││  │   存储大空闲块           │  内存 &lt; 256MB ││  │   [1GB-2GB]             │               ││  │   [5GB-6GB]             │               ││  │   ...                   │               ││  └─────────────────────────┘               ││              ↓ 溢出                         ││  ┌─────────────────────────┐               ││  │   Bitmap (spillover)    │               ││  │   存储小碎片块           │               ││  │   [4KB] [8KB] [12KB]... │               ││  └─────────────────────────┘               │└─────────────────────────────────────────────┘分配逻辑：1. 优先从 AVL 树分配（快速）2. 找不到则查询 Bitmap3. 定期整理：Bitmap 中大块合并回 AVL\n\n内存控制策略// 配置：最大 256MBbluestore_hybrid_alloc_mem_cap = 268435456// 当 AVL 树达到容量限制if (range_tree.size() &gt;= max_ranges) &#123;  // 移除最小的空闲块到 Bitmap  auto smallest = range_size_tree.begin();  spillover_to_bitmap(smallest);&#125;\n\n优缺点优点：\n\n✅ 内存可控：设置上限，可预测\n✅ 性能优秀：大块分配快速（AVL）\n✅ 碎片处理好：小碎片交给 Bitmap\n✅ 生产环境首选 ⭐⭐⭐\n\n缺点：\n\n❌ 实现最复杂\n❌ 需要调优参数\n\n适用场景：\n\n✅ 所有生产环境 🏆\n✅ 大规模部署\n✅ 混合负载\n\n\n5. 性能对比与选择5.1 综合对比表\n\n\n分配器\n内存占用\n分配速度\n碎片处理\n复杂度\n生产推荐\n\n\n\nStupid\n中等\n⭐⭐\n⭐⭐\n简单\n❌ 仅测试\n\n\nBitmap\n固定（高）\n⭐⭐⭐⭐\n⭐⭐⭐⭐\n中等\n✅ SSD\n\n\nAVL\n动态\n⭐⭐⭐\n⭐⭐⭐⭐\n中等\n✅ 通用\n\n\nBtree\n动态\n⭐⭐⭐\n⭐⭐⭐⭐\n高\n✅ 大设备\n\n\nHybrid\n可控\n⭐⭐⭐⭐⭐\n⭐⭐⭐⭐⭐\n高\n✅✅ 首选\n\n\n5.2 选择建议决策树开始 │ ├─ 测试/开发环境？ │   └─ 是 → StupidAllocator │ ├─ 内存严格受限？ │   └─ 是 → HybridAllocator (设置 mem_cap) │ ├─ 设备类型？ │   ├─ SSD + 小文件负载 → BitmapAllocator │   ├─ 大设备 (&gt;20TB) → BtreeAllocator │   └─ 其他 → HybridAllocator ⭐ │ └─ 不确定？→ HybridAllocator (默认推荐)\n\n场景推荐场景 1：高性能 SSD 集群\nbluestore_allocator = hybridbluestore_hybrid_alloc_mem_cap = 536870912  # 512MB\n\n场景 2：HDD 大容量存储\nbluestore_allocator = btree\n\n场景 3：内存受限环境\nbluestore_allocator = hybridbluestore_hybrid_alloc_mem_cap = 134217728  # 128MB\n\n\n6. 源码解析6.1 核心分配流程// src/os/bluestore/BlueStore.ccint BlueStore::_do_alloc_write(    TransContext *txc,    CollectionRef&amp; c,    OnodeRef o,    BlueStore::WriteContext *wctx)&#123;  uint64_t need = wctx-&gt;logical_length;    // 1. 调用分配器分配空间  PExtentVector extents;  int64_t got = alloc-&gt;allocate(    need,                    // 需要的大小    min_alloc_size,          // 最小分配单元 (4KB)    need,                    // 最大分配大小    0,                       // hint (位置提示)    &amp;extents                 // 输出：分配的 extent 列表  );    if (got &lt; 0) &#123;    // 分配失败    return got;  &#125;    // 2. 写入数据到分配的物理位置  for (auto&amp; p : extents) &#123;    r = bdev-&gt;aio_write(      p.offset,              // 物理偏移      bl,                    // 数据      &amp;txc-&gt;ioc,            // IO 上下文      false    );  &#125;    // 3. 更新元数据  o-&gt;extent_map.punch_hole(offset, length);  o-&gt;extent_map.add_extent(offset, extents);    return 0;&#125;\n\n6.2 AVL 分配器核心代码// src/os/bluestore/AvlAllocator.ccint64_t AvlAllocator::_allocate(    uint64_t want,    uint64_t unit,    uint64_t max_alloc_size,    int64_t  hint,    PExtentVector *extents)&#123;  std::lock_guard&lt;std::mutex&gt; l(lock);    uint64_t allocated = 0;    while (allocated &lt; want) &#123;    uint64_t offset, length;        // 选择分配策略    if (_use_first_fit()) &#123;      // First-fit: 按偏移查找（减少碎片）      offset = _pick_block_after(&amp;cursor, want - allocated, unit);    &#125; else &#123;      // Best-fit: 按大小查找（提高利用率）      offset = _pick_block_fits(want - allocated, unit);    &#125;        if (offset == 0) &#123;      break;  // 找不到合适的块    &#125;        // 从树中移除分配的区间    _remove_from_tree(offset, length);        // 记录分配结果    extents-&gt;emplace_back(offset, length);    allocated += length;  &#125;    return allocated;&#125;// 按大小查找uint64_t AvlAllocator::_pick_block_fits(uint64_t size, uint64_t align)&#123;  // 在 range_size_tree 中查找 &gt;= size 的最小块  range_seg_t search_node(0, size);  auto rs_it = range_size_tree.lower_bound(search_node);    if (rs_it == range_size_tree.end()) &#123;    return 0;  // 没有足够大的块  &#125;    uint64_t offset = p2roundup(rs_it-&gt;start, align);  return offset;&#125;\n\n6.3 分配器创建// src/os/bluestore/Allocator.ccAllocator *Allocator::create(    CephContext* cct,    std::string_view type,    int64_t size,    int64_t block_size,    std::string_view name)&#123;  Allocator* alloc = nullptr;    if (type == &quot;stupid&quot;) &#123;    alloc = new StupidAllocator(cct, size, block_size, name);  &#125; else if (type == &quot;bitmap&quot;) &#123;    alloc = new BitmapAllocator(cct, size, block_size, name);  &#125; else if (type == &quot;avl&quot;) &#123;    alloc = new AvlAllocator(cct, size, block_size, name);  &#125; else if (type == &quot;btree&quot;) &#123;    alloc = new BtreeAllocator(cct, size, block_size, name);  &#125; else if (type == &quot;hybrid&quot;) &#123;    uint64_t mem_cap = cct-&gt;_conf.get_val&lt;uint64_t&gt;(      &quot;bluestore_hybrid_alloc_mem_cap&quot;    );    alloc = new HybridAllocator(cct, size, block_size, mem_cap, name);  &#125;    return alloc;&#125;\n\n\n7. 最佳实践7.1 配置优化基础配置# ceph.conf[osd]# 选择分配器类型bluestore_allocator = hybrid# Hybrid 分配器内存上限 (256MB)bluestore_hybrid_alloc_mem_cap = 268435456# 最小分配单元 (HDD: 64KB, SSD: 4KB)bluestore_min_alloc_size_hdd = 65536bluestore_min_alloc_size_ssd = 4096# AVL 分配器参数bluestore_avl_alloc_ff_max_search_count = 100bluestore_avl_alloc_ff_max_search_bytes = 1048576\n\n高性能 SSD 配置[osd]bluestore_allocator = hybridbluestore_hybrid_alloc_mem_cap = 536870912  # 512MBbluestore_min_alloc_size_ssd = 4096\n\n大容量 HDD 配置[osd]bluestore_allocator = btreebluestore_min_alloc_size_hdd = 65536\n\n7.2 监控指标查看分配器状态# 查看 OSD 分配器信息ceph daemon osd.0 bluestore allocator dump# 输出示例&#123;    &quot;allocator_type&quot;: &quot;hybrid&quot;,    &quot;capacity&quot;: 10737418240,    &quot;alloc_unit&quot;: 4096,    &quot;alloc_size_min&quot;: 4096,    &quot;free&quot;: 8589934592,    &quot;fragmentation&quot;: 0.15,    &quot;num_free_ranges&quot;: 12450&#125;\n\n关键指标# 碎片率fragmentation_score = num_free_ranges / (free_blocks - 1)# 空间利用率utilization = (capacity - free) / capacity# 分配成功率alloc_success_rate = allocated / requested\n\nPrometheus 监控# 添加监控告警groups:- name: ceph_allocator  rules:  # 碎片率过高  - alert: HighFragmentation    expr: ceph_bluestore_fragmentation &gt; 0.3    for: 10m    annotations:      summary: &quot;OSD &#123;&#123; $labels.osd &#125;&#125; fragmentation high&quot;        # 可用空间不足  - alert: LowFreeSpace    expr: ceph_bluestore_free_bytes / ceph_bluestore_capacity_bytes &lt; 0.1    for: 5m\n\n7.3 性能调优调优步骤1. 基线测试\n# 使用 fio 测试基线性能fio --name=test --ioengine=libaio --direct=1 \\    --bs=4k --rw=randwrite --numjobs=4 \\    --size=10G --runtime=300\n\n2. 调整分配器\n# 修改配置ceph config set osd.* bluestore_allocator hybridceph config set osd.* bluestore_hybrid_alloc_mem_cap 536870912# 重启 OSDsystemctl restart ceph-osd@0\n\n3. 压力测试\n# RBD 测试rbd bench --io-type write --io-size 4K --io-pattern rand test-pool/test-image# RADOS 测试rados bench -p test-pool 300 write -t 32\n\n4. 对比结果\n# 查看性能指标ceph osd perfceph daemon osd.0 perf dump\n\n常见问题调优问题 1：分配延迟高\n# 症状：apply_latency 和 commit_latency 高# 方案 1：增加内存上限ceph config set osd.* bluestore_hybrid_alloc_mem_cap 1073741824  # 1GB# 方案 2：切换到 bitmap (SSD)ceph config set osd.* bluestore_allocator bitmap\n\n问题 2：内存占用过高\n# 症状：OSD 内存使用持续增长# 方案 1：限制 hybrid 内存ceph config set osd.* bluestore_hybrid_alloc_mem_cap 134217728  # 128MB# 方案 2：触发碎片整理ceph osd compact &lt;osd-id&gt;\n\n问题 3：碎片率过高\n# 症状：fragmentation &gt; 0.3# 方案 1：离线碎片整理systemctl stop ceph-osd@0ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-0 \\                    fsck --deep# 方案 2：数据重平衡ceph osd out osd.0# 等待数据迁移ceph osd in osd.0\n\n7.4 升级和迁移在线切换分配器# 1. 设置新分配器 (下次重启生效)ceph config set osd.0 bluestore_allocator hybrid# 2. 安全重启 OSDceph osd set nooutceph osd set norebalancesystemctl restart ceph-osd@0ceph osd unset nooutceph osd unset norebalance# 3. 验证ceph daemon osd.0 config get bluestore_allocator\n\n批量迁移#!/bin/bash# 批量切换所有 OSD 到 hybrid 分配器for osd in $(ceph osd ls); do  echo &quot;Processing OSD.$osd&quot;    # 设置配置  ceph config set osd.$osd bluestore_allocator hybrid    # 重启  ceph osd set noout  systemctl restart ceph-osd@$osd    # 等待 OSD 恢复  while ! ceph osd stat | grep -q &quot;up $((osd+1))&quot;; do    sleep 5  done    ceph osd unset noout  sleep 30  # 间隔 30 秒done\n\n\n8. 问题排查8.1 常见问题问题 1：分配失败 (ENOSPC)现象：\n客户端写入失败: No space left on device但 ceph df 显示还有空间\n\n原因：\n\n碎片化严重，找不到连续空间\n分配器内部数据结构不一致\n\n排查步骤：\n# 1. 检查碎片率ceph daemon osd.0 bluestore allocator dump | grep fragmentation# 2. 检查空闲空间分布ceph daemon osd.0 bluestore allocator dump | grep num_free_ranges# 3. 查看 OSD 日志tail -f /var/log/ceph/ceph-osd.0.log | grep -i &quot;alloc\\|enospc&quot;\n\n解决方案：\n# 方案 1：整理碎片 (离线)ceph osd out 0systemctl stop ceph-osd@0ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0 \\                       --op compactsystemctl start ceph-osd@0ceph osd in 0# 方案 2：增大分配单元 (重建 OSD)bluestore_min_alloc_size_hdd = 131072  # 128KB\n\n问题 2：内存泄漏现象：\nOSD 内存持续增长最终触发 OOM killer\n\n排查步骤：\n# 1. 检查分配器内存ceph daemon osd.0 dump_mempools | grep bluestore_alloc# 2. 检查空闲区间数量ceph daemon osd.0 bluestore allocator dump | grep num_free_ranges# 3. 如果 num_free_ranges 异常大 (&gt;100万)，说明碎片严重\n\n解决方案：\n# 临时：重启 OSDsystemctl restart ceph-osd@0# 长期：切换到内存可控的分配器ceph config set osd.0 bluestore_allocator hybridceph config set osd.0 bluestore_hybrid_alloc_mem_cap 268435456\n\n问题 3：性能抖动现象：\n写入延迟周期性升高iostat 显示设备利用率不高\n\n排查步骤：\n# 1. 查看分配耗时ceph daemon osd.0 perf dump | grep -A 5 bluestore_alloc# 2. 查看碎片情况ceph daemon osd.0 bluestore allocator dump# 3. 启用详细日志ceph daemon osd.0 config set debug_bluestore 10\n\n解决方案：\n# 增加分配器搜索限制，避免过度搜索ceph config set osd.* bluestore_avl_alloc_ff_max_search_count 50ceph config set osd.* bluestore_avl_alloc_ff_max_search_bytes 524288\n\n8.2 调试工具BlueStore Tool# 检查 allocator 状态ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-0 \\                    show-label# 深度检查ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-0 \\                    fsck --deep# 查看空闲空间ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-0 \\                    free-dump# 修复不一致ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-0 \\                    repair\n\n性能分析# 使用 perf 分析分配器热点perf record -g -p $(pidof ceph-osd) -- sleep 30perf report --stdio | grep -A 10 Allocator# 使用 gdb 调试gdb -p $(pidof ceph-osd)(gdb) thread apply all bt(gdb) print allocator-&gt;get_free()\n\n\n9. 总结9.1 核心要点\n分配器是 BlueStore 的核心\n\n直接影响性能、内存、碎片\n\n\n五种实现，各有千秋\n\nStupid: 测试用\nBitmap: SSD 优化，内存固定\nAVL: 通用，动态平衡\nBtree: 大设备优化\nHybrid: 生产首选 ⭐\n\n\n关键权衡\n\n性能 vs 内存\n碎片控制 vs 分配速度\n简单 vs 功能\n\n\n实践建议\n\n默认使用 Hybrid\n监控碎片率和内存\n定期整理碎片\n根据负载调优\n\n\n\n9.2 决策建议你应该使用哪个分配器？┌─────────────────────────────────────┐│  生产环境？                          ││  ├─ 是 → Hybrid Allocator ⭐⭐⭐    ││  └─ 否 → Stupid (测试) / AVL        │└─────────────────────────────────────┘特殊场景：• 高性能 SSD + 内存充足 → Bitmap• 超大设备 (&gt;20TB) → Btree• 内存严格受限 → Hybrid (设置 mem_cap)\n\n9.3 未来展望Ceph 社区正在持续优化分配器：\n\n更智能的策略\n\n基于 workload 的自适应分配\n机器学习优化位置选择\n\n\n更低的开销\n\n无锁数据结构\n并行分配\n\n\n更好的碎片控制\n\n在线碎片整理\n预测性空间管理\n\n\n持久化内存支持\n\nPMem 加速元数据\n降低重启恢复时间\n\n\n\n\n参考资料\n源码\n\nsrc/os/bluestore/Allocator.h - 分配器接口\nsrc/os/bluestore/*Allocator.cc - 各实现\n\n\n文档\n\nBlueStore Configuration Reference\nBlueStore Allocator Analysis\n\n\n论文\n\n“BlueStore: A New, Faster Storage Backend for Ceph”\n\n\n社区\n\nCeph Dev Mailing List\n#ceph-devel IRC\n\n\n\n\n关于作者by hoshi本文基于 Ceph 最新代码（Pacific&#x2F;Quincy 版本）分析编写，涵盖了分配器的设计原理、实现细节和最佳实践。\n\n如果觉得本文有帮助，欢迎分享！ 🚀\n有任何问题或建议，欢迎提 Issue 。\n","tags":["ceph","seastore"]},{"title":"LRU","url":"/2022/09/18/lru/","content":"用线性表加哈希表实现LRU\n\n\n线性表+哈希表&gt;&gt;LRU算法方法：哈希表 + 双向链表算法LRU 缓存机制可以通过哈希表辅以双向链表实现，我们用一个哈希表和一个双向链表维护所有在缓存中的键值对。\n双向链表按照被使用的顺序存储了这些键值对，靠近头部的键值对是最近使用的，而靠近尾部的键值对是最久未使用的。\n哈希表即为普通的哈希映射（HashMap），通过缓存数据的键映射到其在双向链表中的位置。\n这样以来，我们首先使用哈希表进行定位，找出缓存项在双向链表中的位置，随后将其移动到双向链表的头部，即可在 O(1)O(1)O(1) 的时间内完成 get 或者 put 操作。具体的方法如下：\nint LRU::get(int key)&#123;&#125;\n\n对于 get 操作，首先判断 key 是否存在：\n如果 key 不存在，则返回 −1；\n如果 key 存在，则 key 对应的节点是最近被使用的节点。通过哈希表定位到该节点在双向链表中的位置，并将其移动到双向链表的头部，最后返回该节点的值。\n对于 put 操作，首先判断 key 是否存在：\nvoid LRU::put(int key,int value)&#123;&#125;\n\n如果 key 不存在，使用 key 和 value 创建一个新的节点，在双向链表的头部添加该节点，并将 key 和该节点添加进哈希表中。然后判断双向链表的节点数是否超出容量，如果超出容量，则删除双向链表的尾部节点，并删除哈希表中对应的项；\n如果 key 存在，则与 get 操作类似，先通过哈希表定位，再将对应的节点的值更新为 value，并将该节点移到双向链表的头部。\n上述各项操作中，访问哈希表的时间复杂度为 O(1)，在双向链表的头部添加节点、在双向链表的尾部删除节点的复杂度也为 O(1)。而将一个节点移到双向链表的头部，可以分成「删除该节点」和「在双向链表的头部添加节点」两步操作，都可以在 O(1)时间内完成。\n小贴士在双向链表的实现中，使用一个伪头部（dummy head）和伪尾部（dummy tail）标记界限，这样在添加节点和删除节点的时候就不需要检查相邻的节点是否存在。\n复杂度分析时间复杂度：对于 put 和 get 都是 O(1)。\n空间复杂度：O(capacity)O(\\text{capacity})O(capacity)，因为哈希表和双向链表最多存储 capacity+1\\text{capacity} + 1capacity+1 个元素。\n#include &lt;iostream&gt;#include&lt;list&gt;#include&lt;unordered_map&gt;using namespace std;using std::cout;using std::endl;class LRU&#123;public:    LRU(int cap)    :_capacity(cap)    &#123;        cout &lt;&lt; &quot;LRU(int cap)&quot; &lt;&lt; endl;            &#125;    int get(int key);    void put(int key,int value);private:    struct cacheNode    &#123;        cacheNode(int key,int v)        :_key(key)        ,_value(v)        &#123;            cout &lt;&lt; &quot;cacheNode(int key,int v)&quot; &lt;&lt; endl;        &#125;        int _key;        int _value;    &#125;;    list&lt;cacheNode&gt; _nodes;//双向链表存，    int _capacity;//缓存的大小    unordered_map&lt;int,list&lt;cacheNode&gt;::iterator &gt; _cache;//无序map ，    //存放的是 key值，和链表的迭代器&#125;;int LRU::get(int key)&#123;    //TODO 判断key值是否在map中，如果存在，直接把他    //更新在链表的头，并且返回他的value,不存在则返回-1；        auto it = _cache.find(key);//unordered_map 的 find 函数返回值为                            // 该key值所对应的迭代器    if(it!=_cache.end())    &#123;        _nodes.splice(_nodes.begin(),_nodes,it-&gt;second);        //链表的splice 函数可以将 _nodes链表中的 it-&gt;second所指向的元素        //转移到_nodes.begin()的前面        return it-&gt;second-&gt;_value;    &#125;    else    &#123;        return -1;    &#125;&#125;void LRU::put(int key,int value)&#123;    //TODO 判断key 是否存在，如果存在那么，直接放在链表表头    //如果不存在则判断链表是不是满的，如果满了删除末尾元素    //然后在链表头插入，并且插入到map中        auto it = _cache.find(key);     if(it!=_cache.end())    &#123;        it-&gt;second-&gt;_value= value;        _nodes.splice(_nodes.begin(),_nodes,it-&gt;second);    &#125;    else    &#123;        if((int)_nodes.size()==_capacity)        &#123;            auto &amp;deleteNode = _nodes.back();            _cache.erase(deleteNode._key);//unordered_map 的 earse操作                                        // size_type earse(const key_type&amp;key);            _nodes.pop_back();        &#125;        _nodes.push_front(cacheNode(key,value));        _cache.insert(std::make_pair(key,_nodes.begin()));    &#125;&#125;void test0()&#123;        LRU lru(2);    lru.put(1,88);    cout &lt;&lt; &quot;get(1)&quot; &lt;&lt; lru.get(1) &lt;&lt; endl;    lru.put(3,99);    lru.put(4,77);    cout &lt;&lt; &quot;get(1)&quot; &lt;&lt; lru.get(1) &lt;&lt; endl;&#125;int main(void)&#123;    test0();    return 0;&#125;\n\n","tags":["算法"]},{"title":"Mymap Myset","url":"/2022/10/15/mymap_myset/","content":"红黑树模拟实现map和set\n\n\n一、map和set模板set用value标识元素(value就是key，类型为T)，并且每个value必须唯一 。\ntemplate &lt; class Key&gt;//set\n\n\n\n在map中，键值key通常用于排序和惟一地标识元素，而值value中存储与此键值key关联的内容。键值key和值value的类型可能不同，并且在map的内部，key与value通过成员类型value_type绑定在一起，为其取别名称为pair：\ntypedef pair&lt;const Key, T&gt; value_type;template &lt; class Key, class T&gt;//map\n\n 用红黑树同时封装出set和map时，set传给value的是一个value，map传给value的是一个pair，set和map传给红黑树的value决定了这棵树里面存的节点值类型。上层容器不同，底层红黑树的Key和T也不同。\n\n在上层容器set中，K和T都代表Key，底层红黑树节点当中存储K和T都是一样的；map中，K代表键值Key，T代表由Key和Value构成的键值对，底层红黑树中只能存储T。所以红黑树为了满足同时支持set和map，节点当中存储T\n这就要对红黑树进行改动。\n二、红黑树节点定义1.红黑树节点定义由类模板template&lt;class K,class V&gt;\n\n修改为\ntemplate&lt;class T&gt;\n\n那么节点定义修改为:\n//红黑树节点定义template&lt;class T&gt;struct RBTreeNode&#123;\tRBTreeNode&lt;T&gt;* _left;//节点的左孩子\tRBTreeNode&lt;T&gt;* _right;//节点的右孩子\tRBTreeNode&lt;T&gt;* _parent;//节点的父亲 \tT _data;//节点的值，_data里面存的是K就传K，存的是pair就传pair\tColour _col;//节点颜色 \tRBTreeNode(const T&amp; x)\t\t:_left(nullptr)\t\t, _right(nullptr)\t\t, _parent(nullptr)\t\t, _data(x)\t\t, _col(RED)\t&#123;&#125;&#125;;\n\n由于红黑树不知道上层传的是K还是pair，这是由上层传递的模板参数T决定的，上层是封装我的map和set\n2.仿函数（1）节点比较大小时存在的问题红黑树插入节点时，需要比较节点的大小，kv需要改成_data:\n//插入pair&lt;Node*, bool&gt; Insert(const T&amp; data)&#123;\tif (_root == nullptr)\t&#123;\t\t_root = new Node(data);\t\t_root-&gt;_col = BLACK;\t\treturn make_pair(_root, true);\t&#125;\t//1.先看树中，kv是否存在\tNode* parent = nullptr;\tNode* cur = _root;\twhile (cur)\t&#123;\t\tif (cur-&gt;_data &lt; data)\t\t&#123;\t\t\t//kv比当前节点值大，向右走\t\t\tparent = cur;\t\t\tcur = cur-&gt;_right;\t\t&#125;\t\telse if (cur-&gt;_data &gt; data)\t\t&#123;\t\t\t//kv比当前节点值小，向左走\t\t\tparent = cur;\t\t\tcur = cur-&gt;_left;\t\t&#125;\t\telse\t\t&#123;\t\t\t//kv和当前节点值相等，已存在，插入失败\t\t\treturn make_pair(cur, false);\t\t&#125;\t&#125;\t//2.走到这里，说明kv在树中不存在，需要插入kv，并且cur已经为空，parent已经是叶子节点了\tNode* newNode = new Node(kv);\tnewNode-&gt;_col = RED;\tif (parent-&gt;_data &lt; data)\t&#123;\t\t//kv比parent值大，插入到parent的右边\t\tparent-&gt;_right = newNode;\t\tnewNode-&gt;_parent = parent;\t&#125;\telse\t&#123;\t\t//kv比parent值小，插入到parent的左边\t\tparent-&gt;_left = newNode;\t\tnewNode-&gt;_parent = parent;\t&#125;\tcur = newNode;\t       //如果父亲存在，且父亲颜色为红就要处理\twhile (parent &amp;&amp; parent-&gt;_col == RED)\t&#123;\t\t//情况一和情况二、三的区别关键看叔叔\t\tNode* grandfather = parent-&gt;_parent;//当父亲是红色时，根据规则（2）根节点一定是黑色，祖父一定存在\t\tif (parent == grandfather-&gt;_left)//父亲是祖父的左子树\t\t&#123;\t\t\tNode* uncle = grandfather-&gt;_right;\t\t\t//情况一：叔叔存在且为红\t\t\tif (uncle-&gt;_col == RED)\t\t\t&#123;\t\t\t\tparent-&gt;_col = uncle-&gt;_col = BLACK;\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t//继续向上调整\t\t\t\tcur = grandfather;\t\t\t\tparent = cur-&gt;_parent;\t\t\t&#125;\t\t\telse//情况二+情况三：叔叔不存在或叔叔存在且为黑\t\t\t&#123;\t\t\t\t//情况二：单旋\t\t\t\tif (cur == parent-&gt;_left)\t\t\t\t&#123;\t\t\t\t\tRotateR(grandfather);\t\t\t\t\tparent-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\telse//情况三：双旋\t\t\t\t&#123;\t\t\t\t\tRotateL(parent);\t\t\t\t\tRotateR(grandfather);\t\t\t\t\tcur-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\tbreak;//插入结束\t\t\t&#125;\t\t&#125;\t\telse//父亲是祖父的右子树\t\t&#123;\t\t\tNode* uncle = grandfather-&gt;_left;\t\t\t//情况一：叔叔存在且为红\t\t\tif (uncle &amp;&amp; uncle-&gt;_col == RED)\t\t\t&#123;\t\t\t\tparent-&gt;_col = uncle-&gt;_col = BLACK;\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t//继续往上调整\t\t\t\tcur = grandfather;\t\t\t\tparent = grandfather-&gt;_parent;\t\t\t&#125;\t\t\telse//情况二+情况三：叔叔不存在或叔叔存在且为黑\t\t\t&#123;\t\t\t\t//情况二：单旋\t\t\t\tif (cur == parent-&gt;_right)\t\t\t\t&#123;\t\t\t\t\tRotateL(grandfather);\t\t\t\t\tparent-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\telse//情况三：双旋\t\t\t\t&#123;\t\t\t\t\tRotateR(parent);\t\t\t\t\tRotateL(grandfather);\t\t\t\t\tcur-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\tbreak;//插入结束\t\t\t&#125;\t\t&#125;\t&#125;\t_root-&gt;_col = BLACK;\treturn make_pair(newNode, true);&#125;\n\n但是以上代码在插入新节和查找节点时，当和当前节点比较大小时，Key可以比较，但是pair比较不了，也就是set可以比较，但是map比较不了。这就需要写一个仿函数，如果是map就取_data里面的first也就是Key进行比较，通过泛型解决红黑树里面存的是什么。所以上层容器map需要向底层的红黑树提供仿函数来获取T里面的Key，这样无论上层容器是set还是map，都可以用统一的方式进行比较了。\n(2) 仿函数仿函数让一个类的使用看上去像个函数。仿函数是在类中实现了一个operator( )，是一个类的对象，这个类就有了类似函数的行为，所以这个类就是一个仿函数类，目的是为了让函数拥有类的性质。\n这个类的对象即仿函数，可以当作一般函数去用，只不过仿函数的功能是在一个类中的运算符operator()中实现的，使用的时候把函数作为参进行传递即可。\nset有set的仿函数，map有map的仿函数，尽管set的仿函数看起来没有什么作用，但是，必须要把它传给底层红黑树，这样红黑树就能根据仿函数分别获取set的key和map的first。\n①：set的仿函数\nnamespace delia&#123;\ttemplate&lt;class K&gt;\tclass set\t&#123;\t\t//仿函数，获取set的key\t\tstruct SetKeyOfT\t\t&#123;\t\t\tconst K&amp; operator()(const K&amp; key)\t\t\t&#123;\t\t\t\treturn key;\t\t\t&#125;\t\t&#125;;        public:\t\tbool insert(const K&amp; k)\t\t&#123;\t\t\t_t.Insert(k);\t\t\treturn true;\t\t&#125; \tprivate:\t\tRBTree&lt;K, K,SetKeyOfT&gt; _t;\t&#125;;&#125;\n\n②map的仿函数\nnamespace delia&#123;\ttemplate&lt;class K,class V&gt;\tclass map\t&#123;\t\t//仿函数，获取map的first\t\tstruct MapKeyOfT\t\t&#123;\t\t\tconst K&amp; operator()(const pair&lt;const K, V&gt;&amp; kv)\t\t\t&#123;\t\t\t\treturn kv.first;\t\t\t&#125;\t\t&#125;;     public:        //插入\t\tbool insert(const pair&lt;const K, V&gt;&amp; kv)\t\t&#123;\t\t\t_t.Insert(kv);\t\t\treturn true;\t\t&#125;\tprivate:\t\tRBTree&lt;K, pair&lt;const K, V&gt;, MapKeyOfT&gt; _t;\t&#125;;&#125;\n\n有了仿函数红黑树的类在实现时，就要在模板参数中增加KeyOfT仿函数。\n（3）修改红黑树定义template&lt;class K, class T, class KeyOfT&gt;class RBTree&#123;\ttypedef RBTreeNode&lt;T&gt; Node;\tprivate:\tNode* _root;&#125;;\n\n（4）修改红黑树插入//插入pair&lt;Node*, bool&gt; Insert(const pair&lt;K, V&gt;&amp; kv)&#123;\tif (_root == nullptr)\t&#123;\t\t_root = new Node(kv);\t\t_root-&gt;_col = BLACK;\t\treturn make_pair(_root, true);\t&#125;\tKeyOfT kot;\t//1.先看树中，kv是否存在\tNode* parent = nullptr;\tNode* cur = _root;\twhile (cur)\t&#123;\t\tif (kot(cur-&gt;_data) &lt; kot(data))\t\t&#123;\t\t\t//kv比当前节点值大，向右走\t\t\tparent = cur;\t\t\tcur = cur-&gt;_right;\t\t&#125;\t\telse if (kot(cur-&gt;_data) &gt; kot(data))\t\t&#123;\t\t\t//kv比当前节点值小，向左走\t\t\tparent = cur;\t\t\tcur = cur-&gt;_left;\t\t&#125;\t\telse\t\t&#123;\t\t\t//kv和当前节点值相等，已存在，插入失败\t\t\treturn make_pair(cur, false);\t\t&#125;\t&#125;\t//2.走到这里，说明kv在树中不存在，需要插入kv，并且cur已经为空，parent已经是叶子节点了\tNode* newNode = new Node(kv);\tnewNode-&gt;_col = RED;\tif (kot(parent-&gt;_data) &lt; kot(data))\t&#123;\t\t//kv比parent值大，插入到parent的右边\t\tparent-&gt;_right = newNode;\t\tnewNode-&gt;_parent = parent;\t&#125;\telse\t&#123;\t\t//kv比parent值小，插入到parent的左边\t\tparent-&gt;_left = newNode;\t\tnewNode-&gt;_parent = parent;\t&#125;\tcur = newNode;\t//如果父亲存在，且父亲颜色为红就要处理\twhile (parent &amp;&amp; parent-&gt;_col == RED)\t&#123;\t\t//情况一和情况二、三的区别关键看叔叔\t\tNode* grandfather = parent-&gt;_parent;//当父亲是红色时，根据规则（2）根节点一定是黑色，祖父一定存在\t\tif (parent == grandfather-&gt;_left)//父亲是祖父的左子树\t\t&#123;\t\t\tNode* uncle = grandfather-&gt;_right;\t\t\t//情况一：叔叔存在且为红\t\t\tif (uncle-&gt;_col == RED)\t\t\t&#123;\t\t\t\tparent-&gt;_col = uncle-&gt;_col = BLACK;\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t//继续向上调整\t\t\t\tcur = grandfather;\t\t\t\tparent = cur-&gt;_parent;\t\t\t&#125;\t\t\telse//情况二+情况三：叔叔不存在或叔叔存在且为黑\t\t\t&#123;\t\t\t\t//情况二：单旋\t\t\t\tif (cur == parent-&gt;_left)\t\t\t\t&#123;\t\t\t\t\tRotateR(grandfather);\t\t\t\t\tparent-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\telse//情况三：双旋\t\t\t\t&#123;\t\t\t\t\tRotateL(parent);\t\t\t\t\tRotateR(grandfather);\t\t\t\t\tcur-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\tbreak;//插入结束\t\t\t&#125;\t\t&#125;\t\telse//父亲是祖父的右子树\t\t&#123;\t\t\tNode* uncle = grandfather-&gt;_left;\t\t\t//情况一：叔叔存在且为红\t\t\tif (uncle &amp;&amp; uncle-&gt;_col == RED)\t\t\t&#123;\t\t\t\tparent-&gt;_col = uncle-&gt;_col = BLACK;\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t//继续往上调整\t\t\t\tcur = grandfather;\t\t\t\tparent = grandfather-&gt;_parent;\t\t\t&#125;\t\t\telse//情况二+情况三：叔叔不存在或叔叔存在且为黑\t\t\t&#123;\t\t\t\t//情况二：单旋\t\t\t\tif (cur == parent-&gt;_right)\t\t\t\t&#123;\t\t\t\t\tRotateL(grandfather);\t\t\t\t\tparent-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\telse//情况三：双旋\t\t\t\t&#123;\t\t\t\t\tRotateR(parent);\t\t\t\t\tRotateL(grandfather);\t\t\t\t\tcur-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t&#125;\t\t\t\tbreak;//插入结束\t\t\t&#125;\t\t&#125;\t&#125;\t_root-&gt;_col = BLACK;\treturn make_pair(newNode, true);&#125;void RotateR(Node* parent)&#123;\tNode* subL = parent-&gt;_left;\tNode* subLR = nullptr;\tif (subL)\t&#123;\t\tsubLR = subL-&gt;_right;\t&#125;\t//1.左子树的右子树变我的左子树\tparent-&gt;_left = subLR;\tif (subLR)\t&#123;\t\tsubLR-&gt;_parent = parent;\t&#125;\t//左子树变父亲\tsubL-&gt;_right = parent;\tNode* parentParent = parent-&gt;_parent;\tparent-&gt;_parent = subL;\tif (parent == _root)//parent是根\t&#123;\t\t_root = subL;\t\t_root-&gt;_parent = nullptr;\t&#125;\telse//parent不是根，是子树\t&#123;\t\tif (parentParent-&gt;_left == parent)\t\t&#123;\t\t\t//parent是自己父亲的左子树,将subL作为parent父亲的左孩子\t\t\tparentParent-&gt;_left = subL;\t\t&#125;\t\telse\t\t&#123;\t\t\t//parent是自己父亲的右子树,将subL作为parent父亲的右孩子\t\t\tparentParent-&gt;_right = subL;\t\t&#125;\t\t//subL的父亲就是parent的父亲\t\tsubL-&gt;_parent = parentParent;\t&#125;&#125;void RotateL(Node* parent)&#123;\tNode* subR = parent-&gt;_right;\tNode* subRL = nullptr;\tif (subR)\t&#123;\t\tsubRL = subR-&gt;_left;\t&#125;\t//1.右子树的左子树变我的右子树\tparent-&gt;_right = subRL;\tif (subRL)\t&#123;\t\tsubRL-&gt;_parent = parent;\t&#125;\t//2.右子树变父亲\tsubR-&gt;_left = parent;\tNode* parentParent = parent-&gt;_parent;\tparent-&gt;_parent = subR;\tif (parent == _root)//parent是根\t&#123;\t\t_root = parent;\t\t_root-&gt;_parent = nullptr;\t&#125;\telse//parent不是根，是子树\t&#123;\t\tif (parentParent-&gt;_left == parent)\t\t&#123;\t\t\t//parent是自己父亲的左子树,将subR作为parent父亲的左孩子\t\t\tparentParent-&gt;_left = subR;\t\t&#125;\t\telse\t\t&#123;\t\t\t//parent是自己父亲的右子树,将subR作为parent父亲的右孩子\t\t\tparentParent-&gt;_right = subR;\t\t&#125;\t\t//subR的父亲就是parent的父亲\t\tsubR-&gt;_parent = parentParent;\t&#125;&#125;\n\n（5）修改红黑树查找//查找Node* Find(const K&amp; key)&#123;\tKeyOfT kot;\tNode* cur = _root;\twhile (cur)\t&#123;\t\tif (kot(cur-&gt;_data) &lt; key)\t\t&#123;\t\t\tcur = cur-&gt;_right;\t\t&#125;\t\telse if (kot(cur-&gt;_data) &gt; key)\t\t&#123;\t\t\tcur = cur-&gt;_left;\t\t&#125;\t\telse\t\t&#123;\t\t\treturn cur;\t\t&#125;\t&#125;\treturn nullptr;//空树，直接返回&#125;\n\n三、红黑树迭代器map和set的迭代器的实现其实本质上是红黑树迭代器的实现，迭代器的实现需要定义模板类型、模板类型引用、模板类型指针。 \n1.红黑树中迭代器重命名 在红黑树中重命名模板类型、模板类型引用、模板类型指针，定义为public，外部就能使用iterator了：\ntemplate&lt;class K, class T, class KeyOfT&gt;class RBTree&#123;\ttypedef RBTreeNode&lt;T&gt; Node; public:\ttypedef __TreeIterator&lt;T, T&amp;, T*&gt; iterator;//模板类型、模板类型引用、模板类型指针        //红黑树函数...    private:\tNode* _root;&#125;;\n\n2.正向迭代器定义红黑树的迭代器的本质是对节点指针进行封装，所以迭代器中只有封装红黑树节点指针这一个成员变量 。正向迭代器：\ntemplate&lt;class T,class Ref,class ptr&gt;struct __TreeIterator&#123;\ttypedef RBTreeNode&lt;T&gt; Node;\ttypedef __TreeIterator&lt;T, Ref, ptr&gt; Self;      \tNode* _node;//成员变量\t&#125;;\n\n3.迭代器构造用节点指针构造正向迭代器：\n//构造函数__TreeIterator(Node* node)\t:_node(node)&#123;&#125;\n\n4.正向迭代器重载*Ref对正向迭代器解引用，返回节点数据引用\n//* 解引用，返回节点数据Ref Operator*()&#123;\treturn _node-&gt;_data;&#125;\n\n5.正向迭代器重载-&gt;Ptr对正向迭代器使用-&gt;，返回节点数据指针：\n//-&gt; 返回节点数据地址Ptr Operator-&gt;()&#123;\treturn &amp;_node-&gt;_data;&#125;\n\n6.正向迭代器重载&#x3D;&#x3D;判断节点是否相同\n//判断两个迭代器是否相同bool operator==(const Self&amp; s)&#123;\treturn _node == s._node;//判断节点是否相同&#125;\n\n7.正向迭代器重载！&#x3D;判断节点是否不同\n//判断两个迭代器是否不同bool operator!=(const Self&amp; s)&#123;\treturn _node != s._node;//判断节点是否不同&#125;\n\n8.正向迭代器++①当节点的右子树不为空时，++就要走到右子树的最左节点\n ②当节点的右子树为空时，++就要走到节点的父亲\n\t//红黑树迭代器的++也就是红黑树的++\tSelf operator++()\t&#123;\t\t//1.右子树不为空\t\tif (_node-&gt;_right)\t\t&#123;\t\t\t//下一个访问的是右树的中序第一个节点（即右子树最左节点）。\t\t\tNode* left = _node-&gt;_right; \t\t\t//找最左节点\t\t\twhile (left-&gt;_left)\t\t\t&#123;\t\t\t\tleft = left-&gt;_left;\t\t\t&#125;\t\t\t_node = left;\t\t&#125;\t\telse//2.右子树为空，下一个访问的就是当前节点的父亲\t\t&#123;\t\t\tNode* cur = _node;\t\t\tNode* parent = cur-&gt;_parent;\t\t\twhile (parent &amp;&amp; cur == parent-&gt;_right)\t\t\t&#123;\t\t\t\tcur = cur-&gt;_parent;\t\t\t\tparent = parent-&gt;_parent;\t\t\t&#125;\t\t\t_node = parent;\t\t&#125; \t\treturn *this;\t&#125;&#125;;\n\n9.正向迭代器– ①当节点的左子树不为空时，++就要走到左子树的最右节点\n ②当节点的左子树为空时，++就要走到节点的父亲\n//红黑树迭代器的--也就是红黑树的--Self operator--()&#123;\t//1.左子树不为空\tif (_node-&gt;_left)\t&#123;\t\t//下一个访问的是左树的中序左后节点（即做子树最右节点）。\t\tNode* right = _node-&gt;_left;\t\t//找最右节点\t\twhile (right-&gt;_right)\t\t&#123;\t\t\tright = right-&gt;_right;\t\t&#125;\t\t_node = right;\t&#125;\telse//2.左子树为空，下一个访问的就是当前节点的父亲\t&#123;\t\tNode* cur = _node;\t\tNode* parent = cur-&gt;_parent;\t\twhile (parent &amp;&amp; cur == parent-&gt;_left)\t\t&#123;\t\t\tcur = cur-&gt;_parent;\t\t\tparent = parent-&gt;_parent;\t\t&#125;\t\t_node = parent;\t&#125;\treturn *this;&#125;\n\n10.红黑树中实现迭代器实现begin( )找最左节点，end( )最后一个节点的下一个位置\ntemplate&lt;class K, class T, class KeyOfT&gt;class RBTree&#123;\ttypedef RBTreeNode&lt;T&gt; Node; public:\ttypedef __TreeIterator&lt;T, T&amp;, T*&gt; iterator;//模板类型、模板类型引用、模板类型指针        //找最左节点\titerator begin()\t&#123;\t\tNode* left = _root;\t\twhile (left &amp;&amp; left-&gt;_left)\t\t&#123;\t\t\tleft = left-&gt;_left;\t\t&#125; \t\treturn iterator(left)//返回最左节点的正向迭代器\t&#125; \t//结束\titerator end()\t&#123;\t\treturn iterator(nullptr);\t&#125;    private:\tNode* _root;&#125;;\n\n四、set模拟实现调用红黑树对应接口实现set，插入和查找函数返回值当中的节点指针改为迭代器:\n#pragma once#include &quot;RBTree.h&quot;namespace delia&#123;\ttemplate&lt;class K&gt;\tclass set\t&#123;\t\t//仿函数，获取set的key\t\tstruct SetKeyOfT\t\t&#123;\t\t\tconst K&amp; operator()(const K&amp; key)\t\t\t&#123;\t\t\t\treturn key;\t\t\t&#125;\t\t&#125;;\tpublic:\t\ttypedef typename RBTree&lt;K, K, SetKeyOfT&gt;::iterator iterator;\t\t\t\t//迭代器开始\t\titerator begin()\t\t&#123;\t\t\treturn _t.begin();\t\t&#125; \t\t//迭代器结束\t\titerator end()\t\t&#123;\t\t\treturn _t.end();\t\t&#125; \t\t//插入函数\t\tpair&lt;iterator,bool&gt; insert(const K&amp; key)\t\t&#123;\t\t\t\t\t\treturn _t.Insert(key);\t\t&#125; \t\t//查找\t\titerator find(const K&amp; key)\t\t&#123;\t\t\treturn _t.find(key);\t\t&#125;\tprivate:\t\tRBTree&lt;K, K, SetKeyOfT&gt; _t;\t&#125;;&#125;\n\n五、map模拟实现调用红黑树对应接口实现map，插入和查找函数返回值当中的节点指针改为迭代器，增加operator[ ]的重载:\n#pragma once#include &quot;RBTree.h&quot;namespace delia&#123;\ttemplate&lt;class K, class V&gt;\tclass map\t&#123;\t\t//仿函数，获取map的first\t\tstruct MapKeyOfT\t\t&#123;\t\t\tconst K&amp; operator()(const pair&lt;const K, V&gt;&amp; kv)\t\t\t&#123;\t\t\t\treturn kv.first;\t\t\t&#125;\t\t&#125;;\tpublic:\t\ttypedef typename RBTree&lt;K, K, MapKeyOfT&gt;::iterator iterator; \t\t//迭代器开始\t\titerator begin()\t\t&#123;\t\t\treturn _t.begin();\t\t&#125; \t\t//迭代器结束\t\titerator end()\t\t&#123;\t\t\treturn _t.end();\t\t&#125; \t\t//插入\t\tpair&lt;iterator, bool&gt; insert(const pair&lt;const K, V&gt;&amp; kv)\t\t&#123;\t\t\treturn _t.Insert(kv);\t\t&#125; \t\t//重载operator[]\t\tV&amp; operator[](const K&amp; key)\t\t&#123;\t\t\tpair&lt;iterator, bool&gt; ret = insert(make_pair(key, V()));\t\t\titerator it = ret.first;\t\t\treturn it-&gt;second;\t\t&#125; \t\t//查找\t\titerator find(const K&amp; key)\t\t&#123;\t\t\treturn _t.find(key);\t\t&#125; \tprivate:\t\tRBTree&lt;K, pair&lt;const K, V&gt;, MapKeyOfT&gt; _t;\t&#125;;&#125;\n\n六、红黑树完整代码段#pragma once#include&lt;iostream&gt;using namespace std;  //节点颜色enum Colour&#123;\tRED,\tBLACK,&#125;; //红黑树节点定义template&lt;class T&gt;struct RBTreeNode&#123;\tRBTreeNode&lt;T&gt;* _left;//节点的左孩子\tRBTreeNode&lt;T&gt;* _right;//节点的右孩子\tRBTreeNode&lt;T&gt;* _parent;//节点的父亲 \tT _data;//节点的值\tColour _col;//节点颜色 \tRBTreeNode(const T&amp; x)\t\t:_left(nullptr)\t\t, _right(nullptr)\t\t, _parent(nullptr)\t\t, _data(x)\t\t, _col(RED)\t&#123;&#125;&#125;;  template&lt;class T,class Ref,class ptr&gt;struct __TreeIterator&#123;\ttypedef RBTreeNode&lt;T&gt; Node;\ttypedef __TreeIterator&lt;T, Ref, ptr&gt; Self; \tNode* _node; \t//构造函数\t__TreeIterator(Node* node)\t\t:_node(node)\t&#123;&#125;\t\t//* 解引用，返回节点数据\tRef operator*()\t&#123;\t\treturn _node-&gt;_data;\t&#125; \t//-&gt; 返回节点数据地址\t//Ptr operator-&gt;()\t//&#123;\t//\treturn &amp;_node-&gt;_data;\t//&#125; \t//判断两个迭代器是否相同\tbool operator==(const Self&amp; s)\t&#123;\t\treturn _node == s._node;\t&#125; \t//判断两个迭代器是否不同\tbool operator!=(const Self&amp; s)\t&#123;\t\treturn _node != s._node;\t&#125; \t//红黑树迭代器的++也就是红黑树的++\tSelf operator++()\t&#123;\t\t//1.右子树不为空\t\tif (_node-&gt;_right)\t\t&#123;\t\t\t//下一个访问的是右树的中序第一个节点（即右子树最左节点）。\t\t\tNode* left = _node-&gt;_right; \t\t\t//找最左节点\t\t\twhile (left-&gt;_left)\t\t\t&#123;\t\t\t\tleft = left-&gt;_left;\t\t\t&#125;\t\t\t_node = left;\t\t&#125;\t\telse//2.右子树为空，下一个访问的就是当前节点的父亲\t\t&#123;\t\t\tNode* cur = _node;\t\t\tNode* parent = cur-&gt;_parent;\t\t\twhile (parent &amp;&amp; cur == parent-&gt;_right)\t\t\t&#123;\t\t\t\tcur = cur-&gt;_parent;\t\t\t\tparent = parent-&gt;_parent;\t\t\t&#125;\t\t\t_node = parent;\t\t&#125; \t\treturn *this;\t&#125; \t//红黑树迭代器的--也就是红黑树的--\tSelf operator--()\t&#123;\t\t//1.左子树不为空\t\tif (_node-&gt;_left)\t\t&#123;\t\t\t//下一个访问的是左树的中序左后节点（即做子树最右节点）。\t\t\tNode* right = _node-&gt;_left; \t\t\t//找最右节点\t\t\twhile (right-&gt;_right)\t\t\t&#123;\t\t\t\tright = right-&gt;_right;\t\t\t&#125;\t\t\t_node = right;\t\t&#125;\t\telse//2.左子树为空，下一个访问的就是当前节点的父亲\t\t&#123;\t\t\tNode* cur = _node;\t\t\tNode* parent = cur-&gt;_parent;\t\t\twhile (parent &amp;&amp; cur == parent-&gt;_left)\t\t\t&#123;\t\t\t\tcur = cur-&gt;_parent;\t\t\t\tparent = parent-&gt;_parent;\t\t\t&#125;\t\t\t_node = parent;\t\t&#125; \t\treturn *this;\t&#125;  &#125;; //插入节点颜色是红色好，还是黑色好，红色//因为插入红色节点，可能破坏规则3，影响不大//插入黑色节点，一定破坏规则4 ，并且影响其他路径，影响很大 template&lt;class K, class T, class KeyOfT&gt;class RBTree&#123;\ttypedef RBTreeNode&lt;T&gt; Node;public:\ttypedef __TreeIterator&lt;T, T&amp;, T*&gt; iterator;//模板类型、模板类型引用、模板类型指针 \t//构造函数\tRBTree()\t\t:_root(nullpte)\t&#123;&#125; \t//析构\t~RBTree()\t&#123;\t\t_Destroy(_root);\t\t_root = nullptr;\t&#125; \tvoid _Destroy(Node* root)\t&#123;\t\tif (root == nullptr)\t\t&#123;\t\t\treturn;\t\t&#125;\t\t_Destroy(root-&gt;_left);\t\t_Destroy(root-&gt;_right);\t\tdelete root;\t&#125; \t//找最左节点\titerator begin()\t&#123;\t\tNode* left = _root;\t\twhile (left &amp;&amp; left-&gt;_left)\t\t&#123;\t\t\tleft = left-&gt;_left;\t\t&#125; \t\treturn iterator(left);//返回最左节点的正向迭代器\t&#125; \t//结束\titerator end()\t&#123;\t\treturn iterator(nullptr);\t&#125; \t//构造函数\tRBTree()\t\t:_root(nullptr)\t&#123;&#125; \tvoid Destroy(Node* root)\t&#123;\t\tif (root == nullptr)\t\t&#123;\t\t\treturn;\t\t&#125; \t\tDestroy(root-&gt;_left);\t\tDestroy(root-&gt;_right);\t&#125;\t~RBTree()\t&#123;\t\tDestroy(_root);\t\t_root = nullptr;\t&#125; \t//插入\tpair&lt;Node*, bool&gt; Insert(const T&amp; data)\t&#123;\t\tif (_root == nullptr)\t\t&#123;\t\t\t_root = new Node(data);\t\t\t_root-&gt;_col = BLACK;\t\t\treturn make_pair(_root, true);\t\t&#125; \t\tKeyOfT kot; \t\t//1.先看树中，kv是否存在\t\tNode* parent = nullptr;\t\tNode* cur = _root;\t\twhile (cur)\t\t&#123;\t\t\tif (kot(cur-&gt;_data) &lt; kot(data))\t\t\t&#123;\t\t\t\t//kv比当前节点值大，向右走\t\t\t\tparent = cur;\t\t\t\tcur = cur-&gt;_right;\t\t\t&#125;\t\t\telse if (kot(cur-&gt;_data) &gt; kot(data))\t\t\t&#123;\t\t\t\t//kv比当前节点值小，向左走\t\t\t\tparent = cur;\t\t\t\tcur = cur-&gt;_left;\t\t\t&#125;\t\t\telse\t\t\t&#123;\t\t\t\t//kv和当前节点值相等，已存在，插入失败\t\t\t\treturn make_pair(cur, false);\t\t\t&#125;\t\t&#125; \t\t//2.走到这里，说明kv在树中不存在，需要插入kv，并且cur已经为空，parent已经是叶子节点了\t\tNode* newNode = new Node(data);\t\tnewNode-&gt;_col = RED;\t\tif (kot(parent-&gt;_data) &lt; kot(data))\t\t&#123;\t\t\t//kv比parent值大，插入到parent的右边\t\t\tparent-&gt;_right = newNode;\t\t\tnewNode-&gt;_parent = parent;\t\t&#125;\t\telse\t\t&#123;\t\t\t//kv比parent值小，插入到parent的左边\t\t\tparent-&gt;_left = newNode;\t\t\tnewNode-&gt;_parent = parent;\t\t&#125;\t\tcur = newNode; \t\t//如果父亲存在，且父亲颜色为红就要处理\t\twhile (parent &amp;&amp; parent-&gt;_col == RED)\t\t&#123;\t\t\t//情况一和情况二、三的区别关键看叔叔\t\t\tNode* grandfather = parent-&gt;_parent;//当父亲是红色时，根据规则（2）根节点一定是黑色，祖父一定存在\t\t\tif (parent == grandfather-&gt;_left)//父亲是祖父的左子树\t\t\t&#123;\t\t\t\tNode* uncle = grandfather-&gt;_right;\t\t\t\t//情况一：叔叔存在且为红\t\t\t\tif (uncle-&gt;_col == RED)\t\t\t\t&#123;\t\t\t\t\tparent-&gt;_col = uncle-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED; \t\t\t\t\t//继续向上调整\t\t\t\t\tcur = grandfather;\t\t\t\t\tparent = cur-&gt;_parent;\t\t\t\t&#125;\t\t\t\telse//情况二+情况三：叔叔不存在或叔叔存在且为黑\t\t\t\t&#123;\t\t\t\t\t//情况二：单旋\t\t\t\t\tif (cur == parent-&gt;_left)\t\t\t\t\t&#123;\t\t\t\t\t\tRotateR(grandfather);\t\t\t\t\t\tparent-&gt;_col = BLACK;\t\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t\t&#125;\t\t\t\t\telse//情况三：双旋\t\t\t\t\t&#123;\t\t\t\t\t\tRotateL(parent);\t\t\t\t\t\tRotateR(grandfather);\t\t\t\t\t\tcur-&gt;_col = BLACK;\t\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t\t&#125;\t\t\t\t\tbreak;//插入结束\t\t\t\t&#125;\t\t\t&#125;\t\t\telse//父亲是祖父的右子树\t\t\t&#123;\t\t\t\tNode* uncle = grandfather-&gt;_left;\t\t\t\t//情况一：叔叔存在且为红\t\t\t\tif (uncle &amp;&amp; uncle-&gt;_col == RED)\t\t\t\t&#123;\t\t\t\t\tparent-&gt;_col = uncle-&gt;_col = BLACK;\t\t\t\t\tgrandfather-&gt;_col = RED; \t\t\t\t\t//继续往上调整\t\t\t\t\tcur = grandfather;\t\t\t\t\tparent = grandfather-&gt;_parent;\t\t\t\t&#125;\t\t\t\telse//情况二+情况三：叔叔不存在或叔叔存在且为黑\t\t\t\t&#123;\t\t\t\t\t//情况二：单旋\t\t\t\t\tif (cur == parent-&gt;_right)\t\t\t\t\t&#123;\t\t\t\t\t\tRotateL(grandfather);\t\t\t\t\t\tparent-&gt;_col = BLACK;\t\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t\t&#125;\t\t\t\t\telse//情况三：双旋\t\t\t\t\t&#123;\t\t\t\t\t\tRotateR(parent);\t\t\t\t\t\tRotateL(grandfather);\t\t\t\t\t\tcur-&gt;_col = BLACK;\t\t\t\t\t\tgrandfather-&gt;_col = RED;\t\t\t\t\t&#125;\t\t\t\t\tbreak;//插入结束\t\t\t\t&#125;\t\t\t&#125; \t\t&#125;\t\t_root-&gt;_col = BLACK; \t\treturn make_pair(newNode, true);\t&#125; \tvoid RotateR(Node* parent)\t&#123;\t\tNode* subL = parent-&gt;_left;\t\tNode* subLR = nullptr; \t\tif (subL)\t\t&#123;\t\t\tsubLR = subL-&gt;_right;\t\t&#125;\t\t//1.左子树的右子树变我的左子树\t\tparent-&gt;_left = subLR; \t\tif (subLR)\t\t&#123;\t\t\tsubLR-&gt;_parent = parent;\t\t&#125; \t\t//左子树变父亲\t\tsubL-&gt;_right = parent;\t\tNode* parentParent = parent-&gt;_parent;\t\tparent-&gt;_parent = subL;  \t\tif (parent == _root)//parent是根\t\t&#123;\t\t\t_root = subL;\t\t\t_root-&gt;_parent = nullptr;\t\t&#125;\t\telse//parent不是根，是子树\t\t&#123;\t\t\tif (parentParent-&gt;_left == parent)\t\t\t&#123;\t\t\t\t//parent是自己父亲的左子树,将subL作为parent父亲的左孩子\t\t\t\tparentParent-&gt;_left = subL;\t\t\t&#125;\t\t\telse\t\t\t&#123;\t\t\t\t//parent是自己父亲的右子树,将subL作为parent父亲的右孩子\t\t\t\tparentParent-&gt;_right = subL;\t\t\t&#125; \t\t\t//subL的父亲就是parent的父亲\t\t\tsubL-&gt;_parent = parentParent;\t\t&#125;\t&#125; \tvoid RotateL(Node* parent)\t&#123;\t\tNode* subR = parent-&gt;_right;\t\tNode* subRL = nullptr; \t\tif (subR)\t\t&#123;\t\t\tsubRL = subR-&gt;_left;\t\t&#125; \t\t//1.右子树的左子树变我的右子树\t\tparent-&gt;_right = subRL; \t\tif (subRL)\t\t&#123;\t\t\tsubRL-&gt;_parent = parent;\t\t&#125; \t\t//2.右子树变父亲\t\tsubR-&gt;_left = parent;\t\tNode* parentParent = parent-&gt;_parent;\t\tparent-&gt;_parent = subR; \t\tif (parent == _root)//parent是根\t\t&#123;\t\t\t_root = parent;\t\t\t_root-&gt;_parent = nullptr;\t\t&#125;\t\telse//parent不是根，是子树\t\t&#123;\t\t\tif (parentParent-&gt;_left == parent)\t\t\t&#123;\t\t\t\t//parent是自己父亲的左子树,将subR作为parent父亲的左孩子\t\t\t\tparentParent-&gt;_left = subR;\t\t\t&#125;\t\t\telse\t\t\t&#123;\t\t\t\t//parent是自己父亲的右子树,将subR作为parent父亲的右孩子\t\t\t\tparentParent-&gt;_right = subR;\t\t\t&#125; \t\t\t//subR的父亲就是parent的父亲\t\t\tsubR-&gt;_parent = parentParent;\t\t&#125;\t&#125; \t//查找\tNode* Find(const K&amp; key)\t&#123;\t\tKeyOfT kot;\t\tNode* cur = _root;\t\twhile (cur)\t\t&#123;\t\t\tif (kot(cur-&gt;_data) &lt; key)\t\t\t&#123;\t\t\t\tcur = cur-&gt;_right;\t\t\t&#125;\t\t\telse if (kot(cur-&gt;_data) &gt; key)\t\t\t&#123;\t\t\t\tcur = cur-&gt;_left;\t\t\t&#125;\t\t\telse\t\t\t&#123;\t\t\t\treturn cur;\t\t\t&#125;\t\t&#125;\t\treturn nullptr;//空树，直接返回\t&#125; \tbool _CheckBalance(Node* root, int blackNum, int count)\t&#123;\t\tif (root == nullptr)\t\t&#123;\t\t\tif (count != blackNum)\t\t\t&#123;\t\t\t\tcout &lt;&lt; &quot;黑色节点数量不相等&quot; &lt;&lt; endl;\t\t\t\treturn false;\t\t\t&#125;\t\t\treturn true;\t\t&#125; \t\tif (root-&gt;_col == RED &amp;&amp; root-&gt;_parent-&gt;_col == RED)\t\t&#123;\t\t\tcout &lt;&lt; &quot;存在连续红色节点&quot; &lt;&lt; endl;\t\t\treturn false;\t\t&#125; \t\tif (root-&gt;_col == BLACK)\t\t&#123;\t\t\tcount++;\t\t&#125; \t\treturn _CheckBalance(root-&gt;_left, blackNum, count)\t\t\t&amp;&amp; _CheckBalance(root-&gt;_right, blackNum, count);\t&#125; \t//检查是否平衡\tbool CheckBalance()\t&#123;\t\tif (_root == nullptr)\t\t&#123;\t\t\treturn true;\t\t&#125; \t\tif (_root-&gt;_col == RED)\t\t&#123;\t\t\tcout &lt;&lt; &quot;根节点为红色&quot; &lt;&lt; endl;\t\t\treturn false;\t\t&#125; \t\t//找最左路径做黑色节点数量参考值\t\tint blackNum = 0;\t\tNode* left = _root;\t\twhile (left)\t\t&#123;\t\t\tif (left-&gt;_col == BLACK)\t\t\t&#123;\t\t\t\tblackNum++;\t\t\t&#125;\t\t\tleft = left-&gt;_left;\t\t&#125; \t\tint count = 0;\t\treturn _CheckBalance(_root, blackNum, count);\t&#125;  \t//遍历\tvoid _InOrder(Node* root)\t&#123;\t\tif (root == nullptr)\t\t&#123;\t\t\treturn;\t\t&#125; \t\t_InOrder(root-&gt;_left);\t\tcout &lt;&lt; root-&gt;_kv.first &lt;&lt; &quot;:&quot; &lt;&lt; root-&gt;_kv.second &lt;&lt; endl;\t\t_InOrder(root-&gt;_right);\t&#125; \tvoid InOrder()\t&#123;\t\t_InOrder(_root);\t\tcout &lt;&lt; endl;\t&#125;private:\tNode* _root;&#125;;\n\n七、验证代码#pragma once#include &quot;RBTree.h&quot;#include &lt;vector&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include &quot;Map.h&quot;#include &quot;Set.h&quot; int main()&#123;\tdelia::map&lt;int, int&gt; m;\tm.insert(make_pair(1, 1));\tm.insert(make_pair(3, 3));\tm.insert(make_pair(0, 0));\tm.insert(make_pair(9, 9));  \tdelia::set&lt;int&gt; s;\ts.insert(1);\ts.insert(5);\ts.insert(2);\ts.insert(1);\ts.insert(13);\ts.insert(0);\ts.insert(15);\ts.insert(18);  \tdelia::set&lt;int&gt;::iterator sit = s.begin();\twhile (sit != s.end())\t&#123;\t\tcout &lt;&lt; *sit &lt;&lt; &quot; &quot;;\t\t++sit;\t&#125;\tcout &lt;&lt; endl;  \treturn 0;&#125;\n\n","tags":["cpp"]},{"title":"crimson中的设备管理从Nvme 到Seastore","url":"/2025/09/27/nvme-device/","content":"NVMe 的问世，把存储硬件的性能拉到了 “微秒 + 百万 IOPS” 的时代。传统的内核 IO 栈和同步编程模式，反而成了新的瓶颈，无法充分释放硬件潜力。这迫使软件必须进行一次深刻的变革：从 同步 → 异步，从 内核 → 用户态，从 粗粒度锁 → 精细化无锁并发，才能真正发挥出 NVMe 的极限性能。\n在 Ceph 的设想中，未来的存储引擎 SeaStore 正是为了承担这一使命：\n\n同步到异步：完全基于 Seastar 框架，采用 Future&#x2F;Promise 异步编程模型，杜绝阻塞。\n内核到用户态：借助 Seastar 的用户态网络dpdk与 io_uring 用户态 IO 接口，最大程度减少内核切换开销。\n绑核与分片：每个 CPU shard 独立运行，线程与数据强绑定，端到端的数据路径天然无锁。\n无锁化：通过分片隔离与消息传递机制，避免传统锁竞争，让并发更细粒度、更高效。\n\n这样一来，SeaStore 不仅是 Ceph 的“下一代引擎”，更是面向 NVMe 时代的软件范式转型的具体落地。\n\n\n什么是NVMeNVMe是协议，不是设备也不是驱动，就像HTTP是协议，浏览器和服务器是实现，NVMe 定义了SSD和主机控制器之间如何交流。可减少闪存存储和[固态硬盘 (SSD) ]中使用的每个输入&#x2F;输出 (I&#x2F;O) 的系统开销。\n不再兼容HDD时代的AHCI设计限制（单队列、深度32）\n关键特征：\n\n队列模型\n\n支持64K个提交队列&#x2F;完成队列，每个队列深度64k.\n每个cpu核可以有独立队列，减少延迟，充分释放闪存的并行性。\n\n\n寄存器模型优化\n\n一条IO命令只需要写一个doorbell寄存器，而AHCI需要四次寄存器读写\nAHCI模型（SATA SSD&#x2F;HDD 用的传统协议），AHCI定义了一个命令列表，在内存里。\n当你要下发一条I&#x2F;O命令（比如读4KB数据）：\n\nCPU 把命令描述符写入内存里的 Command List。\nCPU 必须更新多个寄存器：写 命令头寄存器 写 命令表基址寄存器写 PRDT（物理区域描述表）寄存器更新 控制寄存器 告诉控制器有新命令,控制器才能去取命令，开始执行\n每个 I&#x2F;O 至少 4 次 MMIO（寄存器读写）开销，消耗大量 CPU cycle（大概 2000~8000 个 cycle），延迟在微秒级。\n\nNVMe模型，定义了提交队列（SQ）&#x2F; 完成队列（CQ）,放在主机内存中。\n下发一条I&#x2F;O命令的步骤：\n\nCPU 把命令写到内存里的 SQ 队列槽位。\nCPU 只需要 写一次 doorbell 寄存器，告诉控制器“SQ head 已经增加”。\n控制器会直接 DMA 去取命令，不需要 CPU 做额外寄存器交互。\n每个 I&#x2F;O 只有 1 次 MMIO（doorbell write），极大减少 CPU 开销和延迟。\n\n\n\n\n延迟极低\n\n单次 I&#x2F;O 延迟 ~10µs 级别，比 AHCI 少一半以上。\n\n\n协议分层清晰：\n\nPCIe 提供物理链路\nNVMe协议再PCIe之上定义逻辑指令集\n\n\n\n为什么传统软件栈不够用？虽然 Bluestore 已经绕过了文件系统（直接管理裸盘），但它仍然存在两个关键瓶颈：\n\n依赖内核 IO 栈  \n\nBluestore 使用 libaio 提交请求，最终还是要走 Linux 内核的 I&#x2F;O 路径。  \nNVMe 的多队列并行性被内核锁与调度部分抵消。\n\n\n多核扩展差  \n\n传统ceph的OSD 内部有大量线程，多个线程共享一个设备队列，需要加锁。  \nNVMe 给了“每个 core 一个队列”的能力，但传统 OSD 架构无法做到真正的无锁扩展。\n\n\n\n结果就是：  \n\nNVMe 单盘明明能跑 100 万 IOPS，Bluestore OSD 只能吃下 20~30 万。  \n延迟也比硬件指标多出几倍。\n\n这就是 Ceph 需要 Crimson&#x2F;Seastore 的根本原因。\n\nSeastar 与 NVMe 的天然契合NVMe 的设计理念，和 Seastar 的 shard 模型几乎是天然匹配的：  \n\n多队列对多 shard  \n\nNVMe：最多 64K 个提交&#x2F;完成队列，每个 core 可独享一个队列。  \nSeastar：应用拆分成多个 shard，每个 core 独立执行。  \n映射关系：一个 shard ↔ 一个 NVMe 队列，无需锁。\n\n\n无锁并行  \n\n每个 core 只 poll 自己的完成队列，不会和其他 core 争抢。  \nI&#x2F;O 提交和完成完全 core-local，避免了传统 OSD 的“队列共享 + 自旋锁”。\n\n\n性能收益直观  \n\n传统 aio + 内核队列：单盘 4KB 随机读 ~40 万 IOPS  \nio_uring + shard 绑定队列：单盘可跑 150 万 IOPS+  \n在 Crimson&#x2F;Seastore 的实验结果中，相比 Bluestore 延迟下降 35 倍，IOPS 提升 23 倍。\n\n\n\n可以说，Seastore 不是“适配 NVMe”，而是“为 NVMe 而生”。\n\n设备特性\n对齐要求\n\n最小 I&#x2F;O 单位：NVMe SSD 通常要求 4KB 对齐，即提交的 I&#x2F;O 地址和长度最好是 4KB 的整数倍。\n**擦除单元 (Erase Block &#x2F; LBA Block)**：\nNAND 闪存以 Block 为单位擦写。\n随机写小于 Block 会触发 读-改-写操作，造成 写放大（Write Amplification），降低寿命和性能\n\n\n\n\n写放大\n\n为了更新少量数据，SSD 需要擦除和重写整个 Block，导致实际写入数据量 &gt; 应用写入数据量\n\n影响：小随机写 IOPS 下降。设备寿命减少。\n优化方式：使用 顺序写 或 批量写 、合理规划 文件系统块大小 与 NVMe 对齐。\n\n延迟指标\n\n单次 I&#x2F;O 延迟通常：消费级 NVMe：读 10–30 µs，写 20–50 µs。企业级 NVMe：读&#x2F;写可 &lt; 10 µs。\n对比SATA SSD：读&#x2F;写50–100 µs  HDD :读&#x2F;写 5–10 ms\n\n\niops与吞吐上限\n\n单盘 IOPS：随机读(4KB) 500k-3000k .随机写（4kB）300k-700k\nNVMe 原生支持多队列，每个 CPU 核心可绑定一个或多个提交队列（这里就体现出crimsn 中使用seastar框架的重要性了）\n在多核 CPU 和多队列的场景下，IOPS 可以线性扩展到数百万，充分利用硬件并行能力。\n\n\nI&#x2F;O调用方式\n\nlibaio(传统linux异步io,适合单线程或者少量队列异步提交)\nio_uring（linux5.1 真正零拷贝和高性能多队列异步I&#x2F;O）\n\n\n高级特性：\n\nNVMe2.0 :在线升级、多命名空间（128个）、多流、原子写、 sanitize、copy、verify、compare等高级特性。\n遵循OCP 2.5规范协议，Telemetry、Latency Monitor、Thermal Throttle等高级特性\n支持SR-IOV • 最多64个V\n灵活数据放置(FDP) • 提升稳态随机写性能，大幅降低写放大，特殊场景下 可实现WAF&#x3D;1\nTCG OPAL2.0安全规范 • 支持OPAL 2.0协议栈 • 支持SM2&#x2F;SM3&#x2F;SM4&#x2F;AES256数据加密\n端到端保护\n持Secure Boot、Firmware安全校验、Format、 Sanitize等多种企业级安全特性\n\n\n\n\n","tags":["nvme","Crimson"]},{"title":"约瑟夫环问题","url":"/2022/10/20/%E7%BA%A6%E7%91%9F%E5%A4%AB%E7%8E%AF/","content":"约瑟夫环问题的三种解决方法\n\n什么是约瑟夫环问题：约瑟夫环问题在不同平台被”优化”描述的不一样，例如在牛客剑指offer叫孩子们的游戏，还有叫杀人游戏，点名……最直接的感觉还是力扣上剑指offer62的描述：圆圈中最后剩下的数字问题描述：0,1,···,n-1这n个数字排成一个圆圈，从数字0开始，每次从这个圆圈里删除第m个数字（删除后从下一个数字开始计数）。求出这个圆圈里剩下的最后一个数字。例如，0、1、2、3、4这5个数字组成一个圆圈，从数字0开始每次删除第3个数字，则删除的前4个数字依次是2、0、4、1，因此最后剩下的数字是3。\n列表循环模拟：这个问题最本质其实就是循环链表的问题，围成一个圈之后，就没有结尾这就是一个典型的循环链表嘛！一个一个顺序报数，那不就是链表的遍历枚举嘛！数到对应数字的出列，这不就是循环链表的删除嘛！\n并且这里还有非常方便的地方：\n循环链表的向下枚举不需要考虑头尾问题，直接node&#x3D;node.next向下循环聊表的删除也不需要考虑头尾问题，直接node.next&#x3D;node.next.next删除当然也有一些需要注意的地方\n形成环形链表很简单，只需要将普通链表的最后一个节点的next指向第一个节点即可\n循环链表中只有一个节点的时候停止返回，即node.next&#x3D;node的时候\n删除，需要找到待删除的前面节点，所以我们删除计数的时候要少即一位，利用前面的那个节点直接删除后面节点即可\n这样，思路明确，直接开撸代码：\nclass Solution &#123;    class node//链表节点    &#123;        int val;        public node(int value) &#123;            this.val=value;        &#125;        node next;    &#125;    public int lastRemaining(int n, int m) &#123;        if(m==1)return n-1;//一次一个直接返回最后一个即可        node head=new node(0);        node team=head;//创建一个链表        for(int i=1;i&lt;n;i++)        &#123;            team.next=new node(i);            team=team.next;        &#125;        team.next=head;//使形成环        int index=0;//从0开始计数        while (head.next!=head) &#123;//当剩余节点不止一个的时候            //如果index=m-2 那就说明下个节点(m-1)该删除了            if(index==m-2)            &#123;                head.next=head.next.next;                index=0;            &#125;            else &#123;                index++;            &#125;            head=head.next;        &#125;        return head.val;    &#125;&#125;\n当然，这种算法太复杂了，大部分的OJ你提交上去是无法AC的，因为超时太严重了，具体的我们可以下面分析。\n有序集合模拟上面使用链表直接模拟游戏过程会造成非常严重非常严重的超时，n个数字，数到第m个出列。因为m如果非常大远远大于m，那么将进行很多次转圈圈。所以我们可以利用求余的方法判断等价最低的枚举次数，然后将其删除即可，在这里你可以继续使用自建链表去模拟，上面的while循环以及上面只需添加一个记录长度的每次求余算圈数即可：\nint len=n;while (head.next!=head) &#123;  if(index==(m-2)%len)  &#123;    head.next=head.next.next;    index=0;    len--;  &#125;  else &#123;    index++;  &#125;  head=head.next;&#125;\n但我们很多时候不会手动去写一个链表模拟，我们会借助ArrayList和LinkedList去模拟，如果使用LinkedList其底层也是链表，使用ArrayList的话其底层数据结构是数组。不过在使用List其代码方法一致。\nList可以直接知道长度，也可删除元素，使用List的难点是一个顺序表怎们模拟成循环链表？\n咱们仔细思考：假设当前长度为n，数到第m个(通过上面分析可以求余让这个有效的m不大于n)删除，在index位置删除。那么删除后剩下的就是n-1长度，index位置就是表示第一个计数的位置，我们可以通过求余得知走下一个删除需要多少步，那么下个位置怎么确定呢？你可以分类讨论看看走的次数是否越界，但这里有更巧妙的方法，可以直接求的下一次具体的位置，公式就是为：\nindex=(index+m-1)%(list.size());\n因为index是从1计数，如果是循环的再往前m-1个就是真正的位置，但是这里可以先假设先将这个有序集合的长度扩大若干倍，然后从index计数开始找到假设不循环的位置index2，最后我们将这个位置index2%(集合长度)即为真正的长度。使用这个公式一举几得，既能把上面m过大循环过多的情况解决，又能找到真实的位置，就是将这个环先假设成线性的然后再去找到真的位置，如果不理解的话可以再看看这个图：这种情况的话大部分的OJ是可以勉强过关的，面试官的层面也大概率差不多的，具体代码为：\nclass Solution &#123;    public int lastRemaining(int n, int m) &#123;        if(m==1)            return n-1;        List&lt;Integer&gt;list=new ArrayList&lt;&gt;();        for(int i=0;i&lt;n;i++)        &#123;            list.add(i);        &#125;        int index=0;        while (list.size()&gt;1)        &#123;            index=(index+m-1)%(list.size());            list.remove(index);        &#125;        return list.get(0);    &#125;&#125;\n递归公式解决我们回顾上面的优化过程，上面用求余可以解决m比n大很多很多的情况(即理论上需要转很多很多圈的情况)。但是还可能存在n本身就很大的情况，无论是顺序表ArrayList还是链表LinkedList去频繁查询、删除都是很低效的。\n所以聪明的人就开始从数据找一些规律或者关系。\n先抛出公式：\nf(n,m)=(f(n-1,m)+m)%nf(n,m)指n个人，报第m个编号出列最终编号\n下面要认真看一下我的分析过程：\n我们举个例子，有0 1 2 3 4 5 6 7 8 9十个数字，假设m为3,最后结果可以先记成f(10,3)，即使我们不知道它是多少。\n当进行第一次时候，找到元素2 删除，此时还剩9个元素，但起始位置已经变成元素3。等价成3 4 5 6 7 8 9 0 1这9个数字重写开始找。\n此时这个序列最终剩下的一个值即为f(10,3)，这个序列的值和f(9,3)不同，但是都是9个数且m等于3，所以其删除位置是相同的，即算法大体流程是一致的，只是各位置上的数字不一样。所以我们需要做的事情是找找这个序列上和f(9,3)值上有没有什么联系。\n寻找过程中别忘记两点，首先可通过**%符号**对数字有效扩充，即我们可以将3 4 5 6 7 8 9 0 1这个序列看成(3,4,5,6,7,8,9,10,11)%10.这里的10即为此时的n数值。\n另外数值如果是连续的，那么最终一个结果的话是可以找到联系的(差值为一个定制)。所以我们可以就找到f(10,3)和f(9,3)值之间结果的关系，可以看下图：所以f(10,3)的结果就可以转化为f(9,3)的表达,后面也是同理：\nf(10,3)=(f(9,3)+3)%10f(9,3)=(f(8,3)+3)%9……f(2,3)=(f(1,3)+3)%2f(1,3)=0\n这样，我们就不用模拟操作，可以直接从数值的关系找到递推的关系，可以轻轻松松的写下代码：\nclass Solution &#123;    int index=0;    public int lastRemaining(int n, int m) &#123;         if(n==1)            return 0;              return (lastRemaining(n-1,m)+m)%n;    &#125;&#125;\n但是递归效率因为有个来回的规程，效率相比直接迭代差一些，也可从前往后迭代：\nclass Solution &#123;    public int lastRemaining(int n, int m) &#123;        int value=0;            for(int i=1;i&lt;=n;i++)            &#123;                value=(value+m)%i;            &#125;            return  value;    &#125;&#125;\n结语","tags":["算法"]},{"title":"crmson预览版.md","url":"/2025/09/19/crimson%E9%A2%84%E8%A7%88%E7%89%88/","content":"Crimson 是 Crimson OSD 的代码名称，它是下一代用于多核心可扩展性的 OSD 。它通过快速网络和存储设备提高性能，采用包括 DPDK 和 SPDK 的顶级技术。BlueStore 继续支持 HDD 和 SSD。Crimson 旨在与早期版本的 OSD 守护进程与类 Ceph OSD 兼容。\nCrimson 基于 SeaStar C++ 框架构建，是核心 Ceph 对象存储守护进程 OSD 组件的新实现，并替换了 Ceph OSD 。Crimson OSD 最小化延迟并增加 CPU 处理器用量。它使用高性能异步 IO 和新的线程架构，旨在最小化上下文切换和用于跨通信的操作间的线程通信。\n以下分析基于 v19.2.1 进行分析。\n\n\n一、架构对比Ceph OSD 是 Ceph 集群的一部分，负责通过网络提供对象访问、维护冗余和高可用性，并将对象持久化到本地存储设备。作为 Classic OSD 的重写版本，Crimson OSD 从客户端和其他 OSD 的角度兼容现有的 RADOS 协议，提供相同的接口和功能。Ceph OSD 的模块（例如 Messenger、OSD 服务和 ObjectStore）在其职责上保持不变，但跨组件交互的形式和内部资源管理经过了大幅重构，以应用无共享设计和自下而上的用户空间任务调度。\n经典 OSD 的架构对多核处理器并不友好，因为每个组件都包含工作线程池，并且每个组件之间共享队列。举个简单的例子，一个 PG 操作首先需要由一个 Messenger 工作线程处理，将原始数据流组装或解码成一条消息，然后放入消息队列进行调度。之后， PG 工作线程获取该消息，经过必要的处理后，将请求以事务的形式交给 ObjectStore 。事务提交后， PG 将完成操作，并通过发送队列和 Messenger 工作线程再次发送回复。虽然可以通过向池中添加更多线程将工作负载扩展到多个 CPU ，但这些线程默认共享资源，因此需要使用锁，从而引入争用。实际情况会更加复杂，因为每个组件内部都会实现更多的线程池，并且如果跨 OSD 进行复制，数据路径也会更长。\n\n经典架构面临的一个主要挑战是，锁争用开销会随着任务和核心数量的增加而迅速增长，并且每个锁定点在某些情况下都可能成为扩展瓶颈。此外，即使在无争用的情况下，这些锁和队列也会产生延迟成本。多年来，人们在分析和优化更细粒度的资源管理和快速路径实现以跳过排队方面付出了巨大的努力。未来唾手可得的成果将会减少，在类似的设计下，可扩展性似乎正在收敛到某个乘数。此外，还存在其他挑战。由于簿记工作会在工作线程之间委派任务，延迟问题将随着线程池和任务队列的出现而恶化。锁可能会强制上下文切换，这会使情况更加糟糕。\nCrimson 项目希望通过无共享设计和运行至完成模型来解决 CPU 的可扩展性问题。该设计的基本原理是强制每个核心（或 CPU）运行一个固定线程，并在用户空间中调度非阻塞任务。请求及其资源按核心进行分片，因此它们可以在同一核心中处理直至完成。理想情况下，所有锁和上下文切换都不再需要，因为每个正在运行的非阻塞任务都拥有 CPU，直到其完成或协同让出。没有其他线程可以同时抢占该任务。如果无需与数据路径中的其他分片通信，则理想的性能将随着核心数量线性扩展，直到 IO 设备达到其极限。这种设计非常适合 Ceph OSD，因为在 OSD 级别，所有 IO 都已按 PG 分片。\n\n二、配置解析流程配置解析的代码位于 src/crimson/osd/main.cc 文件中的 auto early_config_result = crimson::osd::get_early_config(argc, argv); 函数，该函数主要逻辑如下:\n\n创建一个子进程，在子进程中尝试解析参数后，将参数编码后通过管道传递给父进程；\n父进程解析并返回参数给 main 函数中；\n\n子进程在 _get_early_config 函数中解析参数，其中 ceph 相关的参数使用 ceph_argparse_early_args 函数解析，并且根据 ceph 的 crimson_seastar_cpu_cores 参数来设置 --cpuset $cpu_cores --thread-affinity 1 ；或者根据 ceph 的 crimson_seastar_num_threads 参数来设置 --smp $smp --thread-affinity 0。注意 crimson_seastar_cpu_cores 参数的优先级高于 crimson_seastar_num_threads 参数。\n之后 main 函数中通过 app.run 函数调用，将解析到的参数传递给 seastar ，进而设置了 seastar 要启动的 shard 的数量及绑定 cpu 的配置。但是由于目前 main 中的 seastar::async 函数逻辑中没有显示的使用 seastar::smp::count 来将任务分发给多个 shard 执行，因此关于日志的配置，prometheus 的配置，crimson osd 的对象均是在 shard 0 （即 PRIMARY_CORE ）上执行的。\n三、网络通信流程在 crimson osd 进程启动的时候，会调用 OSD::start() 函数，其内部会对 public_msgr 和 cluster_msgr 两个对象执行 bind 和 start 操作。\n\nbind 操作: 对应的函数为 SocketMessenger::bind ， 该函数内部最终通过调用 seastar 的 invoke_on_all 下发 seastar::listen(s_addr, lo) 操作给所有 shard ，使所有的 shard 开始监听相同的端口；\nstart 操作: 对应的函数为 SocketMessenger::start ， 该函数内部通过调用 ShardedServerSocket::accept ，并在其内部调用 seastar 的 invoke_on_all 方法使每个 shard 接收新连接请求。每个 shard 接收到请求后，会逐步调用 SocketMessenger::accept &#x3D;&gt; SocketConnection::start_accept &#x3D;&gt; ProtocolV2::start_accept &#x3D;&gt; ProtocolV2::execute_accepting 等函数逐步处理请求，最终会调用到 OSD::do_ms_dispatch 函数正式处理客户端请求。\n\n在 OSD::do_ms_dispatch 函数内部，针对于请求消息的类型，有如下操作：\n\n必须在 PRIMARY_CORE shard 上执行的操作: 包括 CEPH_MSG_OSD_MAP、MSG_COMMAND、MSG_OSD_MARK_ME_DOWN 等；\n其他可以在任意 shard 上执行的操作：包括 CEPH_MSG_OSD_MAP、CEPH_MSG_OSD_OP、MSG_COMMAND 等；\n\n\n\n由于 OSD 中的每个 Shard 都会监听网络信息，所以每个 Shard 都可以处理网络请求；\n但是由于需要对请求按照 PG 映射到 Shard 中，所以内部引入了 pg_to_shard_mapping 的映射结构，每个请求都需要在 Shard 中检索映射表；\n如果当前 Shard 中的映射表中缺少 PG 的映射信息，会将请求发送给 Shard 0 来尝试创建对应的映射记录，并将该记录广播给所有的 Shard ；\n\n对于请求类型为 CEPH_MSG_OSD_OP 的关键代码链路如下:\nc// 处理对应的 op 请求seastar::future&lt;&gt; OSD::handle_osd_op(crimson::net::ConnectionRef conn, Ref&lt;MOSDOp&gt; m)&#123;    return pg_shard_manager.start_pg_operation&lt;ClientRequest&gt;(get_shard_services(), conn, std::move(m)).second;&#125;// 开始 pg 操作template&lt;typename T, typename... Args&gt; auto start_pg_operation(Args&amp;&amp;... args)&#123;......    auto fut =        opref.template enter_stage&lt;&gt;(opref.get_connection_pipeline().await_active)            ......            // 从 pg_to_shard_mapping 中获取 pg 与 shard 的对应关系，            // 如果对应的映射关系不存在，则根据各 shard 的负载情况创建映射关系。            .then([this, &amp;opref] &#123; return get_pg_to_shard_mapping().get_or_create_pg_mapping(opref.get_pgid()); &#125;)            .then_wrapped([this, &amp;logger, op = std::move(op)](auto fut) mutable &#123;                ......                auto core = fut.get();                logger.debug(&quot;&#123;&#125;: can_create=&#123;&#125;, target-core=&#123;&#125;&quot;, *op, T::can_create(), core);                // 处理已知 shard id 的 op 请求                return this-&gt;template with_remote_shard_state_and_op&lt;T&gt;(                    core, std::move(op), [this](ShardServices&amp; target_shard_services, typename T::IRef op) &#123;                        auto&amp; opref = *op;                        auto&amp; logger = crimson::get_logger(ceph_subsys_osd);                        logger.debug(&quot;&#123;&#125;: entering create_or_wait_pg&quot;, opref);                        return opref                            .template enter_stage&lt;&gt;(                                opref.get_pershard_pipeline(target_shard_services).create_or_wait_pg)                            .then([this, &amp;target_shard_services, op = std::move(op)]() mutable &#123;                                if constexpr (T::can_create()) &#123;                                    return this-&gt;template run_with_pg_maybe_create&lt;T&gt;(std::move(op),                                                                                        target_shard_services);                                &#125;                                else &#123;                                    return this-&gt;template run_with_pg_maybe_wait&lt;T&gt;(std::move(op),                                                                                    target_shard_services);                                &#125;                            &#125;);                    &#125;);            &#125;);    return std::make_pair(id, std::move(fut));&#125;// 获取或创建 pg 和 shard 的映射关系seastar::future&lt;core_id_t&gt; PGShardMapping::get_or_create_pg_mapping(spg_t pgid, core_id_t core_expected)&#123;    LOG_PREFIX(PGShardMapping::get_or_create_pg_mapping);    auto find_iter = pg_to_core.find(pgid);    if (find_iter != pg_to_core.end()) &#123;        auto core_found = find_iter-&gt;second;        // 一些校验逻辑        assert(core_found != NULL_CORE);        if (core_expected != NULL_CORE &amp;&amp; core_expected != core_found) &#123;            ERROR(&quot;the mapping is inconsistent for pg &#123;&#125;: core &#123;&#125;, expected &#123;&#125;&quot;, pgid, core_found, core_expected);            ceph_abort(&quot;The pg mapping is inconsistent!&quot;);        &#125;        return seastar::make_ready_future&lt;core_id_t&gt;(core_found);    &#125;    else &#123;        DEBUG(&quot;calling primary to add mapping for pg &#123;&#125; to the expected core &#123;&#125;&quot;, pgid, core_expected);        // 如果没有找到 pg 和 shard 的映射关系，则需要创建映射，        // 创建操作必须由 shard 0 执行。        return container()            .invoke_on(                0,                [pgid, core_expected, FNAME](auto&amp; primary_mapping) &#123;                    auto core_to_update = core_expected;                    // 在 shard 0 中判断对应的映射关系是否存在，                    // 如果存在且校验正常则可使用该映射关系                    auto find_iter = primary_mapping.pg_to_core.find(pgid);                    if (find_iter != primary_mapping.pg_to_core.end()) &#123;                        ......                    &#125;                    else &#123;                        // 如果在 shard 0 中也没有找到映射关系，则创建映射                        ceph_assert_always(primary_mapping.core_to_num_pgs.size() &gt; 0);                        std::map&lt;core_id_t, unsigned&gt;::iterator count_iter;                        if (core_expected == NULL_CORE) &#123;                            // 从 shard 中选择 pg 映射数量最少的最为当前 pg 的关联 shard                            count_iter = std::min_element(                                primary_mapping.core_to_num_pgs.begin(),                                primary_mapping.core_to_num_pgs.end(),                                [](const auto&amp; left, const auto&amp; right) &#123; return left.second &lt; right.second; &#125;);                            core_to_update = count_iter-&gt;first;                        &#125;                        ......                    &#125;                    assert(core_to_update != NULL_CORE);                    // 广播同步                    // 通过 invoke_on_others 确保所有 Core 的映射表同步更新                    // 将变更的映射关系广播给其他所有的 shard                    return primary_mapping.container().invoke_on_others(                        [pgid, core_to_update, FNAME](auto&amp; other_mapping) &#123;                            ......                        &#125;);                &#125;)                ......    &#125;&#125;// 处理 op 请求template&lt;typename T, typename F&gt; auto with_remote_shard_state_and_op(core_id_t core, typename T::IRef&amp;&amp; op, F&amp;&amp; f)&#123;    ceph_assert(op-&gt;use_count() == 1);    // 如果 op 请求的目标 shard 为当前 shard ，则在当前 shard 中处理    if (seastar::this_shard_id() == core) &#123;        auto f_conn = op-&gt;prepare_remote_submission();        op-&gt;finish_remote_submission(std::move(f_conn));        auto&amp; target_shard_services = shard_services.local();        return std::invoke(std::move(f), target_shard_services, std::move(op));    &#125;    ......    // 否则，将对应的 op 请求转发给对应的 shard 处理    logger.debug(&quot;&#123;&#125;: send &#123;&#125; to the remote pg core &#123;&#125;&quot;, opref, cc_seq, core);    return opref.get_handle().complete().then([this, core, cc_seq, op = std::move(op), f = std::move(f)]() mutable &#123;        get_local_state().registry.remove_from_registry(*op);        auto f_conn = op-&gt;prepare_remote_submission();        return shard_services.invoke_on(            core,            [this, cc_seq, f = std::move(f), op = std::move(op), f_conn = std::move(f_conn)](                auto&amp; target_shard_services) mutable &#123;                op-&gt;finish_remote_submission(std::move(f_conn));                target_shard_services.local_state.registry.add_to_registry(*op);                return this-&gt;template process_ordered_op_remotely&lt;T&gt;(                    cc_seq, target_shard_services, std::move(op), std::move(f));            &#125;);    &#125;);&#125;\n\n四、线程模型在服务启动时会通过解析 crimson_seastar_cpu_cores 或 crimson_seastar_num_threads 这两个配置来设置 seastar 框架的并发 shard 数量，之后在 PRIMARY_CORE 初始化环境，并通过 seastar 的 invoke_on、invoke_on_others、invoke_on_all、seastar::smp::submit_to 等方法来给 shard 下发任务，从而实现 osd 中相互独立的 shard 任务模型。\n4.1、shard 相关任务seastar 提供的不同的下发任务的方法比较:\n\n\n\n接口\n目标 Shard\n是否依赖 sharded 容器\n典型用途\n\n\n\ninvoke_on\n指定单个 Shard\n是\n访问特定 Shard 上的对象\n\n\ninvoke_on_others\n除当前 Shard 外的所有\n是\n广播操作（排除当前 Shard）\n\n\ninvoke_on_all\n所有 Shard（包括当前）\n是\n全局初始化&#x2F;清理\n\n\nsmp::submit_to\n指定单个 Shard\n否\n任意跨 Shard 任务\n\n\ninvoke_on 的部分操作如下:\nc// 更新配置值并通知所有观察者container().invoke_on(...)// 在 0 号 shard 上停止 shardsthis-&gt;container().invoke_on(0, [](auto&amp; ss) &#123; ... &#125;)// 在 0 号 shard 上新增 pg 和 shard 的映射关系container().invoke_on(0, [pgid, core_expected, FNAME](auto&amp;// 在 0 号 shard 上移除 pg 和 shard 的映射关系container().invoke_on(0, [pgid, FNAME](auto&amp; primary_mapping) &#123; ... &#125;)// 转发请求给特定 shardshard_services.invoke_on(core, ... )\n\ninvoke_on_others 的部分操作如下:\nc// 更新 proxy 配置container().invoke_on_others(...)// 广播 pg shard 新增映射记录primary_mapping.container().invoke_on_others(...)// 广播 pg shard 移除映射记录primary_mapping.container().invoke_on_others(...)\n\ninvoke_on_all 的部分操作如下:\ncseastar::listenss.listener-&gt;accept()ss.listener-&gt;abort_accept()ss.listener.reset()local_store.mkfs()local_store.mount()local_store.umount()local_store.mount_managers()local_store.set_secondaries(...)local_store.mkfs_managers()local_device.do_shard_mount()local_device.shard_mount()local_device.shard_mkfs()local_service.local_state.stop_pgs()local_service.local_state.broadcast_map_to_pgs(local_service, epoch)local_service.local_state.osdmap_gate.got_map(epoch)local_service.local_state.set_up_epoch(e)local_service.local_state.update_shard_superblock(superblock)local.local_state.update_map(...)local.local_state.stop_registry()osd_state._set_active()osd_state._set_stopping()\n\nseastar::smp::submit_to 的部分操作如下:\nc// 在 shard 0 上处理 CEPH_MSG_OSD_MAP/MSG_COMMAND/MSG_OSD_MARK_ME_DOWN 消息seastar::smp::submit_to(PRIMARY_CORE, ... )\n\n4.2、线程示例当 crimson_seastar_num_threads 设置为 2 的时候，crimson osd 的线程情况:\nbash[root@bugwz.host build]# ps -T -p 270088    PID    SPID TTY          TIME CMD 270088  270088 pts/11   00:30:31 crimson-osd 270088  270130 pts/11   00:22:41 reactor-1 270088  270131 pts/11   00:00:00 syscall-0 270088  270132 pts/11   00:00:00 syscall-1 270088  270133 pts/11   00:00:00 crimson-osd 270088  270134 pts/11   00:00:00 reactor-1\n\n当 crimson_seastar_num_threads 设置为 8 的时候，crimson osd 的线程情况:\nbash[root@bugwz.host build]# ps -T -p 345103    PID    SPID TTY          TIME CMD 345103  345103 pts/15   00:00:04 crimson-osd 345103  345145 pts/15   00:00:02 reactor-1 345103  345146 pts/15   00:00:02 reactor-2 345103  345147 pts/15   00:00:02 reactor-3 345103  345148 pts/15   00:00:02 reactor-4 345103  345149 pts/15   00:00:02 reactor-5 345103  345150 pts/15   00:00:02 reactor-6 345103  345151 pts/15   00:00:02 reactor-7 345103  345152 pts/15   00:00:00 syscall-7 345103  345153 pts/15   00:00:00 syscall-0 345103  345154 pts/15   00:00:00 syscall-4 345103  345155 pts/15   00:00:00 syscall-3 345103  345156 pts/15   00:00:00 syscall-2 345103  345157 pts/15   00:00:00 syscall-5 345103  345158 pts/15   00:00:00 syscall-1 345103  345159 pts/15   00:00:00 syscall-6 345103  345160 pts/15   00:00:00 crimson-osd 345103  345161 pts/15   00:00:00 reactor-1 345103  345162 pts/15   00:00:00 reactor-4 345103  345163 pts/15   00:00:00 reactor-5 345103  345164 pts/15   00:00:00 reactor-6 345103  345165 pts/15   00:00:00 reactor-7 345103  345166 pts/15   00:00:00 reactor-2 345103  345167 pts/15   00:00:00 reactor-3\n\n五、存储模块设计5.1、后端对象存储类型main 函数中会通过 crimson::os::FuturizedStore::create 函数来创建 store 对象。根据 osd_objectstore 和 osd_data 参数来配置 store 对象。其中 osd_objectstore 参数指定了后端对象存储的类型，支持的参数有 alienstore/cyanstore/seastore ，默认为 alienstore （即后端存储为 bluestore ）。其中 osd_data 参数指定了数据存储目录（比如当使用 vstart.sh 部署集群时，对应的配置默认为 ./build/dev/osd$id ）。\n对象存储类型:\n\nalienstore: 是 seastar 线程中的一个代理，主要是与 bluestore 进行通信。由于 io 任务会与 bluestore 进行通信，因此无需针对多个 osd 分片进行特殊处理。BlueStore 中没有针对 crimson 的定制，因为 bluestore 依赖于第三方 RocksDB 项目，而该项目仍然采用线程化设计，因此无法真正将其扩展为无共享设计。然而，在 crimson 能够提供经过优化且足够稳定的原生存储后端 seastore 之前，使用合理的开销来换取完善的存储后端解决方案是可以接受的。\ncyanstore: crimson osd 中的 cyanstore 与 classic osd 中的 memstore 相对应。为了支持多分片，唯一的变化是每个分片创建独立的 cyanstore 实例。一个目标是确保虚拟 IO 操作能够在同一核心中完成，以帮助识别 osd 级别的可扩展性问题（如果有）。另一个目标是在 osd 级别与 Classic 进行直接性能比较，而不会受到 objectstore 的复杂影响。\nseastore: seastore 是 crimson osd 的原生 objectstore 解决方案，它使用 seastar 框架开发并采用相同的设计原则。\n\n在 seastore 初始化的时候，会根据 seastore_main_device_type 参数来初始化 seastore 主设备，该参数可选值为 SSD/RANDOM_BLOCK_SSD （代码中还实现了 HDD/ZBD ，但是目前并不支持） ，默认为 SSD 。 在调用 Device::make_device(root, d_type) 函数创建 device 的过程中，会针对不同的设备类型又做了一些区分。\nseastore 设备类型对比:\n\n\n\ndevice_type\nbackend_type\ncreate func\n\n\n\nHDD\nbackend_type_t::SEGMENTED\nSegmentManager::get_segment_manager\n\n\nSSD\nbackend_type_t::SEGMENTED\nSegmentManager::get_segment_manager\n\n\nZBD\nbackend_type_t::SEGMENTED\nSegmentManager::get_segment_manager\n\n\nRANDOM_BLOCK_SSD\nbackend_type_t::RANDOM_BLOCK\nget_rb_device\n\n\n5.2、段存储格式信息当使用 vstart.sh 脚本部署测试集群后会发现 build/dev/osd*/ 目录下会存在一个 block 文件，该文件对应的就是一个 osd 组件后端的对象存储，由于一个 osd 中可能会启用多个 seastar shard ，并且由于 shard 间数据的隔离，因此需要对这大块存储空间进行切割，使每个 shard 各负责一块空间，从而实现操作数据的隔离。\n后端存储的格式化规则:\n\n开始部分为 superblock 空间，存储这个该存储空间的规划及使用信息；\n剩余空间平均分配给每个 shard ，实现独立的操作空间；\n\n创建 superblock 及 shard 空间规划函数如下:\ncusing std::vector;static block_sm_superblock_t make_superblock(device_id_t device_id, device_config_t sm_config, const seastar::stat_data&amp; data)&#123;    LOG_PREFIX(block_make_superblock);    using crimson::common::get_conf;    // seastore_device_size 默认为 50G    auto config_size = get_conf&lt;Option::size_t&gt;(&quot;seastore_device_size&quot;);    size_t size = (data.size == 0) ? config_size : data.size;    // 单个 segment 的大小，默认为 64M    auto config_segment_size = get_conf&lt;Option::size_t&gt;(&quot;seastore_segment_size&quot;);    // 计算 segment 数量： 总大小除以单个 segment 的大小    size_t raw_segments = size / config_segment_size;    // 计算每个 shard 所需要的段状态跟踪器大小    // 默认为一个 data.block_size 大小，如果计算出的每个 shard 所管理的 segments 数量超过 data.block_size 大小，    // 则返回超过 segments 数量的 data.block_size 的倍数值。    //    // seastar::smp::count 为 crimson osd 启动时指定的 shard 数量    // data.block_size 默认为 4096    size_t shard_tracker_size = SegmentStateTracker::get_raw_size(raw_segments / seastar::smp::count, data.block_size);    // 计算全部 shard 的段状态跟踪器的总大小    size_t total_tracker_size = shard_tracker_size * seastar::smp::count;    // 初始的偏移应该从 superblock 之后开始    size_t tracker_off = data.block_size;    // 计算减去 superblock 及所有段状态跟踪器总大小之后的剩余空间可分配的 segments 数量    size_t segments = (size - tracker_off - total_tracker_size) / config_segment_size;    // 计算每个 shard 可分配的 segments 数量    size_t segments_per_shard = segments / seastar::smp::count;    // 初始化每个 shard 信息    vector&lt;block_shard_info_t&gt; shard_infos(seastar::smp::count);    for (unsigned int i = 0; i &lt; seastar::smp::count; i++) &#123;        // 每个 shard 管理的 segments 总大小        shard_infos[i].size = segments_per_shard * config_segment_size;        // 每个 shard 管理的 segments 数量        shard_infos[i].segments = segments_per_shard;        // 标记每个 shard 的段状态跟踪器的在全部空间中的偏移        shard_infos[i].tracker_offset = tracker_off + i * shard_tracker_size;        // 标记每个 shard 的 segment 数据起始位置在全部空间中的偏移        shard_infos[i].first_segment_offset = tracker_off + total_tracker_size + i * segments_per_shard * config_segment_size;    &#125;    // 输出日志信息    INFO(&quot;&#123;&#125; disk_size=&#123;&#125;, segment_size=&#123;&#125;, block_size=&#123;&#125;&quot;,         device_id_printer_t&#123;device_id&#125;,         size,         uint64_t(config_segment_size),         data.block_size);    for (unsigned int i = 0; i &lt; seastar::smp::count; i++) &#123;        INFO(&quot;shard &#123;&#125; infos:&quot;, i, shard_infos[i]);    &#125;    // 返回 superblock 全部信息    return block_sm_superblock_t&#123;seastar::smp::count, config_segment_size, data.block_size, shard_infos, std::move(sm_config)&#125;;&#125;\n\n六、客户端使用方式由于 crimson osd 只支持 message v2 协议，所以我们在挂载 cephfs&#x2F;cephrbd 等的时候需要使用 message v2 的方式。\n相关命令:\nbash# 挂载 cephrbd - kernel 方式rbd device map -t krbd rbdpool/rbdimg01 -o mount_timeout=5,ms_mode=crc# 挂载 cephrbd - nbd 方式rbd device map -t nbd rbdpool/rbdimg01# 取消挂载 cephrbdrbd device unmap rbdpool/rbdimg01 -t krbdrbd device unmap rbdpool/rbdimg01 -t nbd# 挂载 cephfs - kernel 方式mount -t ceph 10.10.10.1:3300:/ /mnt/kernel-cephfs -o name=admin,secret=AQBVokZoak+LJRAAqgeJr6j77v729bfvBl/Z3g==,ms_mode=crc,mount_timeout=5# 挂载 cephfs - fuse 方式ceph-fuse -c /etc/ceph/ceph.conf -n client.admin -m 10.10.10.1:3300 /mnt/fuse-cephfs --client_mountpoint /# 取消挂载 cephfsumount /mnt/kernel-cephfsfusermount -u /mnt/fuse-cephfs\n\n相关代码实现:\nc// 筛选监听地址entity_addrvec_t pick_addresses(int what)&#123;    LOG_PREFIX(osd.cc : pick_addresses);    entity_addrvec_t addrs;    crimson::common::CephContext cct;    // 仅筛选 message v2 的地址    const auto flags = what | CEPH_PICK_ADDRESS_MSGR2;    if (int r = ::pick_addresses(&amp;cct, flags, &amp;addrs, -1); r &lt; 0) &#123;        throw std::runtime_error(&quot;failed to pick address&quot;);    &#125;    for (auto addr : addrs.v) &#123;        INFO(&quot;picked address &#123;&#125;&quot;, addr);    &#125;    return addrs;&#125;// 接收请求seastar::future&lt;&gt; SocketMessenger::start(const dispatchers_t&amp; _dispatchers)&#123;    assert(seastar::this_shard_id() == sid);    dispatchers.assign(_dispatchers);    if (listener) &#123;        // 仅支持 message v2 的地址        ceph_assert(get_myaddr().is_msgr2());        ceph_assert(get_myaddr().get_port() &gt; 0);        // 接收端口请求        return listener-&gt;accept([this](SocketRef _socket, entity_addr_t peer_addr) &#123;            assert(get_myaddr().is_msgr2());            SocketFRef socket = seastar::make_foreign(std::move(_socket));            if (listener-&gt;is_fixed_shard_dispatching()) &#123;                return accept(std::move(socket), peer_addr);            &#125;            else &#123;                return seastar::smp::submit_to(sid, [this, peer_addr, socket = std::move(socket)]() mutable &#123;                    return accept(std::move(socket), peer_addr);                &#125;);            &#125;        &#125;);    &#125;    return seastar::now();&#125;\n\n七、其他特性实现7.1、冷热存储分离当使用 vstart.sh 脚本部署测试的时候，我们会发现 --seastore-secondary-devs 和 --seastore-secondary-devs-type 配置，如果指定了这两个参数，该脚本便会通过 dd 格式化对应盘，然后创建 ./dev/osd$id/block.$type.1 目录，之后执行 ln -s $device ./dev/osd$id/block.$type.1/block 创建一个软链文件。详细的代码可以查看: https://github.com/ceph/ceph/blob/v19.2.1/src/vstart.sh#L1194 。\n按照官方解释这两个参数是用来指定次要块设备的列表和类型，进一步分析 crimson 官方文档 我们发现这两个配置可用于实现 ceph 的冷热存储分离特性，即随着时间的推移逐步将较快设备（主设备）中的冷数据迁移到较慢的设备（次要设备）中，通常要求次要设备的速度不应该比主设备更快。我们能发现该特性与 Cache Tiering 特性比较相似，之后也会做一下对比分析。\n关于主设备剔除数据到次要设备的相关参数:\n\nseastore_multiple_tiers_stop_evict_ratio: 当主设备的使用率低于此值时，停止将冷数据逐出到冷层。默认值为 0.5 。\nseastore_multiple_tiers_default_evict_ratio: 当主设备的使用率达到此值时，开始将冷数据迁移到次要设备。默认值为 0.6 。\nseastore_multiple_tiers_fast_evict_ratio: 当主设备的使用率达到此值时，开始执行快速逐出。默认值为 0.7 。\n\n八、模块解析九、代码逻辑梳理main 函数中启动的 seastar::async 异步任务的关键逻辑如下:\n\n设置日志级别并打开日志文件；\n启动 prometheus api server ；\n创建 client/cluster/hb_front/hb_back 消息管理器 SocketMessenger ；\n创建 store 对象；\n创建、初始化、启动 crimson osd 对象；\n\n9.1、消息管理器创建逻辑通过调用 crimson::net::Messenger::create 函数来依次创建 client/cluster/hb_front/hb_back 消息管理器，最终创建的对象类型为 SocketMessenger 。\n其中创建 client/cluster 消息对象的时候 dispatch_only_on_this_shard 参数为 false ，意味着接收到的消息可能会交由其他的 shard 进行处理；创建 hb_front/hb_back 消息对象的时候 dispatch_only_on_this_shard 参数为 true ，意味着接收到的消息仅会由当前 shard 处理。\n9.2、store 对象创建逻辑通过调用 crimson::os::FuturizedStore::create 函数来创建 store 对象。根据 osd_objectstore 和 osd_data 参数来配置 store 对象。其中 osd_objectstore 参数指定了后端对象存储的类型，支持的参数有 alienstore/cyanstore/seastore ，默认为 alienstore （即后端存储为 bluestore ）。其中 osd_data 参数指定了数据存储目录（比如当使用 vstart.sh 部署集群时，对应的配置默认为 ./build/dev/osd$id ）。\ncrimson 支持以下三个 objectstore 后端:\n\nalienstore: 提供与早期版本的对象存储（即 BlueStore）的兼容性。\ncyanstore: 用于测试的模拟后端，由易失性内存实施。此对象存储在典型的 osd 中的 memstore 后建模。\nseastore: 为 crimson osd 设计的新对象存储。对多个分片支持的路径因后端的特定目标而异。\n\n9.3、crimson osd mkfs 逻辑由于在启动 osd 组件之前，我们需要初始化 osd 的文件系统环境，为此需要执行 OSD::mkfs 函数（相关操作顺序可以参考 vstart.sh 脚本中在启动 osd 组件的步骤，其中在启动 osd 之前需要先对其存储路径的环境执行 mkfs 操作。）\nOSD::mkfs 函数中关键逻辑为:\nc1. store.start()store.mkfs(osd_uuid) // 重点2. store.mount()3. open_or_create_meta_coll(store)4. _write_superblock(...)5. store.write_meta(...)6. store.umount()7. store.stop()\n\n\n1. store.start()\n\n由于 store 的类型存在三种： alienstore/cyanstore/seastore ， 所以对应的 start 逻辑也有三种。由于 alienstore 只是 bluestore 的代理，且实现比较简单，为此不做介绍；而 cyanstore 是作为一个内存存储模块而存在，仅作为开发测试使用，为此这里也不做介绍；所以以下仅介绍 seastore 的实现逻辑，对应的函数为 SeaStore::start 。\nSeaStore::start 函数中关联逻辑为:\nc1. Device::make_device(root, d_type)2. device-&gt;start()3. shard_stores.start(root, device.get(), is_test)\n\n1. Device::make_device(root, d_type) 逻辑解析:在 seastore 中有一个 seastore_main_device_type 参数，用于设置 seastore 主设备的类型，可选值为 SSD/RANDOM_BLOCK_SSD （代码中还实现了 HDD/ZBD ，但是目前并不支持） ，默认为 SSD 。\nDevice::make_device(root, d_type) 函数内部在创建 device 的过程中，会针对不同的设备类型又做了一些区分，详细的类别分类如下:\n\n\n\ndevice_type\nbackend_type\ncreate func\n\n\n\nHDD\nbackend_type_t::SEGMENTED\nSegmentManager::get_segment_manager\n\n\nSSD\nbackend_type_t::SEGMENTED\nSegmentManager::get_segment_manager\n\n\nZBD\nbackend_type_t::SEGMENTED\nSegmentManager::get_segment_manager\n\n\nRANDOM_BLOCK_SSD\nbackend_type_t::RANDOM_BLOCK\nget_rb_device\n\n\n由于 seastore_main_device_type 默认为 SSD ，所以会通过 SegmentManager::get_segment_manager 函数来来创建一个 segment_manager::block::BlockSegmentManager 对象。\n2. device-&gt;start() 逻辑解析:当执行 device-&gt;start() 的时候，调用的就是 BlockSegmentManager::start 方法，继而调用的是 shard_devices.start(device_path, superblock.config.spec.dtype) ，由于 shard_devices 的类型为 seastar::sharded , 所以这里相当于调用了 seastar::sharded::start 函数来初始化了 BlockSegmentManager 对象。\n3. shard_stores.start(root, device.get(), is_test) 逻辑解析:之后的 shard_stores.start(root, device.get(), is_test) 函数执行中，由于 shard_stores 也是一个 seastar::sharded 封装的对象，所以其内部相当于调用了 seastar::sharded::start 函数来初始化了 SeaStore::Shard 对象。\n\n2. store.mount()\n\nstore.mount() 函数对应的是 SeaStore::mount 函数。\nSeaStore::mount 函数中关键逻辑为:\ncdevice-&gt;mount()device-&gt;get_sharded_device().get_secondary_devices()Device::make_device(path, dtype)sec_dev-&gt;start()sec_dev-&gt;mount()set_secondaries()\n\ndevice-&gt;mount() 函数对应的是 BlockSegmentManager::mount 函数，这个之前解释过，其内部通过调用 shard_devices.invoke_on_all 来触发在每个 shard 中执行 local_device.shard_mount() 函数，因此每个 shard 中调用的函数其实是 BlockSegmentManager::shard_mount() ，该函数内部的执行逻辑主要包括打开 device ，读取 superblock 信息，校验 superblock 信息，更新 tracker 信息等。\n\n3. open_or_create_meta_coll(store)\n\nopen_or_create_meta_coll(store) 对应的函数是 OSD::open_or_create_meta_coll 。\nOSD::open_or_create_meta_coll 函数中关键逻辑为:\ncstore.get_sharded_store().open_collection(coll_t::meta())store.get_sharded_store().create_new_collection(coll_t::meta())OSDMeta(ch, store.get_sharded_store())\n\n\n4. _write_superblock(…)\n\n_write_superblock(...) 的完整调用为 _write_superblock(store, std::move(meta_coll), std::move(superblock)) ，其对应的函数是 OSD::_write_superblock 。其内部主要的逻辑为将 superblock 信息写入存储中。\nOSD::_write_superblock 函数中关键逻辑为:\ncmeta_coll.load_superblock()meta_coll.create(t)meta_coll.store_superblock(t, superblock)store.get_sharded_store().do_transaction(meta_coll.collection(), std::move(t))\n\n\n5. store.write_meta(…)\n\nstore.write_meta(…) 对应很多写元信息的操作，操作的元信息包括 ceph_fsid ，magic ，whoami ，osd_key ， osdspec_affinity ， ready 等字段。这些信息位于 osd 运行目录的各个配置对应的文件中。\n\n6. store.umount()\n\nstore.umount() 对应的函数为 SeaStore::umount ， 其内部会同通过调用 shard_stores.invoke_on_all 函数，让每个 shard 执行 local_store.umount() 函数。\n\n7. store.stop()\n\nstore.stop() 对应的函数为 SeaStore::stop 。\nSeaStore::stop 函数中关键逻辑为:\ncsec_dev-&gt;stop()secondaries.clear()device-&gt;stop()shard_stores.stop()\n\n9.3.1、store.mkfs(osd_uuid)SeaStore::mkfs 函数中关键逻辑为:\nc1. read_meta(&quot;mkfs_done&quot;)2. seastar::open_directory(root)        root_f-&gt;list_directory(...)            Device::make_device(path, dtype)            secondaries.emplace_back(std::move(sec_dev))            p_sec_dev-&gt;start()            p_sec_dev-&gt;mkfs()            set_secondaries()3. device-&gt;mkfs(...)4. device-&gt;mount()5. local_store.mkfs_managers() // shard_stores.invoke_on_all(...) // 重点6. prepare_meta(new_osd_fsid)7. umount()\n\n\n\nread_meta(“mkfs_done”)\n\n\nread_meta(&quot;mkfs_done&quot;) 用于校验之前是否已经执行过 mkfs 操作，监测方式为读取 store 目录中的 mkfs_done 文件中的内容。\n\n\nseastar::open_directory(root)\n\n\nseastar::open_directory(root) 的逻辑为检索 store 目录中的文件，筛选前缀名为 block. 的文件&#x2F;目录，通过解析该文件&#x2F;目录的后缀，从而尝试调用 Device::make_device(path, dtype) 函数来创建对应的 device ， 进而操作对应的 device 执行 start 和 mkfs 函数操作。\n\n\ndevice-&gt;mkfs(…)\n\n\ndevice-&gt;mkfs(...) 对应的完整函数为 device-&gt;mkfs(device_config_t::create_primary(new_osd_fsid, id, d_type, sds)) ， 由于 seastore_main_device_type 默认为 SSD ，所以这里的 device-&gt;mkfs 指的是 BlockSegmentManager::mkfs 函数。\nBlockSegmentManager::mkfs 函数中关键逻辑为:\ncshard_devices.local().primary_mkfs(sm_config)    check_create_device(device_path, size)    open_device(device_path)    make_superblock(get_device_id(), sm_config, stat)    write_superblock(get_device_id(), device, sb)    device.close()local_device.shard_mkfs() // shard_devices.invoke_on_all(...)    open_device(device_path)    read_superblock(device, sd)    sb.validate()    tracker.reset(new SegmentStateTracker(shard_info.segments, sb.block_size))    tracker-&gt;write_out(get_device_id(), device, shard_info.tracker_offset)    device.close()\n\n其中 shard_devices.local().primary_mkfs(sm_config) 对应的函数为 BlockSegmentManager::primary_mkfs 。其内部逻辑如下:\n\ncheck_create_device(device_path, size): 通过 seastar::open_file_dma 函数来打开对应的 block 文件，并通过 f.truncate 和 f.allocate(0, size) 函数来调整对应文件的大小，用于后续存储数据。该步骤中的 seastore_block_create 配置用于控制是否创建 block ， 该参数默认为 true ；seastore_device_size 配置用于控制 block 的文件大小，该参数默认为 50GB 。\nopen_device(device_path): 通过 seastar::open_file_dma 方法来打开对应的 block 文件，用于后续的数据操作。\nmake_superblock(get_device_id(), sm_config, stat): 初始化 superblock 信息。其内部根据 seastar::smp::count 的数量，seastore_segment_size 参数（用于控制单个 segment 的大小，默认为 64M ）等信息来初始化 superblock 信息。\nwrite_superblock(get_device_id(), device, sb): 将序列化后的 superblock 信息写入 block 的文件头部。\ndevice.close(): 关闭打开的 device 。\n\n之后通过调用 shard_devices.invoke_on_all(...) 函数，该函数是 Seastar 框架中使用的方法，用于在所有的 seastar shard 上执行给定的函数。之后每个 shard 上执行 local_device.shard_mkfs() 函数。其内部回依次打开 device ，读取 superblock 信息，校验 superblock 信息，更新 tracker 信息等；之后便关闭 device 。\n\n\ndevice-&gt;mount()\n\n\ndevice-&gt;mount() 对应的函数为 BlockSegmentManager::mount 。\nBlockSegmentManager::mount 函数中关键逻辑为:\nclocal_device.shard_mount() // shard_devices.invoke_on_all(...)\n\n这里也是通过调用 shard_devices.invoke_on_all 来触发在每个 shard 中执行 local_device.shard_mount() 函数，因此每个 shard 中调用的函数其实是 BlockSegmentManager::shard_mount() ，该函数内部的执行逻辑主要包括打开 device ，读取 superblock 信息，校验 superblock 信息，更新 tracker 信息等。\n\n\nlocal_store.mkfs_managers()\n\n\n接着又通过调用 shard_stores.invoke_on_all(...) 来触发在每个 shard 中执行 local_store.mkfs_managers() 操作，对应的函数为 SeaStore::Shard::mkfs_managers 。\nSeaStore::Shard::mkfs_managers 函数中关键逻辑为:\ncinit_managers()transaction_manager-&gt;mkfs()init_managers()transaction_manager-&gt;mount()repeat_eagain(...)    transaction_manager-&gt;with_transaction_intr(...)        onode_manager-&gt;mkfs(t)        collection_manager-&gt;mkfs(t)        transaction_manager-&gt;write_collection_root(t, coll_root)        transaction_manager-&gt;submit_transaction(t)\n\n其中 init_managers() 函数指的是 SeaStore::Shard::init_managers() 函数，其内部会初始化 transaction_manager ， collection_manager ， onode_manager 对象。\n\ntransaction_manager: 初始化函数为 TransactionManagerRef make_transaction_manager ，该对象显然用于管理存储设备上的事务。\ncollection_manager: 初始化函数为 FlatCollectionManager::FlatCollectionManager ；\nonode_manager: 初始化函数为 FLTreeOnodeManager::FLTreeOnodeManager ；\n\ntransaction_manager 相关执行逻辑:\n\ntransaction_manager-&gt;mkfs(): 对应 TransactionManager::mkfs 函数；\ntransaction_manager-&gt;mount(): 对应 TransactionManager::mount 函数；\ntransaction_manager-&gt;with_transaction_intr(...): 对应 ExtentCallbackInterface::with_transaction_intr 函数；\n\n其中 TransactionManager::mkfs 函数中关键逻辑为:\ncepm-&gt;mount()journal-&gt;open_for_mkfs()epm-&gt;open_for_write()with_transaction_intr(...)close()\n\n其中 TransactionManager::mount 函数中关键逻辑为:\nccache-&gt;init()epm-&gt;mount()journal-&gt;replay(...)journal-&gt;open_for_mount()journal-&gt;get_trimmer().set_journal_head(start_seq)with_transaction_weak(...)epm-&gt;open_for_write()epm-&gt;start_background()\n\nTODO:\nonode_manager 相关执行逻辑:\n相关操作为 onode_manager-&gt;mkfs(t) ， 对应的函数为 FLTreeOnodeManager::mkfs 函数。 之后继续调用 Btree::mkfs &#x3D;&gt; Node::mkfs\nTODO:\ncollection_manager 相关执行逻辑:\n相关操作为 collection_manager-&gt;mkfs(t)\nTODO:\n\n\nprepare_meta(new_osd_fsid)\n\n\nprepare_meta(new_osd_fsid) 函数对应的是 SeaStore::prepare_meta 函数，其内部主要是写入一些元信息到对应的数据目录的文件中，包括向 fsid 文件中写入集群 id 信息；向 type 文件中写入后后端存储类型（比如 seastore ） ； 往 mkfs_done 文件中写入 yes 。\n\n\numount()\n\n\numount() 函数对应的是 SeaStore::umount 函数，其内部会通过 shard_stores.invoke_on_all 函数通知所有的 shard 执行 local_store.umount() 操作。\n9.4、crimson osd start 逻辑当 osd 通过 mkfs 初始化之后才会被正式的启动，这时候就会调用 OSD::start 函数启动。需要注意该函数内部限制当前的 shard 为 PRIMARY_CORE 。其中 store.start() 和 store.mount() 的执行逻辑之前在 osd mkfs 的逻辑中已经描述过了，这里不再赘述。部分实现比较详细或逻辑接近，因此放在一块一起解释。\nOSD::start 函数中关键逻辑为:\ncstore.start()1. pg_to_shard_mappings.start(...)2. osd_singleton_state.start_single(...)3. osd_states.start()4. shard_services.start(...)5. heartbeat.reset(...)store.mount()6. local_service.report_stats() // shard_services.invoke_on_all(...)7. store.report_stats()8. stats_timer.arm_periodic(...)9. open_meta_coll()10. pg_shard_manager.get_meta_coll().load_superblock()11. pg_shard_manager.set_superblock(superblock)12. pg_shard_manager.get_local_map(superblock.current_epoch)13. pg_shard_manager.update_map(std::move(map))14. local_service.local_state.osdmap_gate.got_map(...) // shard_services.invoke_on_all(...)15. pg_shard_manager.load_pgs(store)16. cluster_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_CLUSTER))    cluster_msgr-&gt;start(dispatchers)    public_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_PUBLIC))    public_msgr-&gt;start(dispatchers)17. monc-&gt;start()    mgrc-&gt;start()18. _add_me_to_crush()19. monc-&gt;renew_subs()20. heartbeat-&gt;start(...)21. start_asok_admin()22. log_client.set_fsid(monc-&gt;get_fsid())23. start_boot()\n\n\n1. pg_to_shard_mappings.start(…)\n\npg_to_shard_mappings.start(...) 的原始调用信息为 pg_to_shard_mappings.start(0, seastar::smp::count) 。由于 pg_to_shard_mappings 的定义为 seastar::sharded pg_to_shard_mappings ，因此这里的 start 函数其实是调用 seastar::sharded::start 函数来初始化了 PGShardMapping 对象。在 PGShardMapping 对象初始化的过程中会向其内部的成员变量 std::map core_to_num_pgs 中添加 seastar::smp::count 个元素。\n\n2. osd_singleton_state.start_single(…)\n\nosd_singleton_state.start_single(...) 的原始调用信息为 osd_singleton_state.start_single(whoami, std::ref(*cluster_msgr), std::ref(*public_msgr), std::ref(*monc), std::ref(*mgrc)) 。由于 osd_singleton_state 的定义为 seastar::sharded osd_singleton_state ，因此这里的 start_single 函数其实是调用了 seastar::sharded::start_single 函数来创建了一个 OSDSingletonState 对象。在 OSDSingletonState 对象初始化的过程中会创建一些 perf 和 recoverystate_perf 对象指针。\n\n3. osd_states.start()\n\n由于 osd_states 的定义为 seastar::sharded osd_states ，因此这里的 start 函数其实是调用 seastar::sharded::start 函数来初始化了 OSDState 对象。\n\n4. shard_services.start(…)\n\nshard_services.start(...) 的原始调用信息为 shard_services.start(std::ref(osd_singleton_state), std::ref(pg_to_shard_mappings), whoami, startup_time, osd_singleton_state.local().perf, osd_singleton_state.local().recoverystate_perf, std::ref(store), std::ref(osd_states)) 。由于 shard_services 的定义为 seastar::sharded shard_services ，因此这里的 start 函数其实是调用 seastar::sharded::start 函数来初始化了 ShardServices 对象。\n\n5. heartbeat.reset(…)\n\n重置 heartbeat 对象。\n\n6. local_service.report_stats()\n\n该函数的调用被封装在 shard_services.invoke_on_all 内部，意味着这会让每个 shard 执行 local_service.report_stats() 函数。但是只有在 crimson_osd_stat_interval 配置了非零的情况下才会执行该逻辑。 crimson_osd_stat_interval 参数默认为 0 。\n\n7. store.report_stats()\n\nstore.report_stats() 对应的函数为 SeaStore::report_stats 。\nSeaStore::report_stats 函数中关键逻辑为:\nclocal_store.get_device_stats(report_detail) // shard_stores.invoke_on_alllocal_store.get_io_stats(report_detail) // shard_stores.invoke_on_allINFO(...);\n\n\n8. stats_timer.arm_periodic(…)\n\nstats_timer.arm_periodic(...) 对应的原始调用为 stats_timer.arm_periodic(std::chrono::seconds(stats_seconds)) 。用于设置一个周期性的定时器，该定时器的运行是由 Seastar 框架的事件循环管理的，与函数调用的生命周期无关。\n\n9. open_meta_coll\n\nopen_meta_coll 对应的函数为 OSD::open_meta_coll 。需要注意该逻辑仅限 PRIMARY_CORE 对应的 shard 执行。\nSeaStore::report_stats 函数中关键逻辑为:\ncstore.get_sharded_store().open_collection(coll_t::meta())pg_shard_manager.init_meta_coll(ch, store.get_sharded_store())\n\n\n10. pg_shard_manager.get_meta_coll().load_superblock()\n\n对应的函数为 OSDMeta::load_superblock 。用于从 store 存储中读取 superblock 信息。\n\n11. pg_shard_manager.set_superblock(superblock)\n\n对应的函数为 PGShardManager::set_superblock 。\nPGShardManager::set_superblock 函数中关键逻辑为:\ncget_osd_singleton_state().set_singleton_superblock(superblock)local_service.local_state.update_shard_superblock(superblock) // shard_services.invoke_on_all\n\n\n12. pg_shard_manager.get_local_map(superblock.current_epoch)\n\n对应的函数为 OSDSingletonState::get_local_map 。\n\n13. pg_shard_manager.update_map(std::move(map))\n\n对应的函数为 PGShardManager::update_map 。\nPGShardManager::update_map 函数中关键逻辑为:\ncget_osd_singleton_state().update_map(...)local.local_state.update_map(...) // shard_services.invoke_on_all\n\n\n14. local_service.local_state.osdmap_gate.got_map(…)\n\n原始的调用为 local_service.local_state.osdmap_gate.got_map(osdmap-&gt;get_epoch()) ， 该函数的调用被封装在 shard_services.invoke_on_all 内部，意味着这会让每个 shard 执行 local_service.local_state.osdmap_gate.got_map(osdmap-&gt;get_epoch()) 函数。\n\n15. pg_shard_manager.load_pgs(store)\n\n对应的函数为 PGShardManager::load_pgs 。\nPGShardManager::load_pgs 函数中关键逻辑为:\ncstore.list_collections()// seastar::parallel_for_eachget_pg_to_shard_mapping().get_or_create_pg_mapping(pgid, shard_core)shard_services.load_pg(pgid)per_shard_state.pg_map.pg_loaded(pgid, std::move(pg))\n\n\n16. cluster_msgr 和 public_msgr\n\n对应的批量的原始调用为:\nccluster_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_CLUSTER))cluster_msgr-&gt;start(dispatchers)public_msgr-&gt;bind(pick_addresses(CEPH_PICK_ADDRESS_PUBLIC))public_msgr-&gt;start(dispatchers)\n\npick_addresses 函数执行的时候，其内部仅会选择 message v2 的地址，因此从这里可以看出在 crimson osd 中不支持 message v1 。\nbind 函数对应的是 SocketMessenger::bind 。 start 函数对应的是 SocketMessenger::start 。\nSocketMessenger::bind 函数中关键逻辑为:\nctry_bind(addrs, local_conf()-&gt;ms_bind_port_min, local_conf()-&gt;ms_bind_port_max)do_listen(entity_addrvec_t&#123;to_bind&#125;)ShardedServerSocket::create(dispatch_only_on_sid)listener-&gt;listen(listen_addr)    seastar::listen(s_addr, lo) // this-&gt;container().invoke_on_all\n\n从上面中可以看出会让每个 shard 都监听相同的端口。\nSocketMessenger::start 函数中关键逻辑为:\nclistener-&gt;accept([this](SocketRef _socket, entity_addr_t peer_addr) &#123;    assert(get_myaddr().is_msgr2());    SocketFRef socket = seastar::make_foreign(std::move(_socket));    // 对于 client 和 cluster 的消息，这里的 fix 是 false     // 对于 heart beat 的消息，这里的 fix 是 true    if (listener-&gt;is_fixed_shard_dispatching()) &#123;        return accept(std::move(socket), peer_addr);    &#125;    else &#123;        // 转发请求到对应的 shard 中        return seastar::smp::submit_to(sid, [this, peer_addr, socket = std::move(socket)]() mutable &#123;            return accept(std::move(socket), peer_addr);        &#125;);    &#125;&#125;);\n\n\n17. monc 和 mgrc 的 start\n\n对应的批量的原始调用为:\ncmonc-&gt;start()mgrc-&gt;start()\n\n其中 monc-&gt;start() 对应的函数为 crimson::mon::Client::start 。 mgrc-&gt;start() 对应的函数为 crimson::mgr::Client::start 。\ncrimson::mon::Client::start 函数中关键逻辑为:\ncauth_registry.refresh_config()load_keyring()monmap.build_initial(crimson::common::local_conf(), false)authenticate()timer.arm_periodic(interval)\n\ncrimson::mgr::Client::start 函数中关键逻辑为:\ncseastar::now()\n\n\n18. _add_me_to_crush()\n\n该函数对应的是 OSD::_add_me_to_crush 。在该函数中，如果 osd_crush_update_on_start 配置为 true ，则会在 osd 启动时尝试将自己的信息添加到 crush map 中。\nOSD::_add_me_to_crush 函数中关键逻辑为:\nclocal_conf().get_val&lt;bool&gt;(&quot;osd_crush_update_on_start&quot;)local_conf().get_val&lt;double&gt;(&quot;osd_crush_initial_weight&quot;)store.stat()get_weight()loc.init_on_startup()monc-&gt;run_command(std::move(cmd), &#123;&#125;)\n\n\n19. monc-&gt;renew_subs()\n\n对应的函数为 crimson::mon::Client::renew_subs 。 内部逻辑为向 monitor 发送 CEPH_MSG_MON_SUBSCRIBE 消息，用于订阅 osd_pg_creates ， mgrmap ， osdmap 的变更消息。\n\n20. heartbeat-&gt;start(…)\n\n原始的调用为 heartbeat-&gt;start(pick_addresses(CEPH_PICK_ADDRESS_PUBLIC), pick_addresses(CEPH_PICK_ADDRESS_CLUSTER)) , 对应的函数为 Heartbeat::start 。\n\n21. start_asok_admin()\n\n对应的函数为 OSD::start_asok_admin 。 用于创建本地的 socket 文件，并注册可执行的命令。\n\n22. log_client.set_fsid(monc-&gt;get_fsid())\n\n设置日志记录中的 fsid 信息。\n\n23. start_boot()\n\n对应的函数为 OSD::start_boot 。\nOSD::start_boot 函数中关键逻辑为:\ncpg_shard_manager.set_preboot()monc-&gt;get_version(&quot;osdmap&quot;)_preboot(oldest, newest)\n\n十、相关资料\nhttps://ceph.io/en/news/crimson/\nhttps://ceph.io/en/news/blog/2023/crimson-multi-core-scalability/\nhttps://ceph.io/en/news/blog/2025/crimson-T-release/\nhttps://docs.ceph.com/en/latest/dev/crimson/crimson/\nhttps://docs.ceph.com/en/latest/cephadm/install/#bootstrap-a-new-cluster\nhttps://www.51cto.com/article/749735.html\nhttps://zhuanlan.zhihu.com/p/667949613\nhttps://docs.redhat.com/zh-cn/documentation/red_hat_ceph_storage/7/html/administration_guide/crimson\nhttps://ceph.io/en/news/blog/2023/crimson-multi-core-scalability/\nhttps://www.icviews.cn/semiCommunity/postDetail/6586\n\n","tags":["Crimson","Seastore","异步编程"]}]